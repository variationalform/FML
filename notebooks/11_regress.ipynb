{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "#### *variationalform* <https://variationalform.github.io/>\n",
    "\n",
    "#### *Just Enough: progress at pace*\n",
    "\n",
    "<https://variationalform.github.io/>\n",
    "\n",
    "<https://github.com/variationalform>\n",
    "\n",
    "Simon Shaw\n",
    "<https://www.brunel.ac.uk/people/simon-shaw>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<p>\n",
    "This work is licensed under CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)\n",
    "\n",
    "<p>\n",
    "Visit <a href=\"http://creativecommons.org/licenses/by-sa/4.0/\">http://creativecommons.org/licenses/by-sa/4.0/</a> to see the terms.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>This document uses python</td>\n",
    "<td>\n",
    "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>and also makes use of LaTeX </td>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/LaTeX_logo.svg/320px-LaTeX_logo.svg.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>in Markdown</td> \n",
    "<td>\n",
    "<img src=\"https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What this is about:\n",
    "\n",
    "- Polynomial regression.\n",
    "- The best straight line (or polynomial) through data.\n",
    "- The standard approach, and also ridge and LASSO regularization.\n",
    "- $p > n$\n",
    "- Logistic regression for classification.\n",
    "- How these work, the maths, and the code.\n",
    "\n",
    "As usual our emphasis will be on *doing* rather than *proving*:\n",
    "*just enough: progress at pace*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assigned Reading\n",
    "\n",
    "For this worksheet you are recommended Chapters 9 of [MML],\n",
    "Chapter 3 of [MLFCES], Chapter 7 of [IPDS], \n",
    "\n",
    "- MML: Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong.\n",
    "  Cambridge University Press. <https://mml-book.github.io>.\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists, by Andreas Lindholm,\n",
    "  Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön. Cambridge University Press. \n",
    "  <http://smlbook.org>.\n",
    "- IPDS: Introduction to Probability for Data Science, by Stanley H. Chan,\n",
    "  <https://probability4datascience.com>\n",
    "\n",
    "These can be accessed legally and without cost.\n",
    "\n",
    "There are also these useful references for coding:\n",
    "\n",
    "- PT: `python`: <https://docs.python.org/3/tutorial>\n",
    "- NP: `numpy`: <https://numpy.org/doc/stable/user/quickstart.html>\n",
    "- MPL: `matplotlib`: <https://matplotlib.org>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "So far we have been mainly concerned with **Classification**. This is a big part of \n",
    "Machine Learning but not the only part. \n",
    "\n",
    "Classification is where we seek to categorise among a set of possible labels.\n",
    "\n",
    "Regression is where we seek to determine a value from a continuous set of \n",
    "possible values.\n",
    "\n",
    "In the limit of lots of labels, or a coarsely discretized set, the distinction\n",
    "between these becomes blurred and some models can then be used for both tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Basic Idea\n",
    "\n",
    "The aim is to build a model that given $x$ (a feature), predicts $y(x)$ (a label).\n",
    "We begin with this example data...\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{l}\n",
    "x_i \\\\ y_i\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{l}\n",
    "0\\\\ 1\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "1\\\\ 3\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "2\\\\ 1\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "4\\\\ 3\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "6\\\\ 7\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "7\\\\ 5\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "We start the indexing at zero, for `python`, so, for example,\n",
    "$(x_3,y_3)^T = (4,3)^T$ (again, we only use column vectors).\n",
    "\n",
    "There are $N_p = 6$ training points. \n",
    "\n",
    "By now we should be familiar with the words **training** and **testing**. In Machine Learning\n",
    "we talk about **training** when we build a model using a set of training data that has a\n",
    "particular set of features. In supervised learning these data points are also labelled. The\n",
    "label is what we want the model to predict from the features. After the model is built, \n",
    "**testing** refers to its use on a hold-out set in order to report on its success on unseen\n",
    "data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's bring in our standard imports - with a new one: `linear_model` from `sklearn` \n",
    "\n",
    "We then set these data points up in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_vals = np.array([[0,1,2,4,6,7]]).T\n",
    "y_vals = np.array([[1,3,1,3,7,5]]).T\n",
    "# this is the number of data points\n",
    "Np = X_vals.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We know these points represent a function, which will have a graph, but it\n",
    "isn't useful to join the points up as a graph yet because they may well \n",
    "contain noise. So we just plot the scatter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_vals,y_vals,',r', marker='o')\n",
    "plt.xlabel('x'); plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we believe that $y = mx+c$ - a straight line. \n",
    "How do we find the **best** straight line? \n",
    "\n",
    "This means, how do we find the **best** $m$ and $c$? What do we mean by **best**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We define $\\boldsymbol{X}$, the **Feature Matrix** (recall that `numpy` indices start at zero), and the training input vector $\\boldsymbol{y}$ as follows:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left(\\begin{array}{ll}\n",
    "1 & x_0 \\\\\n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "1 & x_3 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_{N_p-1} \\\\\n",
    "\\end{array}\\right)\n",
    "\\qquad\\text{ and }\\qquad\n",
    "\\boldsymbol{y} = \\left(\\begin{array}{l}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_{N_p-1} \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Note that $\\boldsymbol{X}$ is sometimes called the **Design Matrix**, or the\n",
    "**Matrix of Regressors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now want to use this training data to construct a predictor \n",
    "$\\hat{y}(x_i)$ of $y_i$ where $\\hat{y} = mx+c$ is the\n",
    "**best straight line** through the data.\n",
    "We write $\\boldsymbol{\\theta} = (c, m)^T$ and note that \n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{y}} = \\left(\\begin{array}{l}\n",
    "\\hat{y}(x_0) \\\\\n",
    "\\hat{y}(x_1) \\\\\n",
    "\\hat{y}(x_2) \\\\\n",
    "\\hat{y}(x_3) \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}(x_{N_p-1}) \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{l}\n",
    "m x_0 + c \\\\\n",
    "m x_1 + c\\\\\n",
    "m x_2 + c\\\\\n",
    "m x_3 + c\\\\\n",
    "\\vdots \\\\\n",
    "m x_{N_p-1} + c \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{ll}\n",
    "1 & x_0 \\\\\n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "1 & x_3 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_{N_p-1} \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{l}\n",
    "c\\\\ m\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\boldsymbol{X}\\boldsymbol{\\theta}.\n",
    "$$\n",
    "\n",
    "Notice that we can write $y = \\boldsymbol{\\theta}^T\\boldsymbol{x}$ for \n",
    "$\\boldsymbol{x} = (1, x)^T$ an **augmented (with unity) data point** for which we want to \n",
    "predict $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss and Cost\n",
    "\n",
    "To define **best** straight line we need the notions of **cost** and **loss**.\n",
    "The *best* will then  be the *cheapest* as measured by this *cost*.\n",
    "\n",
    "These terms are used in quite specific ways.\n",
    "\n",
    "We have set up a straight line as $\\hat{y}(x) = mx+c$ \n",
    "\n",
    "For each $i$ we then measure the difference between $y_i$\n",
    "(what we want $x_i$ to give), and $\\hat{y}_i = m x_i +c$ (what\n",
    "$x_i$ actually gives from the model).\n",
    "\n",
    "We actually look at the squared difference because we want to eliminate \n",
    "the sign. This quantity, $\\big(y_i - \\hat{y}(x_i)\\big)^2$\n",
    "is what we call the **loss** - in this case, the *squared error loss*.\n",
    "\n",
    "The loss depends on the choice of data point. To get the overall\n",
    "picture of how well the model is doing we add all these squared error\n",
    "losses together and divide by their number to get the average.\n",
    "This **mean-squared-error** (MSE) is called the **cost**. It is given by,\n",
    "\n",
    "$$\n",
    "\\mathcal{E}\n",
    "= \\frac{1}{N_p}\\sum_i\\vert y_i - \\hat{y}_i\\vert^2\n",
    "= \\frac{1}{N_p}\\sum_i\\vert y_i - (m x_i + c)\\vert^2\n",
    "= \\frac{1}{N_p}\\Vert\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}\\Vert_2^2.\n",
    "$$\n",
    "\n",
    "This is because the vector $2$-norm sums up all the\n",
    "squares of the vector elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In [MLFCES, Chapter 3] they talk about the cost, as above, being the average of\n",
    "the squared losses, $y_i - \\hat{y}$.\n",
    "\n",
    "In [MML, Chapter 8] the cost is called the **empirical risk**.\n",
    "\n",
    "This terminology is not always used consistently though. In [IPDS, Chapter 7]\n",
    "the term *loss* refers to\n",
    "$\\Vert\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}\\Vert_2^2$.\n",
    "Although this isn't our cost because the average isn't taken.\n",
    "\n",
    "Don't get too hung up on this. The MSE Cost s what we are going to work with.\n",
    "\n",
    "And the process we are going to follow is essentially the same in all sources.\n",
    "\n",
    "We will choose $m$ and $c$, as captured in $\\boldsymbol{\\theta} = (c,m)^T$,\n",
    "such that this MSE cost (or empirical risk) is as small as it can be given\n",
    "this training set of data.\n",
    "\n",
    "This is called **Ordinary Least Squares** (OLS) regression: we are minimizing the \n",
    "average of the squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have this *cost*, $\\mathcal{E}(\\boldsymbol{\\theta}) =\n",
    "N_p^{-1}\\Vert \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}\\Vert_2^2$,\n",
    "and we want to find its minimum value. The data \n",
    "are fixed - the only variable we can control is $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Let's think about this: ideally we would like to determine the regression\n",
    "polynomial, the parameters in $\\boldsymbol{\\theta} = (c,m)^T$, so that they\n",
    "bring the cost to zero: $\\mathcal{E}(\\boldsymbol{\\theta})=0$.\n",
    "\n",
    "This may not be possible (why?), but we can at least ask for it to be as small as\n",
    "possible. (It can't be negative - so zero is the greatest lower bound.)\n",
    "Before we think about minimization let's play with this cost expression and\n",
    "see what it reveals.\n",
    "\n",
    "Consider how the value of $\\mathcal{E}$ changes between the point\n",
    "$\\boldsymbol{\\theta}\\in\\mathbb{R}^2$ and another point\n",
    "$\\boldsymbol{\\theta}+\\boldsymbol{v}\\in\\mathbb{R}^2$. We need some algebra..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With $N_p\\mathcal{E}(\\boldsymbol{\\theta}) =\n",
    "\\Vert \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}\\Vert_2^2$, \n",
    "we express $N_p\\mathcal{E}(\\boldsymbol{\\theta}+\\boldsymbol{v})=\n",
    "\\Vert \\boldsymbol{y} - \\boldsymbol{X}(\\boldsymbol{\\theta}+\\boldsymbol{v})\\Vert_2^2$\n",
    "in terms of\n",
    "$N_p\\mathcal{E}(\\boldsymbol{\\theta}) + \\text{'extras'}$.\n",
    "Recall that the $2$-norm arises from the scalar product...\n",
    "\n",
    "\\begin{align}\n",
    "N_p\\mathcal{E}(\\boldsymbol{\\theta}+\\boldsymbol{v})\n",
    "& =\n",
    "\\Vert \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2,\n",
    "\\\\\n",
    "& =\n",
    "(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{X}\\boldsymbol{v},\n",
    "\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{X}\\boldsymbol{v}),\n",
    "\\\\\n",
    "& =\n",
    "\\Vert \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}\\Vert_2^2\n",
    "-\n",
    "2(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta},\\boldsymbol{X}\\boldsymbol{v})\n",
    "+\n",
    "\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2,\n",
    "\\\\\n",
    "& = \n",
    "N_p\\mathcal{E}(\\boldsymbol{\\theta})\n",
    "+\n",
    "2(\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y},\\boldsymbol{X}\\boldsymbol{v})\n",
    "+\n",
    "\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2.\n",
    "\\end{align}\n",
    "\n",
    "Next, in general, by **taking the transpose through**,\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{v},\\boldsymbol{K}\\boldsymbol{u})\n",
    "=\\boldsymbol{v}^T\\boldsymbol{K}\\boldsymbol{u}\n",
    "=(\\boldsymbol{K}^T\\boldsymbol{v})^T\\boldsymbol{u}\n",
    "=(\\boldsymbol{K}^T\\boldsymbol{v},\\boldsymbol{u})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{We then have:}\\qquad\n",
    "2(\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y},\\boldsymbol{X}\\boldsymbol{v})\n",
    "=\n",
    "2(\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y}),\\boldsymbol{v}).\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have therefore shown that \n",
    "\n",
    "$$\n",
    "N_p\\mathcal{E}(\\boldsymbol{\\theta}+\\boldsymbol{v})\n",
    "= \n",
    "N_p\\mathcal{E}(\\boldsymbol{\\theta})\n",
    "+\n",
    "2(\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y}),\\boldsymbol{v})\n",
    "+\n",
    "\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2.\n",
    "$$\n",
    "\n",
    "Suppose now that we consider the particular case where\n",
    "$\\boldsymbol{\\theta}$ satisfies\n",
    "$\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y})$.\n",
    "\n",
    "Then, any departure from $\\boldsymbol{\\theta}$ to a different point\n",
    "$\\boldsymbol{\\theta}+\\boldsymbol{v}$ produces this cost,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{E}(\\boldsymbol{\\theta}+\\boldsymbol{v})\n",
    "& = \n",
    "\\mathcal{E}(\\boldsymbol{\\theta})\n",
    "+\n",
    "\\frac{2}{N_p}(\\underbrace{\n",
    "\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y})\n",
    "}_{=0},\\boldsymbol{v})\n",
    "+\n",
    "\\frac{1}{N_p}\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "\\\\\n",
    "& = \n",
    "\\mathcal{E}(\\boldsymbol{\\theta})\n",
    "+\n",
    "\\frac{1}{N_p}\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "\\ge \n",
    "\\mathcal{E}(\\boldsymbol{\\theta})\n",
    "\\text{ for all non-zero } \\boldsymbol{v}.\n",
    "\\end{align}\n",
    "\n",
    ">**HENCE:** cost is minimized when $\\boldsymbol{\\theta}$ satisfies\n",
    "$\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}=\\boldsymbol{X}^T\\boldsymbol{y}$.\n",
    "\n",
    ">These are called the **Normal Equations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Normal Equations\n",
    "\n",
    "We refer to the system \n",
    "\n",
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}=\\boldsymbol{X}^T\\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "as *the normal equations*. The matrix $\\boldsymbol{X}^T\\boldsymbol{X}$\n",
    "in our example above is\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X} =\n",
    "\\left(\\begin{array}{llllll}\n",
    "1 & 1 & 1 & 1 & \\cdots & 1 \\\\\n",
    "x_0 & x_1 & x_2 & x_3 & \\cdots & x_{N_p-1}\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{ll}\n",
    "1 & x_0 \\\\\n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "1 & x_3 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_{N_p-1} \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{ll}\n",
    "N_p & \\sum_i x_i \\\\\n",
    "\\sum_i x_i & \\boldsymbol{x}^T\\boldsymbol{x}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{x} = (x_0, x_1, x_2, \\ldots)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Least Squares Regression Solved\n",
    "\n",
    "Given the data set $\\boldsymbol{x} = (x_0, x_1, \\ldots)^T$ and\n",
    "$\\boldsymbol{y} = (y_0, y_1, \\ldots)^T$, where for example earlier we had,\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{l}\n",
    "x_i \\\\ y_i\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{l}\n",
    "0\\\\ 1\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "1\\\\ 3\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "2\\\\ 1\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "4\\\\ 3\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "6\\\\ 7\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "7\\\\ 5\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "\n",
    "we form the design matrix $\\boldsymbol{X} = (\\boldsymbol{1}, \\boldsymbol{x})$ \n",
    "(here $\\boldsymbol{1}$ denotes a column of $1$'s).\n",
    "\n",
    "The solution $\\boldsymbol{\\theta}=(c,m)^T$ to the normal equations\n",
    "$\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}=\\boldsymbol{X}^T\\boldsymbol{y}$\n",
    "then produces a straight line $\\hat{y}(x) = mx +c$ through these data points.\n",
    "\n",
    "This line is the **best straight line through these data** in the sense of \n",
    "minimum (MSE) cost $N_p^{-1}\\mathcal{E}(\\boldsymbol{\\theta})\n",
    "= \\Vert\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}\\Vert_2^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's do this in `numpy`. we already have `X_vals` and `y_vals` set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f'X_vals = {X_vals.T} and y_vals = {y_vals.T}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# linear polynomial regression: yhat1 = mx+c\n",
    "# set up the design matrix\n",
    "X = np.c_[np.ones([Np,1]), X_vals]\n",
    "# solve the normal equations\n",
    "theta1 = theta = np.linalg.solve(X.T @ X, X.T @ y_vals)\n",
    "print(f'c = {theta[0]} and m = {theta[1]}')\n",
    "plt.plot(X_vals, y_vals,'.r', marker='o')\n",
    "y_hat1 = theta[0] + theta[1]*X_vals\n",
    "plt.plot(X_vals, y_hat1,'g', marker='x')\n",
    "plt.xlabel('x'); plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Higher Degree Polynomial Regression\n",
    "\n",
    "Again, we are given data $\\boldsymbol{x} = (x_0, x_1, \\ldots)^T$ and\n",
    "$\\boldsymbol{y} = (y_0, y_1, \\ldots)^T$, but this time we want to fit the\n",
    "**best degree-$n$** polynomial through the points.\n",
    "\n",
    "This means that we want\n",
    "$\\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\ldots, \\theta_n)^T$\n",
    "such that \n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\cdots +\\theta_n x^n \n",
    "$$\n",
    "\n",
    "predicts $y$ with minimum MSE cost\n",
    "$\\mathcal{E}(\\boldsymbol{\\theta})\n",
    "=N_p^{-1}\\Vert\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}\\Vert_2^2$.\n",
    "This time, the design matrix looks like this,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} =\n",
    "\\left(\\begin{array}{lllll}\n",
    "1 & x_0 & x^2_0 & \\cdots & x^n_0 \\\\\n",
    "1 & x_1 & x^2_1 & \\cdots & x^n_1 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N_p-1} & x^2_{N_p-1} & \\cdots & x^n_{N_p-1} \\\\\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "Exactly the same reasoning as above tells us that \n",
    "$\\boldsymbol{\\theta}$ solves the normal equations:\n",
    "$\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}=\\boldsymbol{X}^T\\boldsymbol{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# quadratic polynomial regression: stack up the design matrix\n",
    "X = np.hstack( (np.ones([Np,1]), X_vals, X_vals**2) )\n",
    "# solve the normal equations\n",
    "theta = np.linalg.solve(X.T @ X, X.T @ y_vals)\n",
    "plt.plot(X_vals,y_vals,'.r',marker='o')\n",
    "#y_hat2 = theta[0] + theta[1]*X_vals + theta[2]*X_vals*X_vals\n",
    "# the line above is not wrong, but this is more elegant...\n",
    "y_hat2 = X @ theta\n",
    "plt.plot(X_vals, y_hat2,'g', marker='x')\n",
    "plt.xlabel('x'); plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It doesn't take much though to see that we can go to arbitrary polynomial degree.\n",
    "**BUT SHOULD WE?**\n",
    "\n",
    "There are six data points - and a quintic can fit to these exactly. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# quintic polynomial regression: stack up the design matrix\n",
    "X = np.ones([Np,1])\n",
    "for k in range(1,6): X = np.hstack( (X, X_vals**k) )\n",
    "theta = np.linalg.solve(X.T @ X, X.T @ y_vals)\n",
    "y_hat5 = X @ theta\n",
    "plt.plot(X_vals, y_vals,'.r',marker='o')\n",
    "plt.plot(X_vals, y_hat5,'b',marker='x')\n",
    "plt.xlabel('x'); plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is no good - we need many more $x$ values to see the smooth curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_vals,y_vals,'.r',marker='o')\n",
    "# a fine grid of x-values, and a re-built design matrix with them\n",
    "X_grid = np.arange(0,1+X_vals[Np-1],0.1).reshape(-1,1)\n",
    "X = np.c_[np.ones([X_grid.shape[0],1]), X_grid]\n",
    "for k in range(2,Np): X = np.c_[X, np.power(X_grid,k)]\n",
    "y_hat5 = X @ theta\n",
    "plt.plot(X_grid, y_hat5,'b')\n",
    "plt.xlabel('x'); plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This seems perfect. Are you happy with this model for predicting $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What about new data arriving? Will this be a good predictor?\n",
    "There is a danger that it is severly **over-fitted** to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose in testing the model these new unseen data points arrive\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{l}\n",
    "x_i \\\\ y_i\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{l}\n",
    "1\\\\ 2\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "3\\\\ 3\n",
    "\\end{array}\\right),\n",
    "\\left(\\begin{array}{l}\n",
    "6\\\\ 5\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "Let's plot them with black diamonds. The linear regression model is in green\n",
    "and the quintic one in blue. The original training data is red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Overfitting - generalization\n",
    "X_new = np.array([[1,3,6]]).T\n",
    "y_new = np.array([[2,3,5]]).T \n",
    "plt.plot(X_vals, y_vals,'.r', marker='o')\n",
    "plt.plot(X_grid, y_hat5,'b')\n",
    "plt.plot(X_vals, y_hat1,'g', marker='x')\n",
    "plt.plot(X_new,  y_new, 'dk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**COMMENTS?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What we are seeing here is an example of **overfitting**. The training set is modelled\n",
    "perfectly but at the expense of the model being able to **generalise** to unseen data.\n",
    "\n",
    "For this reason a *validation* hold out set is often introduced into a ML workflow in\n",
    "order to capture such undesirable properties in models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression in Scikit-Learn\n",
    "\n",
    "Let's see how to implement the standard OLS **linear** regression model in\n",
    "the `sklearn` library.\n",
    "\n",
    "The details can be found here\n",
    "<https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# standard regression\n",
    "reg_linear = linear_model.LinearRegression()\n",
    "reg_linear.fit(X_vals, y_vals)\n",
    "print('reg_coef_ = ', reg_linear.coef_)\n",
    "print('reg_intercept_ = ', reg_linear.intercept_)\n",
    "print(f'Our earlier values were: c = {theta1[0]} and m = {theta1[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can plot using the model's predicted values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions from the fitted model\n",
    "y_pred = reg_linear.predict(X_vals)\n",
    "plt.plot(X_vals,y_vals,'.r',marker='o')\n",
    "plt.plot(X_vals,y_pred,'b',marker='d')\n",
    "#plt.plot(X_vals,y_hat, 'g',marker='x', markersize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "We're now going to explore the topic of *regularization*. This is a useful concept \n",
    "when the problem we are trying to solve is in some sense *ill posed*, or likely to\n",
    "suggest spurious solutions. This can happen when for example we get a lot of data\n",
    "points clustered around a straight line and a few way off. The ones way off of the\n",
    "line - the *outliers* - may well result from measurement errors, or other erroneous\n",
    "data and yet may result in the straight line regressor that we compute\n",
    "being altered to one *nearby* but different to the one we actually want.\n",
    "\n",
    "The following discussion is distilled from\n",
    "\n",
    ">Chapter 9, Section 1,\n",
    ">**Theoretical Numerical Analysis: an introduction to advanced techniques**, by Peter Linz, \n",
    ">Dover 2001.\n",
    "\n",
    "You don't need to track that down and read it - the main ideas follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To eliminate unwanted spurious, or *nearby*, solutions, Andrey Nikolayevich Tikhonov - \n",
    "see e.g. <https://en.wikipedia.org/wiki/Andrey_Nikolayevich_Tikhonov> - put forward an\n",
    "idea that is now known as *Tikhonov regularization* - see e.g. <https://en.wikipedia.org/wiki/Ridge_regression#Tikhonov_regularization>.\n",
    "\n",
    "The idea is to add an extra term to the cost. In regression we typically a term\n",
    "proportional to the $\\ell_2$ or $\\ell_1$ norm of the parameters. So, with\n",
    "$p=2$ or $p=1$ we write our cost as\n",
    "\n",
    "$$\n",
    "\\mathcal{E}_\\alpha(\\boldsymbol{\\theta}) =\n",
    "\\Vert \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\theta}\\Vert_2^2\n",
    "+\n",
    "\\alpha\\Vert\\boldsymbol{\\theta}\\Vert_p^p.\n",
    "$$\n",
    "\n",
    "Here we are working with **Total Squared Error** (TSE) just to keep the \n",
    "formulae simpler. It's easy to divide by the number of data points\n",
    "to get the MSE cost as above.\n",
    "\n",
    "In this TSE cost, $\\alpha\\ge 0$, is the **regularization parameter**.\n",
    "Note that  $\\alpha = 0$ gets us back to where we were, and it makes no sense to\n",
    "allow $\\alpha < 0$.\n",
    "\n",
    "There three cases of interest:\n",
    "\n",
    "- $p=2$: Ridge regression\n",
    "- $p=1$: LASSO (*Least Absolute Shrinkage and Selection Operator*)\n",
    "- $p=1$ and $p=2$ Elastic net - see e.g. <https://en.wikipedia.org/wiki/Elastic_net_regularization>\n",
    "\n",
    "It takes a lot of work to properly understand why this regularization\n",
    "can be useful, but let's try and develop some intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The best case scenario is where we can find $\\boldsymbol{\\theta}$ such that\n",
    "$\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\theta}$. If in this case\n",
    "$\\boldsymbol{\\psi}$ minimizes $\\mathcal{E}_\\lambda$, then \n",
    "\n",
    "$$\n",
    "\\Vert\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\psi}\\Vert_2^2 \\le\n",
    "\\mathcal{E}_\\alpha(\\boldsymbol{\\psi}) \\le\n",
    "\\mathcal{E}_\\alpha(\\boldsymbol{\\theta}) =\n",
    "\\alpha\\Vert\\boldsymbol{\\theta}\\Vert_p^p\n",
    "$$\n",
    "\n",
    "which means we have some control over $\\boldsymbol{\\psi}$. In particular,\n",
    "though it is not necessarily the case that\n",
    "$\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\psi}$, this suggests\n",
    "that by taking $\\alpha$ small we can make \n",
    "$\\Vert\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\psi}\\Vert_2$\n",
    "as small as we please.\n",
    "\n",
    "On the other hand though, if we do make $\\alpha$ small then we get back to the \n",
    "original least squares regression - and we presumably don't want that \n",
    "because if we did we wouldn't have introduced $\\alpha$ in the first place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $\\boldsymbol{\\psi}$ minimizes $\\mathcal{E}_\\alpha$, then, when $p=2$,\n",
    "we have already seen that\n",
    "$\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\psi} +\\alpha\\boldsymbol{\\psi}\n",
    "=\\boldsymbol{X}^T\\boldsymbol{y}$.\n",
    "\n",
    "Using this, as we will have seen before, \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{E}_\\alpha(\\boldsymbol{\\psi}+\\boldsymbol{v})\n",
    "& =\n",
    "\\Vert \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\psi}-\\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "+\n",
    "\\alpha\\Vert\\boldsymbol{\\psi}+\\boldsymbol{v}\\Vert_2^2,\n",
    "\\\\\n",
    "& =\n",
    "\\Vert \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\psi}\\Vert_2^2\n",
    "-\n",
    "2(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\psi},\\boldsymbol{X}\\boldsymbol{v})\n",
    "+\n",
    "\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "+\n",
    "\\alpha\\Vert\\boldsymbol{\\psi}\\Vert_2^2\n",
    "+\n",
    "2\\alpha(\\boldsymbol{\\psi},\\boldsymbol{v})\n",
    "+\n",
    "\\alpha\\Vert\\boldsymbol{v}\\Vert_2^2,\n",
    "\\\\\n",
    "& = \n",
    "\\mathcal{E}_\\alpha(\\boldsymbol{\\psi})\n",
    "+\n",
    "2(\\boldsymbol{X}\\boldsymbol{\\psi} - \\boldsymbol{y},\\boldsymbol{X}\\boldsymbol{v})\n",
    "+\n",
    "2\\alpha(\\boldsymbol{\\psi},\\boldsymbol{v})\n",
    "+\n",
    "\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "+\n",
    "\\alpha\\Vert\\boldsymbol{v}\\Vert_2^2,\n",
    "\\\\\n",
    "& = \n",
    "\\mathcal{E}_\\alpha(\\boldsymbol{\\psi})\n",
    "+\n",
    "2(\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\psi} - \\boldsymbol{y}),\\boldsymbol{v})\n",
    "+\n",
    "2\\alpha(\\boldsymbol{\\psi},\\boldsymbol{v})\n",
    "+\n",
    "\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "+\n",
    "\\alpha\\Vert\\boldsymbol{v}\\Vert_2^2,\n",
    "\\\\\n",
    "& = \n",
    "\\mathcal{E}_\\alpha(\\boldsymbol{\\psi})\n",
    "+\n",
    "2(\\underbrace{\n",
    "\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\psi} - \\boldsymbol{y})+\\alpha\\boldsymbol{\\psi}\n",
    "}_{=0},\\boldsymbol{v})\n",
    "+\n",
    "\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "+\n",
    "\\alpha\\Vert\\boldsymbol{v}\\Vert_2^2,\n",
    "\\\\\n",
    "& = \n",
    "\\mathcal{E}_\\alpha(\\boldsymbol{\\psi})\n",
    "+\n",
    "\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "+\n",
    "\\alpha\\Vert\\boldsymbol{v}\\Vert_2^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So: $\\mathcal{E}_\\alpha(\\boldsymbol{\\psi}+\\boldsymbol{v})\n",
    "= \\Vert \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\psi}-\\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "= \\mathcal{E}_\\alpha(\\boldsymbol{\\psi})\n",
    "+\\Vert \\boldsymbol{X}\\boldsymbol{v}\\Vert_2^2\n",
    "+\\lambda\\Vert\\boldsymbol{v}\\Vert_2^2$.\n",
    "\n",
    "This is telling us that if we have spurious solutions, and if they are \n",
    "somehow *nearby* $\\boldsymbol{\\psi}$ in the sense that we can reach them with\n",
    "$\\boldsymbol{v}$ being quite small, then the cost can be made significantly\n",
    "different, even for a nearby solution, by choosing $\\alpha$ large.\n",
    "\n",
    "Here, then, is the dilemma:\n",
    "\n",
    "> $\\alpha$ small keeps us near to the original problem, but this problem\n",
    "> might be hard to solve because of nearby spurious solutions.\n",
    ">\n",
    "> $\\alpha$ large makes it easier to find a well defined minimum cost, \n",
    "> but this cost is may be some distance from the one we actually want\n",
    "> minimized.\n",
    "\n",
    "In the end, we often have to proceed by trial and error.\n",
    "\n",
    ">**NOTE:** there is no simple manipulation like that above for $p=1$. This is\n",
    "because the $2$ norm arises from an inner product, whereas the $1$ norm has\n",
    "no useful alternative representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how to implement **Ridge** and **LASSO** in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('previous reg_coef_ = ', reg_linear.coef_, end=', ')\n",
    "print('previous reg_intercept_ = ', reg_linear.intercept_)\n",
    "# ridge regression with \n",
    "reg_ridge = linear_model.Ridge(alpha=0.5)\n",
    "reg_ridge.fit(X_vals, y_vals)\n",
    "# Make predictions using the testing set\n",
    "y_hat_ridge = reg_linear.predict(X_vals)\n",
    "print('reg_coef_ = ', reg_ridge.coef_, end=', ')\n",
    "print('reg_intercept_ = ', reg_ridge.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The coefficients differ, but only slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are the training data in red, original OLS linear predictor in blue, and the ridge\n",
    "predictor in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_vals, y_vals,'.r',marker='o')\n",
    "plt.plot(X_vals, y_hat1,'.b',marker='d')\n",
    "plt.plot(X_vals, y_hat_ridge,'g',marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can do the comparisons all in one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# standard regression\n",
    "reg_linear = linear_model.LinearRegression()\n",
    "reg_linear.fit(X_vals, y_vals)\n",
    "# Make predictions\n",
    "y_hatOL = reg_linear.predict(X_vals)\n",
    "# ridge regression\n",
    "reg_ridge = linear_model.Ridge(alpha=0.5)\n",
    "reg_ridge.fit(X_vals, y_vals)\n",
    "# Make predictions\n",
    "y_hatR = reg_ridge.predict(X_vals)\n",
    "print('linear reg_coef_ = ', reg_linear.coef_)\n",
    "print('linear reg_intercept_ = ', reg_linear.intercept_)\n",
    "print('ridge reg_coef_ = ', reg_ridge.coef_)\n",
    "print('ridge reg_intercept_ = ', reg_ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_vals,y_vals,'.r',marker='o')\n",
    "plt.plot(X_vals,y_hatOL,'.b',marker='d')\n",
    "plt.plot(X_vals,y_hatR,'g',marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For LASSO we can just alter a few bits of code like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('linear reg_coef_ = ', reg_linear.coef_)\n",
    "print('linear reg_intercept_ = ', reg_linear.intercept_)\n",
    "print('ridge reg_coef_ = ', reg_ridge.coef_)\n",
    "print('ridge reg_intercept_ = ', reg_ridge.intercept_)\n",
    "# standard regression\n",
    "reg_linear = linear_model.LinearRegression()\n",
    "reg_linear.fit(X_vals, y_vals)\n",
    "# Make predictions\n",
    "y_hatOL = reg_linear.predict(X_vals)\n",
    "# LASSO regression\n",
    "reg_lasso = linear_model.Lasso(alpha=.5)\n",
    "reg_lasso.fit(X_vals, y_vals)\n",
    "# Make predictions\n",
    "y_hatL = reg_lasso.predict(X_vals)\n",
    "print('LASSO reg_coef_ = ', reg_lasso.coef_)\n",
    "print('LASSO reg_intercept_ = ', reg_lasso.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This time the coefficient differences are more noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are the training data in red, original OLS linear predictor in blue, the ridge\n",
    "predictor in green, and the LASSO predictor in cyan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_vals,y_vals,'.r',marker='o')\n",
    "plt.plot(X_vals,y_hatOL,'.b',marker='d')\n",
    "plt.plot(X_vals,y_hatR,'g',marker='x')\n",
    "plt.plot(X_vals,y_hatL,'c',marker='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "Standard OLS linear regression is ubiquitous. A regularized version is used when there \n",
    "are particular needs. We'll return to this point below when we have discussed \n",
    "multi-variate linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Linear Regression\n",
    "\n",
    "It is common that there is more than independent variable in play and that \n",
    "we want a linear predictor for $y$ as a function of these many independent\n",
    "variables. In this case we could think of,\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots +\\theta_p x_p. \n",
    "$$\n",
    "\n",
    "Note that this is quite different to the expression above for a high-degree\n",
    "polynomial. Don't get these notions confused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case we would still expect to have training data,\n",
    "$y_i$ and $\\boldsymbol{x}_i = (x_{1,i}, x_{2,i}, \\ldots, x_{p,i})^T$\n",
    "(for $i=1,\\ldots,n$ say),\n",
    "that can be placed in a design matrix, and so we assume that we have,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left(\\begin{array}{llll}\n",
    "1 & x_{1,1} & x_{2,1} & \\cdots \\\\\n",
    "1 & x_{1,2} & x_{2,2} & \\cdots \\\\\n",
    "1 & x_{1,3} & x_{2,3} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots \\\\\n",
    "\\end{array}\\right),\n",
    "\\quad\\boldsymbol{\\theta} = \\left(\\begin{array}{l}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\theta_3 \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}\\right)\n",
    "\\quad\\text{ and }\\quad\n",
    "\\boldsymbol{y} = \\left(\\begin{array}{l}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "We can still set up the MSE cost,\n",
    "$\\mathcal{E}=N_p^{-1}\\Vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\Vert_2^2$\n",
    "and show as before that we can mimimize it if we can solve the normal equations.\n",
    "\n",
    "The details can be found in the recommended reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "Standard OLS linear regression is ubiquitous. The regularized versions are used when there \n",
    "are particular needs.\n",
    "\n",
    "- Ridge: this is often used to prevent *overfitting* - the trap that we discussed above\n",
    "where we lose the the model's ability to generalise to unseen data.\n",
    "\n",
    "- LASSO: the $\\ell_1$ norm encourages *sparsity*. This means that the vector of\n",
    "regression coefficents, $\\boldsymbol{\\theta}$, may contain several zeros. This \n",
    "propensity of the LASSO regularizer is a useful **feature selection** technique.\n",
    "This is useful for multi-variable regression (as above).\n",
    "\n",
    "For more discussion see [MLFCES, Chapter 3.3], [MML, Chap 9.2.4] and, in particular, [IPDS, Chapter 7.4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The $p > n$ Issue\n",
    "\n",
    "Suppose that\n",
    "$\\hat{y}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots +\\theta_p x_p$.\n",
    "with $n$ observations. Then..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For $i=1,\\ldots,n$ we have training data,\n",
    "$y_i$ and $\\boldsymbol{x}_i = (x_{1,i}, x_{2,i}, \\ldots, x_{p,i})^T$...\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left(\\begin{array}{llllll}\n",
    "1 & x_{1,1} & x_{2,1} & \\cdots & \\cdots x_{p,1} \\\\\n",
    "1 & x_{1,2} & x_{2,2} & \\cdots & \\cdots x_{p,2} \\\\\n",
    "1 & x_{1,3} & x_{2,3} & \\cdots & \\cdots x_{p,3} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots \\\\\n",
    "1 & x_{1,n} & x_{2,n} & \\cdots & \\cdots x_{p,n} \\\\\n",
    "\\end{array}\\right),\n",
    "\\quad\\boldsymbol{\\theta} = \\left(\\begin{array}{l}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_p \\\\\n",
    "\\end{array}\\right)\n",
    "\\quad\\text{ and }\\quad\n",
    "\\boldsymbol{y} = \\left(\\begin{array}{l}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The normal equations are\n",
    "$\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}=\\boldsymbol{X}^T\\boldsymbol{y}$\n",
    "with $\\boldsymbol{X}^T\\boldsymbol{X}$ is a $p$ by $p$ square symmetric matrix,\n",
    "and $\\boldsymbol{X}^T\\boldsymbol{y}$ an $p$ by $1$ column vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ">**THEOREM:** $\\boldsymbol{X}^T\\boldsymbol{X}$ is invertible if and only if $\\boldsymbol{X}$ has\n",
    "full column rank.\n",
    "\n",
    "In our earlier terms the column rank is the number of independent equations that the columns \n",
    "can describe.\n",
    "\n",
    "The row rank is the number of independent equations that the rows can describe.\n",
    "\n",
    ">**THEOREM:** The row and column ranks of a matrix are identical.\n",
    "\n",
    "*Why?* Use the SVD: $\\boldsymbol{K} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$\n",
    "and so $\\boldsymbol{K}^T = \\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T$.\n",
    "In both cases number of non-zero singular values (the rank) is the same. The rank is\n",
    "equal to the row rank because (for us) they both mean the same thing\n",
    "(the number of independent equations).\n",
    "\n",
    ">**WARNING**: (1) if $p>n$ then the column rank of $\\boldsymbol{X}$ cannot exceed $n$.\n",
    "\n",
    ">**WARNING**: (2) if $p>n$ then then $\\boldsymbol{X}$ does not have the full column rank of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**THEREFORE:** if $p>n$ then $\\boldsymbol{X}^T\\boldsymbol{X}$ is **not invertible** and so the \n",
    "Normal Equations cannot be solved in the sense that we have meant above where we took\n",
    "$\\boldsymbol{\\theta}=\\big(\\boldsymbol{X}^T\\boldsymbol{X}\\big)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}$.\n",
    "\n",
    "See [IPDS, Chapter 7.1.4] for a deeper treatment.\n",
    "\n",
    "Fortunately there are other ways that we can seek to minimize the cost. The `sklearn`\n",
    "routines that we introduced will be able to do this.\n",
    "\n",
    "This is another good reason to use reputable software libraries rather than always write \n",
    "your own code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We can use these regression ideas for binary classification.\n",
    "The idea is to use a linear regression to create a decision boundary\n",
    "between the two classes.\n",
    "\n",
    "The logistic, or sigmoid, function can then be used to create a \n",
    "switch from **off** to **on** as we cross the decision boundary.\n",
    "\n",
    "Here, for $a\\in\\mathbb{R}$, is the logistic function, often called the **sigmoid**:\n",
    "\n",
    "$$\n",
    "\\sigma(x\\mid a) = \\frac{1}{1+\\exp(-ax)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sigma(x, a):\n",
    "    return (1+np.exp(-a*x))**(-1)\n",
    "\n",
    "x_vals = np.arange(-20, 20.1, 0.1)\n",
    "y_vals_1 = sigma(x_vals, 1)\n",
    "y_vals_03 = sigma(x_vals, 0.3)\n",
    "y_vals_10 = sigma(x_vals, 10)\n",
    "\n",
    "plt.figure(figsize=(10,4)); plt.gca().set_aspect(10)\n",
    "plt.plot(x_vals, y_vals_1, color='blue', label='a = 1')\n",
    "plt.plot(x_vals, y_vals_03, color='red', label='a = 0.3')\n",
    "plt.plot(x_vals, y_vals_10, color='green', label='a = 10')\n",
    "plt.xlabel('x'); plt.ylabel('sigma'); \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It can be scaled by the choice of $a$, and also translated by $x_0$: \n",
    "\n",
    "$$\n",
    "\\sigma(x \\mid a, x_0) = \\frac{1}{1+\\exp\\big(-a(x-x_0)\\big)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigma(x, a, x0):\n",
    "    return (1+np.exp(-a*(x-x0)))**(-1)\n",
    "\n",
    "x_vals = np.arange(-20, 20.1, 0.1)\n",
    "y_vals_0 = sigma(x_vals, 1, 0)\n",
    "y_vals_5 = sigma(x_vals, 0.3, 5)\n",
    "y_vals_m10 = sigma(x_vals, 10, -10)\n",
    "\n",
    "plt.figure(figsize=(10,4)); plt.gca().set_aspect(10)\n",
    "plt.plot(x_vals, y_vals_0, color='blue', label='a = 1')\n",
    "plt.plot(x_vals, y_vals_5, color='red', label='a = 0.3')\n",
    "plt.plot(x_vals, y_vals_m10, color='green', label='a = 10')\n",
    "plt.xlabel('x'); plt.ylabel('sigma'); \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In 2D we can write this kind of thing\n",
    "\n",
    "$$\n",
    "\\sigma(x_1, x_2 \\mid a, b, c) = \\frac{1}{1+\\exp\\big(-(a x_1+b x_2+c)\\big)}\n",
    "$$\n",
    "\n",
    "For example, with $a=-1$, $b=1$ and $c=0$ we have,\n",
    "\n",
    "$$\n",
    "\\sigma(x_1, x_2 \\mid -1, 1, 0) = \\frac{1}{1+\\exp\\big(-(x_2-x_1)\\big)}\n",
    "$$\n",
    "\n",
    "Now, along the line $x-1 = x_2$ we have $\\sigma(x_1, x_2 \\mid -1, 1, 0)=0.5$.\n",
    "\n",
    "To one side of the line $\\sigma \\to 0$ and to the other side of the line \n",
    "$\\sigma \\to 1$. This means we have a switch, or a signal, that we can think\n",
    "of as being off on one side of the line and switched on if we are on \n",
    "the other side.\n",
    "\n",
    "This is **binary classification**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Define dimensions\n",
    "Nx, Ny, Nz = 10, 10, 1\n",
    "X, Y, Z = np.meshgrid(np.arange(-2,Nx,0.1), np.arange(-2,Ny,0.1), np.arange(Nz))\n",
    "\n",
    "# Create sigmoid data\n",
    "sigmoid = (1+np.exp(-(Y-X)))**(-1)\n",
    "# plot surface\n",
    "ax.plot_surface(X[:, :, 0], Y[:, :, 0], sigmoid[:, :, 0], cmap=cm.Wistia, alpha=0.75)\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_zlabel('logistic values')\n",
    "ax.view_init(30, 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Iris Data Set\n",
    "\n",
    "We are going to illustrate the idea behind logistic regression using the Iris Data Set.\n",
    "See, for example, <https://en.wikipedia.org/wiki/Iris_flower_data_set> for the details.\n",
    "\n",
    "\n",
    "This is very well known. Something that any aspiring Data Scientist ought to be aware of.\n",
    "\n",
    "It is not without controversy though. It was used by Fisher, and Fisher was associated with\n",
    "**eugenics**. You can read more about that here:\n",
    "\n",
    "<https://www.nature.com/articles/s41437-020-00394-6>\n",
    "\n",
    "If this offends you then you can replace the use of the Iris data below with \n",
    "the penguins data we have been using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load it in and take a look\n",
    "sns.get_dataset_names()\n",
    "dfi = sns.load_dataset('iris')\n",
    "dfi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(dfi, hue='species', height = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we want a binary classifier so we drop the virginica data\n",
    "dfid = dfi[ (dfi['species'] != 'virginica') == True ]\n",
    "sns.pairplot(dfid, hue='species', height = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's use petal length and sepal width as our features\n",
    "xall = dfid.iloc[:,[1,2]].values\n",
    "# and species as our label\n",
    "yall = dfid.iloc[:, 4].values\n",
    "print(xall[0:5,:], yall[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# select the setosa and versicolor feature rows\n",
    "x_set = xall[yall == 'setosa',:] \n",
    "x_ver = xall[yall == 'versicolor',:]\n",
    "# set the vertical coordinate for the 3D surface plot\n",
    "z_set = (1+np.exp(-(x_set[:,1]-x_set[:,0]) ))**(-1)\n",
    "z_ver = (1+np.exp(-(x_ver[:,1]-x_ver[:,0]) ))**(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6)) # some of this set up was done above\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot_surface(X[:, :, 0], Y[:, :, 0], sigmoid[:, :, 0], cmap=cm.Wistia, alpha=0.75)\n",
    "ax.scatter(x_set[:,0], x_set[:,1], z_set, c='black', marker='o')\n",
    "ax.scatter(x_ver[:,0], x_ver[:,1], z_ver, c='red', marker='o')\n",
    "ax.set_xlabel('sepal width'); ax.set_ylabel('petal length')\n",
    "ax.set_zlabel('logistic values'); ax.view_init(10, 10); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion\n",
    "\n",
    "What you see here is that the data are being separated by the sigmoid,\n",
    "or logistic, function's **ramp** from $0$ to $1$. \n",
    "\n",
    "The idea behind **logistic regression** is to use training data to \n",
    "determine a line $ax_1+bx_2+c=0$ so that  \n",
    "\n",
    "$$\n",
    "\\sigma(x_1, x_2 \\mid a, b, c) = \\frac{1}{1+\\exp\\big(-(a x_1+b x_2+c)\\big)}\n",
    "$$\n",
    "\n",
    "can be used as a classifier. The line is then a **decision boundary**.\n",
    "\n",
    "For example, for input feature values\n",
    "$x_1$ and $x_2$, and for $a$, $b$ and $c$ determined by the regression,\n",
    "we would classify as:\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\sigma(x_1, x_2 \\mid a, b, c) \n",
    "\\ge 0.5 & (x_1, x_2)\\text{ indicate Class 1 (e.g. versicolor)};\n",
    "\\\\\n",
    "\\sigma(x_1, x_2 \\mid a, b, c) \n",
    "< 0.5 & (x_1, x_2)\\text{ indicate Class 2 (e.g. setosa)};\n",
    "\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Technical Details\n",
    "\n",
    "Suppose we use $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\theta_2)^T$ with\n",
    "$\\boldsymbol{x} = (1, x_1, x_2)^T$ to represent the decision boundary with\n",
    "$\\boldsymbol{\\theta}^T\\boldsymbol{x}=0$. Then, with\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1+\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}\n",
    "$$\n",
    "\n",
    "we get \n",
    "\n",
    "$$\n",
    "1-p\n",
    "= 1 - \\frac{1}{1+\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}\n",
    "= \\frac{1+\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}{1+\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}\n",
    "- \\frac{1}{1+\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}\n",
    "= \\frac{\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}{1+\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\frac{p}{1-p}\n",
    "= \\frac{1+\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}{\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}\n",
    "\\frac{1}{1+\\exp(-\\boldsymbol{\\theta}^T\\boldsymbol{x})}\n",
    "=\\exp(\\boldsymbol{\\theta}^T\\boldsymbol{x})\n",
    "$$\n",
    "\n",
    "and $0< p < 1$ - so $p$ is a probability and $p/(p-1)$ the **odds**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The log-odds\n",
    "\n",
    "Taking the log of both sides \n",
    "\n",
    "$$\n",
    "\\ln\\left(\\frac{p}{1-p}\\right)\n",
    "=\\ln\\exp(\\boldsymbol{\\theta}^T\\boldsymbol{x})\n",
    "$$\n",
    "\n",
    "Therefore, simplifying,\n",
    "\n",
    "$$\n",
    "\\ln\\left(\\frac{p}{1-p}\\right)\n",
    "=\\boldsymbol{\\theta}^T\\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "and there is the connection to linear regression. We fit the straight line\n",
    "to the log-odds which, in turn, are estimated from the training data.\n",
    "\n",
    "This can be done in `sklearn`. The details are here: \n",
    "\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>\n",
    "\n",
    "along with an actual example on the Iris data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Closing Remarks\n",
    "\n",
    "We just covered a great deal. We finish with a couple of remarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The $p>n$ Issue\n",
    "\n",
    "We discussed the fact that $\\boldsymbol{X}^T\\boldsymbol{X}$ is\n",
    "invertible if $\\boldsymbol{X}$ has full column rank\n",
    "\n",
    "For a discussion and proof see Page 149 of *Linear Algebra for Everyone*, \n",
    "by Gilbert Strang, Wellesley Cambridge Press, 2020. \n",
    "\n",
    "This doesn't mean that we can't minimize our cost\n",
    "$\\mathcal{E}(\\boldsymbol{\\theta})=\\Vert\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\Vert_2^2$ though. \n",
    "\n",
    "But it may mean that the minimizer is not unique.\n",
    "\n",
    "The situation is very technical - and beyond our scope.\n",
    "\n",
    "[IPDS, Chapter 7.1.4] has an excellent discussion of this and you are referred there if you\n",
    "want to see the mathematical details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias-Variance Decomposition and Trade-Off\n",
    "\n",
    "Consider a model that produces a random variable $z$ as an estimate of an unknown\n",
    "$z_0$. We would like $\\bar{z}$ - the mean of $z$ - to be $z_0$. \n",
    "\n",
    "- We call $\\bar{z}-z_0$ the **bias** in the model.\n",
    "\n",
    "Further, the random variable will have a variance, \n",
    "\n",
    "- $\\text{Var}(z) = \\mathbb{E}\\Big((z-\\bar{z})^2\\Big)$\n",
    "\n",
    "We have been using the squared error as a cost function. What is its expected value?\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\Big((z-z_0)^2\\Big)\n",
    "& = \\mathbb{E}\\Big(\\big((z-\\bar{z})+(\\bar{z}-z_0)\\big)^2\\Big),\n",
    "\\\\\n",
    "& = \\mathbb{E}\\Big((z-\\bar{z})^2+2(z-\\bar{z})(\\bar{z}-z_0)+(\\bar{z}-z_0)^2\\Big).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using **linearity of expectation**\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\Big((z-z_0)^2\\Big)\n",
    "& = \\mathbb{E}\\Big((z-\\bar{z})^2\\Big)\n",
    "+2(\\bar{z}-z_0)\\mathbb{E}(z-\\bar{z})\n",
    "+\\mathbb{E}\\Big((\\bar{z}-z_0)^2\\Big),\n",
    "\\\\\n",
    "& = \\underbrace{\\mathbb{E}\\Big((z-\\bar{z})^2\\Big)}_{\\mathrm{variance}}\n",
    "+2(\\bar{z}-z_0)\\underbrace{\\mathbb{E}(z-\\bar{z})}_{\\mathrm{zero}}\n",
    "+\\underbrace{(\\bar{z}-z_0)^2}_{\\mathrm{bias}^2}.\n",
    "\\end{align*}\n",
    "\n",
    "Hence $\\mathbb{E}\\Big((z-z_0)^2\\Big)\n",
    "= (\\bar{z}-z_0)^2 + \\mathbb{E}\\Big((z-\\bar{z})^2\\Big)\n",
    "= \\mathrm{bias}^2 + \\mathrm{variance}$.\n",
    "\n",
    "What this means is that for small MSE cost we need to control both\n",
    "the bias and the variance. \n",
    "\n",
    "Typically, driving down the bias will mean increasing the variance and \n",
    "*vice versa*: there is a **trade off**.\n",
    "\n",
    "See [IPDS, Theorem 7.5] and, in particular, [MLFCES, Chapter 4.4] for much\n",
    "higher quality discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "We covered *just enough*, to make *progress at pace*. We looked at\n",
    "\n",
    "- Polynomial Regression using a least squares cost function.\n",
    "- Ordinary Least Squares as well as regularized.\n",
    "- Multivariable Regression.\n",
    "- Logistic Regression.\n",
    "\n",
    "Now we can start putting all of this material to work.\n",
    "\n",
    "### Homework\n",
    "\n",
    "In the next session we will briefly discuss **Support Vector Machines** (SVM's) and \n",
    "quickly move to to discuss the **perceptron**. \n",
    "\n",
    "Our SVM treatment will be very shallow - to prepare yourself for this your homework\n",
    "is to read\n",
    "<https://www.syncfusion.com/succinctly-free-ebooks/support-vector-machines-succinctly>\n",
    "up page 43 prior to the next session. This isn't as much as it seems: pages 1-20 are\n",
    "a revision of the vector material we have already covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Technical Notes, Production and Archiving\n",
    "\n",
    "Ignore the material below. What follows is not relevant to the material being taught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Production Workflow\n",
    "\n",
    "- Finalise the notebook material above\n",
    "- Clear and fresh run of entire notebook\n",
    "- Create html slide show:\n",
    "  - `jupyter nbconvert --to slides 11_regress.ipynb `\n",
    "- Set `OUTPUTTING=1` below\n",
    "- Comment out the display of web-sourced diagrams\n",
    "- Clear and fresh run of entire notebook\n",
    "- Comment back in the display of web-sourced diagrams\n",
    "- Clear all cell output\n",
    "- Set `OUTPUTTING=0` below\n",
    "- Save\n",
    "- git add, commit and push to FML\n",
    "- copy PDF, HTML etc to web site\n",
    "  - git add, commit and push\n",
    "- rebuild binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some of this originated from\n",
    "\n",
    "<https://stackoverflow.com/questions/38540326/save-html-of-a-jupyter-notebook-from-within-the-notebook>\n",
    "\n",
    "These lines create a back up of the notebook. They can be ignored.\n",
    "\n",
    "At some point this is better as a bash script outside of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NBROOTNAME=11_regress\n",
    "OUTPUTTING=0\n",
    "\n",
    "if [ $OUTPUTTING -eq 1 ]; then\n",
    "  jupyter nbconvert --to html $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.html ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.html\n",
    "  mv -f $NBROOTNAME.html ./formats/html/\n",
    "\n",
    "  jupyter nbconvert --to pdf $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.pdf ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.pdf\n",
    "  mv -f $NBROOTNAME.pdf ./formats/pdf/\n",
    "\n",
    "  jupyter nbconvert --to script $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.py ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.py\n",
    "  mv -f $NBROOTNAME.py ./formats/py/\n",
    "else\n",
    "  echo 'Not Generating html, pdf and py output versions'\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
