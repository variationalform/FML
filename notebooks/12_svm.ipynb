{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "#### *variationalform* <https://variationalform.github.io/>\n",
    "\n",
    "#### *Just Enough: progress at pace*\n",
    "\n",
    "<https://variationalform.github.io/>\n",
    "\n",
    "<https://github.com/variationalform>\n",
    "\n",
    "Simon Shaw\n",
    "<https://www.brunel.ac.uk/people/simon-shaw>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<p>\n",
    "This work is licensed under CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)\n",
    "\n",
    "<p>\n",
    "Visit <a href=\"http://creativecommons.org/licenses/by-sa/4.0/\">http://creativecommons.org/licenses/by-sa/4.0/</a> to see the terms.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>This document uses python</td>\n",
    "<td>\n",
    "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>and also makes use of LaTeX </td>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/LaTeX_logo.svg/320px-LaTeX_logo.svg.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>in Markdown</td> \n",
    "<td>\n",
    "<img src=\"https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What this is about:\n",
    "\n",
    "- Binary Classification.\n",
    "\n",
    "- Linear separability.\n",
    "\n",
    "- Separating planes, hyperplanes: **decision boundaries**.\n",
    "\n",
    "- Support Vectors, and SVM (Support Vector Machine) classification.\n",
    "\n",
    "\n",
    "As usual our emphasis will be on *doing* rather than *proving*:\n",
    "*just enough: progress at pace*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this material you are recommended Pages 1-43 of [SVMS] - **this was set as homework**. \n",
    "Also recommended is Chapter 12 of [MML] and, less so, Chapter 8.5 of [MLFCES].\n",
    "\n",
    "- SVMS: Support Vector Machines Succinctly by Alexandre Kowalczyk.\n",
    "  <https://www.syncfusion.com/succinctly-free-ebooks/support-vector-machines-succinctly>\n",
    "- MML: Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong.\n",
    "  Cambridge University Press. <https://mml-book.github.io>.\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists, by Andreas Lindholm,\n",
    "  Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön. Cambridge University Press. \n",
    "  <http://smlbook.org>.\n",
    "\n",
    "These can be accessed legally and without cost.\n",
    "\n",
    "There are also these useful references for coding:\n",
    "\n",
    "- PT: `python`: <https://docs.python.org/3/tutorial>\n",
    "- NP: `numpy`: <https://numpy.org/doc/stable/user/quickstart.html>\n",
    "- MPL: `matplotlib`: <https://matplotlib.org>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "In the last session we moved from classification to regression and then, with logistic \n",
    "regression, we moved back to classification again. \n",
    "\n",
    "In this session we will continue with the classification theme and briefly discuss\n",
    "**Support Vector Machines** (SVM's). We will then be able to move quickly on to\n",
    "discuss the **perceptron** which will set us up for **deep neural networks**.\n",
    "\n",
    "In the homework you were asked to read\n",
    "<https://www.syncfusion.com/succinctly-free-ebooks/support-vector-machines-succinctly>\n",
    "up page 43. Pages 1-20 are a revision of the vector material we have already covered.\n",
    "\n",
    "The following material assumes familiarity with that source.\n",
    "\n",
    "We're going to start with the Iris Data Set, with the **virginica** data removed - just\n",
    "as with the logistic regression example in the last session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.get_dataset_names()\n",
    "dfi = sns.load_dataset('iris')\n",
    "dfi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(dfi, hue='species', height = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want a binary classifier so we drop the virginica data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we want a binary classifier, so drop the virginica data\n",
    "dfid = dfi[ (dfi['species'] != 'virginica') == True ]\n",
    "# this leave just two classes.\n",
    "sns.pairplot(dfid, hue='species', height = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, we focus on the well separated petal length and sepal width for our features.\n",
    "\n",
    "We'll use them to predict species: setosa or versicolor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# we use petal length and sepal width as our features\n",
    "X = dfid.iloc[:,[1,2]].values\n",
    "# and species as our label\n",
    "y = dfid.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's plot them in different colours - find the species array index sets\n",
    "indxS = np.where(y == 'setosa')[0]\n",
    "indxV = np.where(y != 'setosa')[0]\n",
    "ax = plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[indxS,0], X[indxS,1], color='blue')\n",
    "plt.scatter(X[indxV,0], X[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We want a decision boundary - in this case it will be a straight line that separates \n",
    "the classes. Then we can classify new data accrding to which side of the line it\n",
    "appears. This is just like we described with **Logistic Regression**.\n",
    "\n",
    "Note that we can clearly see that it is possible to separate the classes with a single\n",
    "line. Data sets for which this is true are called **linearly separable**.\n",
    "\n",
    "You can read more about this concept in, for example, [MML, Chapter 12]. Data sets \n",
    "which cannot be seprated in this way are more difficult to work and may required\n",
    "so-called kernel methods. They are beyond our scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Back to our data. Let's plot a few lines - **which one would you prefer to use?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(6,4))\n",
    "plt.scatter(X[indxS,0], X[indxS,1], color='blue')\n",
    "plt.scatter(X[indxV,0], X[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length')\n",
    "plt.plot([1,5],[1,5],':m', label='line 1')\n",
    "plt.plot([0,5],[1,3],':c', label='line 2')\n",
    "plt.plot([2,4],[1,5],':g', label='line 3')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Which one now?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(6,4))\n",
    "plt.scatter(X[indxS,0], X[indxS,1], color='blue')\n",
    "plt.scatter(X[indxV,0], X[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length')\n",
    "plt.plot([1,5],[1,5],':m', label='line 1')\n",
    "plt.plot([0,5],[1,3],':c', label='line 2')\n",
    "plt.plot([2,4],[1,5],':g', label='line 3')\n",
    "plt.plot([0,5],[.85,3.55],'-.k', label='line 4')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Let's explore this...** Does this give the widest gap? Or maximum margin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(6,4))\n",
    "plt.scatter(X[indxS,0], X[indxS,1], color='blue')\n",
    "plt.scatter(X[indxV,0], X[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length')\n",
    "plt.plot([0,5],[.85,3.55],':k')\n",
    "plt.plot([0,5],[0.85+0.79,3.55+0.79],':k')\n",
    "plt.plot([0,5],[0.85-0.79,3.55-0.79],':k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Margin\n",
    "\n",
    "The previous picture illustrates the central concept behind the SVM\n",
    "(**Support Vector Machine**) classifier.\n",
    "\n",
    "The idea is to find the **decision boundary** as the mid-line \n",
    "between the two parallel separating lines that are as far apart as possible.\n",
    "This separating gap is called the **separating margin**.\n",
    "\n",
    "We aim to find the **maximum separating margin** using the training data\n",
    "and then, because the gap is as wide as possible, we hope that all of the\n",
    "unseen test and future data will fall the correct side of the \n",
    "decision boundary.\n",
    "\n",
    "This method is appropriate for **linearly separable data** as described \n",
    "in the homework reading [SVMS],\n",
    "<https://www.syncfusion.com/succinctly-free-ebooks/support-vector-machines-succinctly>\n",
    "on page 21 onwards. The use of **kernels** can make this method applicable to\n",
    "non-linearly separable data but that is an advanced topic and beyond our scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The General Set-Up\n",
    "\n",
    "We illustrated the maximum margin with 2D data. In general though our data will lie in\n",
    "higher dimensional space, and for that we will need more than 1D lines to describe the\n",
    "separating margin.\n",
    "\n",
    "If the data were in 3D then we would separate it using the 2D version of a\n",
    "line, which is a plane.\n",
    "\n",
    "Think of a two adjacent rooms - the wall between is\n",
    "like a plane separating the larger combined space into the two rooms. If\n",
    "we used just a line then it would be like a thread of cotton running from\n",
    "one wall to another - this would not make it clear how the room is divided \n",
    "into two.\n",
    "\n",
    "Mental images and intuition fail us when the data live in higher dimensional\n",
    "space. But maths saves use - we use **hyperplanes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Points, Lines, Planes and Hyperplanes\n",
    "\n",
    "In two dimensions we are very familiar with the idea of a straight line\n",
    "having equation $y=mx+c$ where $m$ is called the gradient and $c$ the\n",
    "$y$-intercept.\n",
    "\n",
    "These are useful but for us it will be more useful to write the vector\n",
    "$\\boldsymbol{x}=(x_1, x_2)^T$ in place of $(x,y)$ and define our\n",
    "straight line by the equation\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{x} = \\phi\n",
    "$$\n",
    "\n",
    "for given **weights**, $\\boldsymbol{w} = (w_1, w_2)^T$, and\n",
    "some constant $\\phi$.\n",
    "\n",
    "For example, the equation $y=\\frac{4}{5}x+7$ becomes\n",
    "\n",
    "$$\n",
    "x_2 = \\frac{4}{5}x_1+7 \n",
    "\\quad\\Longrightarrow\\quad\n",
    "-4x_1+5x_2 = 35 \n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{x} = \\phi\n",
    "$$\n",
    "\n",
    "for $\\boldsymbol{w}=(-4,5)^T$ and $\\phi=35$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $\\boldsymbol{x}_a$ and $\\boldsymbol{x}_b$ be two distinct points on\n",
    "the line - as represented by vectors pointing from the origin and ending\n",
    "on the line - then,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{x}_a - \\boldsymbol{w}\\cdot\\boldsymbol{x}_b\n",
    "= \\phi - \\phi = 0\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\boldsymbol{w}\\cdot(\\boldsymbol{x}_a - \\boldsymbol{x}_b) = 0.\n",
    "$$\n",
    "\n",
    "> **THINK ABOUT:** what does this mean geometrically? What is the angle\n",
    "> between the line and the vector $\\boldsymbol{w}$?\n",
    "\n",
    "In more than two dimensions, in $\\mathbb{R}^n$ say, we would have\n",
    "$\\boldsymbol{x}= (x_1, x_2, \\ldots, x_n)^T$ and a vector of *weights*\n",
    "$\\boldsymbol{w}$ for which\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{x} = \\phi \n",
    "$$\n",
    "\n",
    "just as above. If $\\boldsymbol{x}= (x_1, x_2, x_3)^T$ then this is\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}_1\\boldsymbol{x}_1\n",
    "+\\boldsymbol{w}_2\\boldsymbol{x}_2\n",
    "+\\boldsymbol{w}_3\\boldsymbol{x}_3\n",
    "=\\phi\n",
    "$$\n",
    "\n",
    "which defines a *plane* - an infinite flat surface. In higher\n",
    "dimensions we call it a *hyperplane*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose a point $Q$ is described by the vector $\\boldsymbol{q}$ rooted\n",
    "at the origin. We can ask: **What is the shortest distance from this point\n",
    "to the hyperplane?**\n",
    "\n",
    "To answer this imagine you are standing on one side of a busy road and\n",
    "want to cross to the other side. You would want to minimise the time you\n",
    "spend on the road and so would choose to cross by moving at a right\n",
    "angle to the road.\n",
    "\n",
    "That is just the case with our point $Q$. The shortest distance to the\n",
    "hyperplane is along the path that meets the plane at a right angle in\n",
    "all directions.\n",
    "\n",
    "> It's like a tall tree growing vertically from the plane ground. However:\n",
    "> this is just an analogy, not a plug for **flat earth theory**.\n",
    "\n",
    "To figure out this distance we move from the origin to $Q$, by moving to\n",
    "the end of the vector $\\boldsymbol{q}$. We then move a distance $d$\n",
    "along a line parallel to $\\boldsymbol{w}$ in the direction towards the plane.\n",
    "\n",
    "$$\n",
    "\\text{This takes us to the point}\\qquad\n",
    "\\boldsymbol{y} = \\boldsymbol{q} + \\frac{d\\,\\boldsymbol{w}}{\\Vert\\boldsymbol{w}\\Vert_2}.\n",
    "$$\n",
    "\n",
    "> **THINK ABOUT:** why is the length of $\\boldsymbol{w}$ in the denominator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we insist that the point $\\boldsymbol{y}$ is actually a point\n",
    "$\\boldsymbol{x}$ on the hyperplane, then\n",
    "$\\boldsymbol{w}^T\\boldsymbol{x}=\\phi$ and\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{x}\\qquad\\text{ means }\\qquad\n",
    "\\phi\n",
    "= \n",
    "\\boldsymbol{w}^T\\boldsymbol{y} = \n",
    "\\boldsymbol{w}^T\\boldsymbol{x} =\n",
    "\\boldsymbol{w}^T\\boldsymbol{q} + d\\frac{\\boldsymbol{w}^T\\boldsymbol{w}}{\\Vert\\boldsymbol{w}\\Vert_2}\n",
    "= \n",
    "\\boldsymbol{w}^T\\boldsymbol{q} + d\\Vert\\boldsymbol{w}\\Vert_2.\n",
    "$$\n",
    "\n",
    "The shortest distance $d$ from $Q$ to the hyperplane is then\n",
    "\n",
    "$$\n",
    "d = \\frac{\\phi-\\boldsymbol{w}^T\\boldsymbol{q}}{\\Vert\\boldsymbol{w}\\Vert_2}.\n",
    "$$\n",
    "\n",
    "Note that this distance is signed. See also [SVMS, page 46] for an alternative derivaiton.\n",
    "\n",
    "We've already seen why this is important in data science with the earlier\n",
    "**Iris Data Set** example.\n",
    "The situation is ubiquitous. Here's another example of why this geometrical \n",
    "view is relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen how data sets comprise of rows of observations with each \n",
    "having several characteristics or **features**.\n",
    "\n",
    "Each stock in the FTSE 100 for example has a\n",
    "daily high, a daily low, percentage change, a yield, a volume traded,\n",
    "market capitalization and so on. Each item can therefore be thought of\n",
    "as a point in high dimensional space. For example, for a given stock,\n",
    "the six quantities, or *features*, just listed could be represented by\n",
    "this *feature vector*,\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{r}\n",
    "x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6\n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\left(\n",
    "\\begin{array}{r}\n",
    "\\text{daily high} \\\\\n",
    "\\text{daily low} \\\\\n",
    "\\text{percentage change} \\\\\n",
    "\\text{yield} \\\\\n",
    "\\text{volume traded} \\\\\n",
    "\\text{market cap.}\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen with our Iris data above that a way to classify such data\n",
    "is to try and separate these points into two distinct groups.\n",
    "\n",
    "If we can do that we have in some sense reduced our\n",
    "mass of data down to two essential clusters.\n",
    "\n",
    "We can do that by trying to find a hyperplane that passes between the\n",
    "two clusters. We would want the hyperplane to be as far away from all of\n",
    "the points as possible so that it makes a clear distinction between the\n",
    "two clusters.\n",
    "\n",
    "This would mean finding $d$ for each point, each feature vector, and\n",
    "making sure that the minimum value of all such $d$-values is as large as\n",
    "possible.\n",
    "\n",
    "> **THINK ABOUT:** can you sketch this situation in 2D? Does this seem\n",
    "> to be a difficult thing to do in general?\n",
    "\n",
    "Fortunately we only have to understand this in the simplest case. We\n",
    "will be using software to solve the 'real' problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization Problem\n",
    "\n",
    "We now have a mathematical expression for the signed distance $d$ between a point \n",
    "$\\boldsymbol{q}$ and the hyperplane $\\boldsymbol{w}^T\\boldsymbol{x}=\\phi$.\n",
    "\n",
    "The two classes in the training data are labelled as positive or negative.\n",
    "A data point in the positive class is labelled as $y=+1$ and a point in \n",
    "the negative class is labelled as $y=-1$.\n",
    "\n",
    "The negative class corresponds to negative distances, $d$.\n",
    "\n",
    "We imagine visiting every training data point $\\boldsymbol{q}_k$, for $k=1,\\ldots,N$\n",
    "(say there are $N$ in total), finding the label $y_k$ and distance\n",
    "$d_k$, and calculating $D_k = d_k y_k \\ge 0$. Then:\n",
    "\n",
    "**Determine $\\phi$ and $\\boldsymbol{w}$ such that \n",
    "the minimum of $D_k$ is maximized.**\n",
    "\n",
    "This **optimization problem** is mathematically very challenging. See \n",
    "[SVMS, Chapter 4] and [MML, Chapter 12] if you want some insight as to why.\n",
    "\n",
    "We'll use software..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM using `sklearn`\n",
    "\n",
    "Recall that we already set up `X` and `y` for the petal length and sepal width\n",
    "iris data above with\n",
    "\n",
    "```\n",
    "X = dfid.iloc[:,[1,2]].values\n",
    "y = dfid.iloc[:, 4].values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfid.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Support Vector Machine, or SVM, classification method uses the setting described\n",
    "above to determine a maximum separating margin from training data. The mid-plane (in \n",
    "general hyperplane) is then a decision boundary between the two classes.\n",
    "\n",
    "The rationale for this approach is that because the separation is maximized, we expect\n",
    "that even unseen test and future data will fall on the correct side of the decision\n",
    "boundary. This will be enough for correct classification - even though it may stray into\n",
    "the separating margin.\n",
    "\n",
    "We'll use the `sklearn` SVM capability: <https://scikit-learn.org/stable/modules/svm.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We'll use 50% of the data to test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Should we scale the data? On the face of it, it seems that it isn't needed.\n",
    "We'll use a `python` `if` statement to leave the option open.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  scaler = StandardScaler()\n",
    "  # initialise the scaler by feeding it the training data\n",
    "  scaler.fit(X_train)\n",
    "  # now carry out the transformation of all of the feature data\n",
    "  X_train = scaler.transform(X_train)\n",
    "  X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Toggle `if False:` and `if True:` to switch scaling off/on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import the SVM classifier\n",
    "from sklearn import svm\n",
    "# instance it\n",
    "svmclf = svm.SVC(kernel='linear')\n",
    "# and fit the training data\n",
    "svmclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "y_pred = svmclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get the confusion matrix and accuracy data\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "accsc = accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy:\", accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot a nicer confusion matrix\n",
    "cmplot = ConfusionMatrixDisplay(cm, display_labels=svmclf.classes_)\n",
    "cmplot.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is perfect - but it ought to be because the original data were well separated.\n",
    "\n",
    "Let's look at the data and the SVM in pictures - we will get more insight.\n",
    "\n",
    "we'll start with the training data - these are the data points the SVM used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indxS = np.where(y_train == 'setosa')[0]\n",
    "indxV = np.where(y_train != 'setosa')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train[indxS,0], X_train[indxS,1], color='blue')\n",
    "plt.scatter(X_train[indxV,0], X_train[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The SVM classifier provides **support vectors** ... These are not easy to explain. \n",
    "The optimization process determines a subset of the data set that can be used to\n",
    "determine the maximum separating margin. Those points in the subset are called \n",
    "**support vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get support vectors\n",
    "SVecs = svmclf.support_vectors_\n",
    "print('The (transposed) support vectors are:\\n', SVecs.T)\n",
    "# get number of support vectors for each class\n",
    "NumSVecs = svmclf.n_support_\n",
    "print('There are these many per class', NumSVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The support vectors tell us which data points are forming the margin.\n",
    "(It's actually a bit more complicated than that - we'll come back to this below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train[indxS,0], X_train[indxS,1], color='blue')\n",
    "plt.scatter(X_train[indxV,0], X_train[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length');\n",
    "# plot the support vectors\n",
    "for k in range(NumSVecs.sum()):\n",
    "  plt.plot([0,SVecs[k,0]],[0,SVecs[k,1]],'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The SVM classifier can also tell us the equation of the hyperplane decision boundary.\n",
    "\n",
    "It has the form $ax_1 + bx_2 + c = 0$ where..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# a x1 + b x2 + c = 0\n",
    "a = svmclf.coef_[0,0]\n",
    "b = svmclf.coef_[0,1]\n",
    "c = svmclf.intercept_[0]\n",
    "print(f'a={a}, b={b}, c={c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's add this to the plot using $x_2 = -(a x_1 + c)/b$ (which requires $b\\ne 0$).\n",
    "\n",
    "We can set up two points and join them with a straight line:\n",
    "\n",
    "$$\n",
    "P_1:\\ (x_1, x_2) = (0, -c/b)\n",
    "\\qquad\\text{ and }\\qquad\n",
    "P_2:\\ (x_1, x_2) = (6, -(6a + c)/b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "P1 = np.array([0, -c/b]); P2=np.array([6, -(6*a + c)/b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train[indxS,0], X_train[indxS,1], color='blue')\n",
    "plt.scatter(X_train[indxV,0], X_train[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length');\n",
    "# plot the support vectors\n",
    "for k in range(NumSVecs.sum()):\n",
    "  plt.plot([0,SVecs[k,0]],[0,SVecs[k,1]],':k')\n",
    "# the decision boundary\n",
    "plt.plot([P1[0],P2[0]],[P1[1],P2[1]],'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also draw parallel lines through the support vector data points.\n",
    "\n",
    "In general, with the decision boundary given by $ax_1 + bx_2 +c = 0$,\n",
    "a parallel line passing through $\\boldsymbol{q}=(q_1, q_2)^T$ satisfies\n",
    "\n",
    "$$\n",
    "a(x_1 - q_1) + b(x_2 - q_2) = 0\n",
    "$$\n",
    "\n",
    "because $a$ and $b$ control the gradient and so must be the same.\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "a x_1 + b x_2 + c_q = 0 \n",
    "\\quad\\text{where}\\quad\n",
    "c_q = - a q_1 - bq_2.\n",
    "$$\n",
    "\n",
    "We can add the parallel margin edges to the plot with this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train[indxS,0], X_train[indxS,1], color='blue')\n",
    "plt.scatter(X_train[indxV,0], X_train[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length');\n",
    "plt.plot([P1[0],P2[0]],[P1[1],P2[1]],'k')\n",
    "for k in range(NumSVecs.sum()):\n",
    "  plt.plot([0,SVecs[k,0]],[0,SVecs[k,1]],':k')\n",
    "  q = np.array([SVecs[k,0],SVecs[k,1]])\n",
    "  cq = -a*q[0]-b*q[1]; P1[1]=-(0*a + cq)/b; P2[1]=-(6*a + cq)/b\n",
    "  plt.plot([P1[0],P2[0]],[P1[1],P2[1]],'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***NOTE***\n",
    "\n",
    "The code above may seem a bit overwhelming but in the end we are just identifying\n",
    "points and joining them up with straight lines. \n",
    "\n",
    "The most important thing to gain from this are the pictures, for they explain how\n",
    "the SVM classifier works.\n",
    "\n",
    "Let's plot the same pictures, but with the predicted values from the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indxS = np.where(y_pred == 'setosa')[0]\n",
    "indxV = np.where(y_pred != 'setosa')[0]\n",
    "P1 = np.array([0, -c/b]); P2=np.array([6, -(6*a + c)/b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_test[indxS,0], X_test[indxS,1], color='blue')\n",
    "plt.scatter(X_test[indxV,0], X_test[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length');\n",
    "plt.plot([P1[0],P2[0]],[P1[1],P2[1]],'k')\n",
    "for k in range(NumSVecs.sum()):\n",
    "  plt.plot([0,SVecs[k,0]],[0,SVecs[k,1]],':k')\n",
    "  q = np.array([SVecs[k,0],SVecs[k,1]])\n",
    "  cq = -a*q[0]-b*q[1]; P1[1]=-(0*a + cq)/b; P2[1]=-(6*a + cq)/b\n",
    "  plt.plot([P1[0],P2[0]],[P1[1],P2[1]],'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is working well because we have very highly separated data.\n",
    "\n",
    "Let's go back to the data set and pick another pair of features for which the \n",
    "separation is not as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(dfid, hue='species', height = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's choose *sepal width* and *sepal length*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfid.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we use sepal length and sepal width as our features\n",
    "X = dfid.iloc[:,[1,0]].values\n",
    "# and species as our label\n",
    "y = dfid.iloc[:, 4].values\n",
    "\n",
    "# We'll use 50% of the data to test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "# instance the SVM - C affects the 'goodness' of the decision boundary\n",
    "svmclf = svm.SVC(kernel='linear', C=100)\n",
    "# and fit the training data\n",
    "svmclf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = svmclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# get the confusion matrix and accuracy data\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "accsc = accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy:\", accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot a nicer confusion matrix\n",
    "cmplot = ConfusionMatrixDisplay(cm, display_labels=svmclf.classes_)\n",
    "cmplot.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are a lot more support vectors this time. These are the ones\n",
    "used in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indxS = np.where(y_train == 'setosa')[0]\n",
    "indxV = np.where(y_train != 'setosa')[0]\n",
    "# get support vectors and number of them for each class\n",
    "NumSVecs = svmclf.n_support_\n",
    "print('There are these many per class', NumSVecs)\n",
    "SVecs = svmclf.support_vectors_\n",
    "print('The transposed support vectors are:\\n', SVecs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can get the equation of the decision boundary as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# a x1 + b x2 + c = 0 => x2 = -(a x1 + c)/b\n",
    "print('svmclf.intercept_ = ', svmclf.intercept_.shape)\n",
    "a = svmclf.coef_[0,0]\n",
    "b = svmclf.coef_[0,1]\n",
    "c = svmclf.intercept_[0]\n",
    "print(f'a={a}, b={b}, c={c}')\n",
    "\n",
    "P1 = np.array([2, -(2*a + c)/b]); P2=np.array([4, -(4*a + c)/b])\n",
    "print('P1 = ', P1, ', P2 = ', P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the training data with the support vectors\n",
    "plt.figure(figsize=(5,5));\n",
    "plt.scatter(X_train[indxS,0], X_train[indxS,1], color='blue')\n",
    "plt.scatter(X_train[indxV,0], X_train[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length');\n",
    "# plot the support vectors in black - stemming form the origin\n",
    "for k in range(NumSVecs.sum()):\n",
    "  plt.plot([0,SVecs[k,0]],[0,SVecs[k,1]],'k')\n",
    "plt.xlim(2,5); plt.ylim(4,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot training data with decision boundary\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train[indxS,0], X_train[indxS,1], color='blue')\n",
    "plt.scatter(X_train[indxV,0], X_train[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length');\n",
    "plt.plot([P1[0],P2[0]],[P1[1],P2[1]],'k')\n",
    "#plt.plot(xx, yy, \"y-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "indxS = np.where(y_pred == 'setosa')[0]\n",
    "indxV = np.where(y_pred != 'setosa')[0]\n",
    "P1 = np.array([2, -(2*a + c)/b]); P2=np.array([4, -(4*a + c)/b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# test data with predictions and decision boundary\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_test[indxS,0], X_test[indxS,1], color='blue')\n",
    "plt.scatter(X_test[indxV,0], X_test[indxV,1], color='red')\n",
    "plt.axis('equal'); plt.xlabel('sepal_width'); plt.ylabel('petal_length');\n",
    "plt.plot([P1[0],P2[0]],[P1[1],P2[1]],'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The separation of the data is less than before, so the SVM might not do as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "We covered *just enough*, to make *progress at pace*. We looked at\n",
    "\n",
    "- lines, planes, hyperplanes\n",
    "- distance from a point to a hyperplane: optimisation\n",
    "- Binary Classification with SVM\n",
    "\n",
    "### Next...\n",
    "\n",
    "The SVM is an important tool, and like many of the other things we have discussed,\n",
    "we could talk much more about it. It can for example be used in multi-class\n",
    "applications and also for regression.\n",
    "\n",
    "However, we have a much bigger goal: **deep neural networks**.\n",
    "\n",
    "For that we need to move on to the **perceptron**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Technical Notes, Production and Archiving\n",
    "\n",
    "Ignore the material below. What follows is not relevant to the material being taught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Production Workflow\n",
    "\n",
    "- Finalise the notebook material above\n",
    "- Clear and fresh run of entire notebook\n",
    "- Create html slide show:\n",
    "  - `jupyter nbconvert --to slides 12_svm.ipynb `\n",
    "- Set `OUTPUTTING=1` below\n",
    "- Comment out the display of web-sourced diagrams\n",
    "- Clear and fresh run of entire notebook\n",
    "- Comment back in the display of web-sourced diagrams\n",
    "- Clear all cell output\n",
    "- Set `OUTPUTTING=0` below\n",
    "- Save\n",
    "- git add, commit and push to FML\n",
    "- copy PDF, HTML etc to web site\n",
    "  - git add, commit and push\n",
    "- rebuild binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some of this originated from\n",
    "\n",
    "<https://stackoverflow.com/questions/38540326/save-html-of-a-jupyter-notebook-from-within-the-notebook>\n",
    "\n",
    "These lines create a back up of the notebook. They can be ignored.\n",
    "\n",
    "At some point this is better as a bash script outside of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NBROOTNAME=12_svm\n",
    "OUTPUTTING=0\n",
    "\n",
    "if [ $OUTPUTTING -eq 1 ]; then\n",
    "  jupyter nbconvert --to html $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.html ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.html\n",
    "  mv -f $NBROOTNAME.html ./formats/html/\n",
    "\n",
    "  jupyter nbconvert --to pdf $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.pdf ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.pdf\n",
    "  mv -f $NBROOTNAME.pdf ./formats/pdf/\n",
    "\n",
    "  jupyter nbconvert --to script $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.py ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.py\n",
    "  mv -f $NBROOTNAME.py ./formats/py/\n",
    "else\n",
    "  echo 'Not Generating html, pdf and py output versions'\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
