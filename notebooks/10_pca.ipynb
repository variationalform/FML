{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "#### *variationalform* <https://variationalform.github.io/>\n",
    "\n",
    "#### *Just Enough: progress at pace*\n",
    "\n",
    "<https://variationalform.github.io/>\n",
    "\n",
    "<https://github.com/variationalform>\n",
    "\n",
    "Simon Shaw\n",
    "<https://www.brunel.ac.uk/people/simon-shaw>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<p>\n",
    "This work is licensed under CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)\n",
    "\n",
    "<p>\n",
    "Visit <a href=\"http://creativecommons.org/licenses/by-sa/4.0/\">http://creativecommons.org/licenses/by-sa/4.0/</a> to see the terms.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>This document uses python</td>\n",
    "<td>\n",
    "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>and also makes use of LaTeX </td>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/LaTeX_logo.svg/320px-LaTeX_logo.svg.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>in Markdown</td> \n",
    "<td>\n",
    "<img src=\"https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What this is about:\n",
    "\n",
    "- **Principal Component Analysis**, or PCA. What it is, mathematically and in code.\n",
    "- How it works, with examples.\n",
    "- The connection between PCA and eigenvalues, and the SVD (**Singular Value Decomposition**).\n",
    "\n",
    "As usual our emphasis will be on *doing* rather than *proving*:\n",
    "*just enough: progress at pace*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assigned Reading\n",
    "\n",
    "For this worksheet you are recommended Chapters 4 and 10 of [MML],\n",
    "Chapter 10 of [MLFCES], Chapter 5.3 of [IPDS], \n",
    "\n",
    "- MML: Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong.\n",
    "  Cambridge University Press. <https://mml-book.github.io>.\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists, by Andreas Lindholm,\n",
    "  Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön. Cambridge University Press. \n",
    "  <http://smlbook.org>.\n",
    "- IPDS: Introduction to Probability for Data Science, by Stanley H. Chan,\n",
    "  <https://probability4datascience.com>\n",
    "\n",
    "These can be accessed legally and without cost.\n",
    "\n",
    "There are also these useful references for coding:\n",
    "\n",
    "- PT: `python`: <https://docs.python.org/3/tutorial>\n",
    "- NP: `numpy`: <https://numpy.org/doc/stable/user/quickstart.html>\n",
    "- MPL: `matplotlib`: <https://matplotlib.org>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "\n",
    "We have seen these \n",
    "\n",
    "- Eigenvalue decomposition\n",
    "- SVD, the **Singular Value Decomposition**\n",
    "\n",
    "Let's review them..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigen-systems of Symmetric Matrices\n",
    "\n",
    "Given a real square symmetric $n$-row by $n$-column matrix,\n",
    "$\\boldsymbol{A}\\in\\mathbb{R}^{n\\times n}$, the eigenvalue problem\n",
    "is that of finding scalar eigenvalues $\\lambda$\n",
    "and $n$-dimensional eigenvectors $\\boldsymbol{v}$ such that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{v}=\\lambda\\boldsymbol{v}    \n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\boldsymbol{A}\\boldsymbol{V}=\\boldsymbol{V}\\boldsymbol{D}  \n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\boldsymbol{A} = \n",
    "\\sum_{k=1}^n\n",
    "\\lambda_k\\boldsymbol{v}_k\\boldsymbol{v}_k^T.\n",
    "$$\n",
    "\n",
    "The eigensystem is **real**.\n",
    "\n",
    "We have the *Spectral Theorem* - see [MML, Theorem 4.15]\n",
    "\n",
    "> **Spectral Theorem (for matrices)**\n",
    "> If $\\boldsymbol{A}$ is real and symmetric then its eigenvalues are\n",
    "> all real and its eigenvector matrix $\\boldsymbol{V}$ can be taken\n",
    "> as *orthogonal* so that $\\boldsymbol{V}^{-1}=\\boldsymbol{V}^T$.\n",
    "Hence...\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}=\\boldsymbol{V}\\boldsymbol{D}\\boldsymbol{V}^T  \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The SVD: Singular Value Decomposition\n",
    "\n",
    "Given a real $m$-row by $n$-column matrix, \n",
    "$\\boldsymbol{B}\\in\\mathbb{R}^{m\\times n}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "=\\sum_{j=1}^{p} \\sigma_j \\boldsymbol{u}_j\\boldsymbol{v}_j^T\n",
    "$$\n",
    "\n",
    "where: for the left singular vectors: $\\boldsymbol{U}\\in\\mathbb{R}^{m\\times m}$;\n",
    "for the singular values: $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{m\\times n}$;\n",
    "and, for the right singular vectors, $\\boldsymbol{V}\\in\\mathbb{R}^{n\\times n}$.\n",
    "Here $p=\\min\\{m,n\\}$.\n",
    "\n",
    "Note that $\\boldsymbol{\\Sigma}=\\text{diag}(\\sigma_1,\\ldots,\\sigma_p) + \\mathit{zeros}$,\n",
    "and we can always arrange that $0 \\le \\sigma_p\\le\\cdots\\le\\sigma_1$.\n",
    "\n",
    "As $\\boldsymbol{B}$ is real,\n",
    "$\\boldsymbol{U}$ and $\\boldsymbol{V}$ are real and *orthogonal*.\n",
    "\n",
    "If $\\sigma_r\\ne 0$ and $\\sigma_p= 0$ for all $p>r$ then\n",
    "$r$ is the rank of $\\boldsymbol{B}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How are these factorizations connected?\n",
    "\n",
    "On the face of it they are very different. the first applies only to \n",
    "square symmetric matrices, while the second applies also to\n",
    "rectangular, and hence (why?) non-symmetric matrices.\n",
    "\n",
    "But... Look at this... Given the SVD \n",
    "$\\boldsymbol{B} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$\n",
    "we have,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}^T\\boldsymbol{B}\n",
    "= \\Big(\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\\Big)^T\n",
    "\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "$$\n",
    "\n",
    "and remembering that, in general,\n",
    "$(\\boldsymbol{K}\\boldsymbol{L})^T = \\boldsymbol{L}^T\\boldsymbol{K}^T$\n",
    "(this could be called *taking the transpose through*), we can write,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}^T\\boldsymbol{B}\n",
    "= \\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T\n",
    "\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "= \\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "$$\n",
    "because $\\boldsymbol{U}^T\\boldsymbol{U}=\\boldsymbol{I}$ (orthogonal).\n",
    "\n",
    "Similarly, because also\n",
    "$\\boldsymbol{V}^T\\boldsymbol{V}=\\boldsymbol{I}$ (orthogonal),\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\\boldsymbol{B}^T\n",
    "= \n",
    "\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "\\Big(\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\\Big)^T\n",
    "= \n",
    "\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T\n",
    "= \n",
    "\\boldsymbol{U}\\boldsymbol{\\Sigma}\n",
    "\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T.\n",
    "$$\n",
    "\n",
    "Do you recognise these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have just shown that,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}^T\\boldsymbol{B}\n",
    "= \\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "\\qquad\\text{ and }\\qquad\n",
    "\\boldsymbol{B}\\boldsymbol{B}^T\n",
    "= \n",
    "\\boldsymbol{U}\\boldsymbol{\\Sigma}\n",
    "\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T.\n",
    "$$\n",
    "\n",
    "Familiar? Think about $\\boldsymbol{A}=\\boldsymbol{V}\\boldsymbol{D}\\boldsymbol{V}^T$.\n",
    "\n",
    "- Put $\\boldsymbol{A} = \\boldsymbol{B}^T\\boldsymbol{B}$ (symmetric) and \n",
    "$\\boldsymbol{D} = \\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}$. Then,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}^T\\boldsymbol{B}\n",
    "= \\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "\\qquad\\text{becomes}\\qquad\n",
    "\\boldsymbol{A}=\\boldsymbol{V}\\boldsymbol{D}\\boldsymbol{V}^T.\n",
    "$$\n",
    "\n",
    "- Put $\\boldsymbol{A} = \\boldsymbol{B}\\boldsymbol{B}^T$ (symmetric) and \n",
    "$\\boldsymbol{D} = \\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^T$. Then,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\\boldsymbol{B}^T\n",
    "= \n",
    "\\boldsymbol{U}\\boldsymbol{\\Sigma}\n",
    "\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T\n",
    "\\qquad\\text{becomes}\\qquad\n",
    "\\boldsymbol{A}=\\boldsymbol{U}\\boldsymbol{D}\\boldsymbol{U}^T.\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol{V}$, the right singular vectors in the SVD are the eigenvectors of \n",
    "$\\boldsymbol{B}^T\\boldsymbol{B}$.\n",
    "\n",
    "- $\\boldsymbol{U}$, the left singular vectors in the SVD are the eigenvectors of \n",
    "$\\boldsymbol{B}\\boldsymbol{B}^T$.\n",
    "\n",
    "- In both cases $\\boldsymbol{\\Sigma}$ contains the positive square\n",
    "roots of the eigenvalues of $\\boldsymbol{B}^T\\boldsymbol{B}$\n",
    "and $\\boldsymbol{B}\\boldsymbol{B}^T$.\n",
    "\n",
    "- **NOTE:** $\\boldsymbol{B}^T\\boldsymbol{B}$ and $\\boldsymbol{B}\\boldsymbol{B}^T$\n",
    "have the same non-zero eigenvalues (same rank).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why does this matter?\n",
    " \n",
    "Our data, $\\boldsymbol{X}$, is organized into rows of feature values with one observation per row \n",
    "and one feature per column. We write this as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\Big(\n",
    "\\boldsymbol{X}_0, \\boldsymbol{X}_1, \\cdots, \\boldsymbol{X}_D\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "If $D=3$ (four features)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... we recall that the **covariance matrix** takes this form:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \n",
    "\\left(\\begin{array}{llll}\n",
    "\\mathrm{Var}(X_0)  &  \\mathrm{Cov}(X_0,X_1)  &  \\mathrm{Cov}(X_0,X_2)  &  \\mathrm{Cov}(X_0,X_3) \\\\\n",
    "\\mathrm{Cov}(X_1,X_0)  &  \\mathrm{Var}(X_1)  &  \\mathrm{Cov}(X_1,X_2)  &  \\mathrm{Cov}(X_1,X_3) \\\\\n",
    "\\mathrm{Cov}(X_2,X_0)  &  \\mathrm{Cov}(X_2,X_1)  &  \\mathrm{Var}(X_2)  &  \\mathrm{Cov}(X_2,X_3) \\\\\n",
    "\\mathrm{Cov}(X_3,X_0)  &  \\mathrm{Cov}(X_3,X_1)  &  \\mathrm{Cov}(X_3,X_2)  &  \\mathrm{Var}(X_3) \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "because $\\mathrm{Cov}(X,X)=\\mathrm{Var}(X)$. Since $\\mathrm{Cov}(X,Y)=\\mathrm{Cov}(Y,X)$, this matrix is **symmetric**\n",
    "and so has real eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen that if the data are already centred then,\n",
    "\n",
    "$$\n",
    "(N-1)\\boldsymbol{S} = \n",
    "\\left(\\begin{array}{llll}\n",
    "\\boldsymbol{X}_0\\cdot\\boldsymbol{X}_0 & \\boldsymbol{X}_0\\cdot\\boldsymbol{X}_1 &\n",
    "\\boldsymbol{X}_0\\cdot\\boldsymbol{X}_2 & \\boldsymbol{X}_0\\cdot\\boldsymbol{X}_3\n",
    "\\\\\n",
    "\\boldsymbol{X}_1\\cdot\\boldsymbol{X}_0 & \\boldsymbol{X}_1\\cdot\\boldsymbol{X}_1 &\n",
    "\\boldsymbol{X}_1\\cdot\\boldsymbol{X}_2 & \\boldsymbol{X}_1\\cdot\\boldsymbol{X}_3\n",
    "\\\\\n",
    "\\boldsymbol{X}_2\\cdot\\boldsymbol{X}_0 & \\boldsymbol{X}_2\\cdot\\boldsymbol{X}_1 &\n",
    "\\boldsymbol{X}_2\\cdot\\boldsymbol{X}_2 & \\boldsymbol{X}_2\\cdot\\boldsymbol{X}_3\n",
    "\\\\\n",
    "\\boldsymbol{X}_3\\cdot\\boldsymbol{X}_0 & \\boldsymbol{X}_3\\cdot\\boldsymbol{X}_1 &\n",
    "\\boldsymbol{X}_3\\cdot\\boldsymbol{X}_2 & \\boldsymbol{X}_3\\cdot\\boldsymbol{X}_3\n",
    "\\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{l}\n",
    "\\boldsymbol{X}_0^T\n",
    "\\\\\n",
    "\\boldsymbol{X}_1^T\n",
    "\\\\\n",
    "\\boldsymbol{X}_2^T\n",
    "\\\\\n",
    "\\boldsymbol{X}_3^T\n",
    "\\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{llll}\n",
    "\\boldsymbol{X}_0\n",
    "&\n",
    "\\boldsymbol{X}_1\n",
    "&\n",
    "\\boldsymbol{X}_2\n",
    "&\n",
    "\\boldsymbol{X}_3\n",
    "\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "and, hence (in general), the (sample) covariance matrix for $N$ observations is\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \n",
    "\\frac{1}{(N-1)}\\boldsymbol{X}^T\\boldsymbol{X}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "We just introduced the **sample** covariance matrix:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \n",
    "\\frac{1}{(N-1)}\\boldsymbol{X}^T\\boldsymbol{X}.\n",
    "$$\n",
    "\n",
    "The $N-1$ in the denominator makes this an **unbiased** estimate of the \n",
    "population statistics. When $N$ is large we can just work with \n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \n",
    "\\frac{1}{N}\\boldsymbol{X}^T\\boldsymbol{X}\n",
    "$$\n",
    "\n",
    "and call it the **empirical** covariance matrix.\n",
    "\n",
    "This terminology is discussed in [MML, Section 6.4.2]. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conventions\n",
    "\n",
    "We have now adopted a convention that our data matrix $\\boldsymbol{X}$ has \n",
    "features varying along the rows, and observations varying down the columns\n",
    "so that:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\Big(\n",
    "\\boldsymbol{X}_0, \\boldsymbol{X}_1, \\cdots, \\boldsymbol{X}_D\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "gives a data set with $D+1$ features. The length (they must all be the same)\n",
    "of the column vectors \n",
    "$\\boldsymbol{X}_0,\\ \\boldsymbol{X}_1,\\ \\boldsymbol{X}_2,\\ \\ldots,\\ \\boldsymbol{X}_D$\n",
    "tell us how many observations there are. We've been denoting this\n",
    "by $N$.\n",
    "\n",
    ">**HOWEVER**: in some sources this convention is transposed. The different\n",
    "features occupy their own rows of the matrix,\n",
    "with the observations recorded along the rows. \n",
    "\n",
    "This is the case in [MML]. It means that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \n",
    "\\frac{1}{N}\\boldsymbol{X}^T\\boldsymbol{X}\n",
    "\\qquad\\text{for us, becomes}\\qquad\n",
    "\\boldsymbol{S} = \n",
    "\\frac{1}{N}\\boldsymbol{X}\\boldsymbol{X}^T\n",
    "\\qquad\\text{for them}\n",
    "$$\n",
    "\n",
    "because our $\\boldsymbol{X}$ is their $\\boldsymbol{X}^T$.\n",
    "\n",
    "**BE CAREFUL: this is not uncommon**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features and Observations\n",
    "\n",
    "Let's say there are $D+1$ features (columns) in our data set\n",
    "$\\boldsymbol{X}$ and $N$ observations (rows).\n",
    "\n",
    "An observation takes the form $\\boldsymbol{x}_j = (x_1, x_2, \\ldots, x_d)^T$,\n",
    "a **column vector**, for $j=1,2,\\ldots,N$.\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X}\n",
    "= \\Big(\n",
    "\\boldsymbol{X}_0, \\boldsymbol{X}_1, \\cdots, \\boldsymbol{X}_D\n",
    "\\Big)\n",
    "= \\Big(\n",
    "\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\cdots, \\boldsymbol{x}_N\n",
    "\\Big)^T\n",
    "$$\n",
    "\n",
    "Note: we are using **upper case** $\\boldsymbol{X}_k$ for a column\n",
    "vector of observations of a feature (in the column indexed by $k$),\n",
    "and **lower case**, $\\boldsymbol{x}_j$, for a feature vector\n",
    "arising from a single observation (in the row indexed by $j$). \n",
    "\n",
    "We only use column vectors in these notes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA - Principal Component Analysis\n",
    "\n",
    "The main idea and motivation behind this is that high dimensional data\n",
    "often lives very close to a lower dimensional subspace.\n",
    "\n",
    "A typical and often used example of this is that in Figure 10.1 of [MML, Chap. 10].\n",
    "\n",
    "We can see that here: <https://mml-book.github.io>.\n",
    "\n",
    "PCA will analyze a data set and determine the direction in which\n",
    "most variation occurs. If we are to approximate using a lower dimesional space\n",
    "then this is a good direction (subspace component) to start with.\n",
    "\n",
    "Technically, PCA determines directions which maximize variance.\n",
    "\n",
    "Let's go through this slowly - it can be quite confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA - outline algorithm.\n",
    "\n",
    "- Take our $(D+1)$-column by $N$-row data set $\\boldsymbol{X}$\n",
    "and ensure that the column means are zero.\n",
    "**This is referred to as _centering the data_ **.\n",
    "\n",
    "- It means that $\\mathbb{E}(\\boldsymbol{X}_d)=0$ for\n",
    "columns $d=0,1,2,\\ldots,D$.\n",
    "\n",
    "- We want a $(D+1)$-row by $M$-column matrix\n",
    "$\\boldsymbol{B}\\in\\mathbb{R}^{D+1,M}$, called the **code** in [MML],\n",
    "such that we can define $\\boldsymbol{Z}$ as follows:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{Z} = \\boldsymbol{X}\\boldsymbol{B}\\boldsymbol{B}^T\n",
    "$$\n",
    "\n",
    "- If $M=D+1$ we insist that $\\boldsymbol{Z}=\\boldsymbol{X}$. Otherwise\n",
    "we have $M\\le D$ and $\\boldsymbol{Z}$ is an approximation (a projection)\n",
    "of $\\boldsymbol{X}$ in a lower dimensional subspace.\n",
    "\n",
    "- We determine $\\boldsymbol{B}$ by minimizing the reconstruction\n",
    "error:\n",
    "\n",
    "$$\n",
    "\\mathscr{J} = \\frac{1}{N}\\sum_{n=1}^N\n",
    "\\Vert\\boldsymbol{x}_n-\\boldsymbol{z}_n\\Vert_2^2\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{z}_n$ is the $n$-th row of $\\boldsymbol{Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some Technicalities\n",
    "\n",
    "> **THINK ABOUT**: If $\\boldsymbol{X} = \\boldsymbol{X}\\boldsymbol{B}\\boldsymbol{B}^T$\n",
    "when $M=D$ then $\\boldsymbol{B}\\boldsymbol{B}^T=\\boldsymbol{I}_D$. Is\n",
    "$\\boldsymbol{B}$ square?\n",
    "\n",
    "> **THINK ABOUT**: If $\\boldsymbol{Z} = \\boldsymbol{X}\\boldsymbol{B}\\boldsymbol{B}^T$\n",
    "when $M<D$ then what shape is $\\boldsymbol{Z}$? Is it the same shape as \n",
    "$\\boldsymbol{X}$?\n",
    "\n",
    "> **THINK ABOUT**: If we set \n",
    "$\\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{B}$ then \n",
    "$\\boldsymbol{Z} = \\boldsymbol{Y}\\boldsymbol{B}^T$. What shape is \n",
    "$\\boldsymbol{Y}$?\n",
    "\n",
    "> **THINK ABOUT**: $\\boldsymbol{Y}$ will be $N$-rows by $M$-columns.\n",
    "$\\boldsymbol{Y}$ is **smaller** than $\\boldsymbol{X}$ if $M<D$ and\n",
    "represents dimensionality reduction. \n",
    "\n",
    "> **THINK ABOUT**: the reduction $\\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{B}$,\n",
    "and the subsequent enlargement $\\boldsymbol{Z} = \\boldsymbol{Y}\\boldsymbol{B}^T$\n",
    "is the basis of an **autoencoder**. $\\boldsymbol{Z}$ is a\n",
    "reconstruction of $\\boldsymbol{X}$ resulting from a data compression step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This minimization referred to above is a long technical excursion in\n",
    "multivariable calculus. The important result of it is that we need to\n",
    "find the eigensystem of the empirical data covariance matrix,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \n",
    "\\frac{1}{N}\\boldsymbol{X}^T\\boldsymbol{X}.\n",
    "$$\n",
    "\n",
    "This means we want to solve\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}\\boldsymbol{v} = \\lambda\\boldsymbol{v}\n",
    "$$\n",
    "\n",
    "for the eigenpairs\n",
    "$(\\lambda_1, \\boldsymbol{v}_1),\\ (\\lambda_2, \\boldsymbol{v}_2),\\ \\ldots$\n",
    "\n",
    "Then $\\boldsymbol{B} = (\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_M)$\n",
    "and the eigenvalues tell us how much variance of the original data set is captured\n",
    "by the $M$-dimensional projection.\n",
    "\n",
    "Let's see this in action..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Worked example\n",
    "\n",
    "Consider this set of data (already centered),\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left(\\begin{array}{rr}\n",
    "1 & 2 \\\\ 2 & 1 \\\\ -2 & -1 \\\\ -1 & -2\n",
    "\\end{array}\\right)\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\boldsymbol{S}\n",
    "= \\frac{1}{N}\\boldsymbol{X}^T\\boldsymbol{X}\n",
    "= \\frac{1}{4}\\left(\\begin{array}{rr}\n",
    "10 & 8 \\\\ 8 & 10\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The eigensystem,\n",
    "with $\\boldsymbol{S}\\boldsymbol{V} = \\boldsymbol{V}\\boldsymbol{D}$,\n",
    "is \n",
    "\n",
    "$$\n",
    "\\boldsymbol{V} = \\frac{1}{\\sqrt{2}}\n",
    "\\left(\\begin{array}{rr}\n",
    "1 & -1 \\\\ 1 & 1\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{rr}\n",
    "\\boldsymbol{v}_0 & \\boldsymbol{v}_1\n",
    "\\end{array}\\right)\n",
    "\\quad\\text{ and }\\quad\n",
    "\\boldsymbol{D}\n",
    "=\n",
    "\\frac{1}{2}\\left(\\begin{array}{rr}\n",
    "9 & 0 \\\\ 0 & 1\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{rr}\n",
    "\\lambda_0 & 0 \\\\ 0 & \\lambda_1\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "Let's see this in python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# set up the feature matrix and check the column means are zero\n",
    "D=2; N=4\n",
    "X = np.array([[1, 2], [2, 1], [-2, -1], [-1, -2]])\n",
    "print(f'Column means: col 1, {X[:,0].mean()} and col 2, {X[:,1].mean()}')\n",
    "# and the empirical covariance matrix\n",
    "S = 1/N*X.T @ X\n",
    "print('S = \\n',S)\n",
    "# solve the eigenvalue problem\n",
    "lmda, V = np.linalg.eig(S)\n",
    "print('evals = ', lmda)\n",
    "print('evecs = \\n', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# a picture will tell us much more...\n",
    "plt.figure(figsize=(4,4)); plt.gca().set_aspect('equal')\n",
    "# plot the data in blue\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')\n",
    "# plot the eigenvectors...\n",
    "# the first in red from -v0 to +v0 with length 2*lambda_0\n",
    "x0 = lmda[0]*V[0,[0]]; y0 = lmda[0]*V[1,[0]]\n",
    "plt.plot([-x0,x0],[-y0,y0],'-', color='r')\n",
    "# the second in green from -v1 to +v1 with length 2*lambda_1\n",
    "x1 = lmda[1]*V[0,[1]]; y1 = lmda[1]*V[1,[1]]\n",
    "plt.plot([-x1,x1],[-y1,y1],'-', color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We see that the direction of maximum variance is given by the\n",
    "dominant eigenpair. The next eigenpair is **orthogonal**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we know that $\\boldsymbol{V}$ is an orthogonal matrix, so that\n",
    "$\\boldsymbol{V}\\boldsymbol{V}^T = \\boldsymbol{I}$.\n",
    "\n",
    "It is therefore clear that \n",
    "$\\boldsymbol{X}=\\boldsymbol{X}\\boldsymbol{V}\\boldsymbol{V}^T$.\n",
    "Now, with $\\boldsymbol{V}=(\\boldsymbol{v}_0, \\boldsymbol{v}_1)$,\n",
    "we observe that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{Z}_0\n",
    "=\\boldsymbol{X}\\boldsymbol{v}_0\\boldsymbol{v}_0^T\n",
    "= \\frac{3}{2}\\left(\\begin{array}{rr}\n",
    "1 & 1 \\\\ 1 & 1 \\\\ -1 & -1 \\\\ -1 & -1\n",
    "\\end{array}\\right)\n",
    "\\quad\\text{and}\\quad\n",
    "\\boldsymbol{Z}_1\n",
    "=\\boldsymbol{X}\\boldsymbol{v}_1\\boldsymbol{v}_1^T\n",
    "= \\frac{1}{2}\\left(\\begin{array}{rr}\n",
    "-1 & 1 \\\\ 1 & -1 \\\\ -1 & 1 \\\\ 1 & -1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The rows give us the projections of the original rows (observations)\n",
    "onto the lower dimensional subspaces.\n",
    "\n",
    "Let's see it in code, and then in pictures (building on the picture above)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('X - X V V.T = \\n', X - X @ V @ V.T )\n",
    "\n",
    "v0 = V[:,[0]]; Z0 = X @ v0 @ v0.T\n",
    "print('X @ v0 @ v0.T = \\n', Z0)\n",
    "\n",
    "v1 = V[:,[1]]; Z1 = X @ v1 @ v1.T\n",
    "print('X @ v1 @ v1.T = \\n', Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3)); plt.gca().set_aspect('equal')\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')\n",
    "plt.plot([-x0,x0],[-y0,y0],'-', color='r')\n",
    "plt.plot([-x1,x1],[-y1,y1],'-', color='g')\n",
    "# just the first row of Z0 for the moment\n",
    "plt.plot([X[0,0], Z0[0,0]], [X[0,1], Z0[0,1]], ':', marker='o', color='r', markevery=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the first row of $\\boldsymbol{Z}_0 =\\boldsymbol{X}\\boldsymbol{v}_0\\boldsymbol{v}_0^T$,\n",
    "the original point at $(1,2)$ is projected to the new point $(1.5,1.5)$\n",
    "on the dominant lower dimensional subspace. What about \n",
    "$\\boldsymbol{Z}_1 =\\boldsymbol{X}\\boldsymbol{v}_1\\boldsymbol{v}_1^T$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3)); plt.gca().set_aspect('equal')\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')\n",
    "plt.plot([-x0,x0],[-y0,y0],'-', color='r')\n",
    "plt.plot([-x1,x1],[-y1,y1],'-', color='g')\n",
    "# just the last row of Z1 for the moment\n",
    "plt.plot([X[3,0], Z1[3,0]], [X[3,1], Z1[3,1]], ':', marker='o', color='g', markevery=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now in the last row of $\\boldsymbol{Z}_1 =\\boldsymbol{X}\\boldsymbol{v}_1\\boldsymbol{v}_1^T$,\n",
    "the original point at $(-1,-2)$ is projected to the new point $(0.5,-0.5)$\n",
    "on the next-dominant lower dimensional subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see all the projections in one picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3)); plt.gca().set_aspect('equal')\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')\n",
    "plt.plot([-x0,x0],[-y0,y0],'-', color='r')\n",
    "plt.plot([-x1,x1],[-y1,y1],'-', color='g')\n",
    "for k in range(4):\n",
    "  plt.plot([X[k,0], Z0[k,0]], [X[k,1], Z0[k,1]], ':', marker='o', color='r', markevery=[1])\n",
    "  plt.plot([X[k,0], Z1[k,0]], [X[k,1], Z1[k,1]], ':', marker='o', color='g', markevery=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Local Coordinate System\n",
    "\n",
    "If we treat the eigenvectors as subspaces then the length along each eigenvector is the local coordinate in that subspace. How can we get these coordinates? Well, look at this:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left(\\begin{array}{rr}\n",
    "1 & 2 \\\\ 2 & 1 \\\\ -2 & -1 \\\\ -1 & -2\n",
    "\\end{array}\\right)\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\boldsymbol{X}\\boldsymbol{v}_0\n",
    "=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{r} 3 \\\\ 3 \\\\ -3 \\\\ -3 \\end{array}\\right)\n",
    "\\quad\\text{and}\\quad\n",
    "\\boldsymbol{X}\\boldsymbol{v}_1\n",
    "=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{r} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{array}\\right).\n",
    "$$\n",
    "\n",
    "These tell us that the first two points (rows) in $\\boldsymbol{X}$ \n",
    "project to coincident points a distance $\\frac{3}{\\sqrt{2}}$ along\n",
    "the dominant eigenvector, and that the second two points \n",
    "project to coincident points a distance $\\frac{-3}{\\sqrt{2}}$ along\n",
    "the dominant eigenvector.\n",
    "\n",
    "On the other hand, the first and third points project to distances \n",
    "$\\frac{1}{\\sqrt{2}}$ along the second eigenvector, while the second\n",
    "and fourth project to distances $\\frac{-1}{\\sqrt{2}}$.\n",
    "\n",
    "Here is some code to illustrate this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xv0 = X @ v0\n",
    "Xv1 = X @ v1\n",
    "# multiply by root 2 to tidy up the output\n",
    "print('sqrt(2) X v0 = \\n', np.sqrt(2) * Xv0)\n",
    "print('sqrt(2) X v1 = \\n', np.sqrt(2) * Xv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4)); plt.gca().set_aspect('equal')\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')\n",
    "plt.plot([-x0,x0],[-y0,y0],'-', color='r')\n",
    "plt.plot([-x1,x1],[-y1,y1],'-', color='g')\n",
    "cos45 = sin45 = 1/np.sqrt(2)\n",
    "# for data point 0 along v0\n",
    "plt.plot([0, Xv0[0,0]*cos45], [0, Xv0[0,0]*sin45], ':', marker='o', color='k', markevery=[1])\n",
    "# for data point 3 along v1\n",
    "plt.plot([0, -Xv1[3,0]*cos45], [0, Xv1[3,0]*sin45], ':', marker='o', color='k', markevery=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explained Variance\n",
    "\n",
    "There is yet more to see... The column-wise variances in the orignal data set,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\left(\\begin{array}{rr}\n",
    "1 & 2 \\\\ 2 & 1 \\\\ -2 & -1 \\\\ -1 & -2\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "are \n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{array}{rrrr}\n",
    "\\mathrm{Var}(\\boldsymbol{X}_0)\n",
    "& = \\mathbb{E}(\\boldsymbol{X}_0\\cdot\\boldsymbol{X}_0)\n",
    "& = \\frac{1}{4}\\big(1+2^2+(-2)^2+1\\big) & = \\frac{5}{2},\n",
    "\\\\\\ \\\\\n",
    "\\mathrm{Var}(\\boldsymbol{X}_1)\n",
    "& = \\mathbb{E}(\\boldsymbol{X}_1\\cdot\\boldsymbol{X}_1)\n",
    "& = \\frac{1}{4}\\big(2^2+1+1+(-2)^2\\big) & = \\frac{5}{2}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "and the total variance in the data set is $\\frac{5}{2}+\\frac{5}{2}=5$ (Note - \n",
    "this isn't what you get by stacking the data and taking a single variance because\n",
    "the mean(s) may get altered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The sum of the individual feature variances is\n",
    "$\\frac{5}{2}+\\frac{5}{2}=5$.\n",
    "\n",
    "In PCA, each eigenvalue gives the variance in the direction of its eigenvector.\n",
    "\n",
    "Our eigenvalues were $\\lambda_0 = \\frac{9}{2}$ and $\\lambda_1 = \\frac{1}{2}$. The\n",
    "total variance is therefore $\\frac{9}{2}+\\frac{1}{2}=5$.\n",
    "\n",
    "> **THINK ABOUT**: the trace of a matrix is the sum of its eigenvalues. Relevance?\n",
    "\n",
    "We talk about each eigenvalue **explaining** variance in the original data set.\n",
    "\n",
    "Here the first eigenvalue explains $\\frac{9}{2}\\div 5 =90\\%$ of the \n",
    "original variance. The remaining $5\\%$ is in the orthogonal direction\n",
    "of the second eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The connection to SVD\n",
    "\n",
    "We just performed PCA using an eigenvalue analysis of the empirical\n",
    "covariance matrix\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \n",
    "\\frac{1}{N}\\boldsymbol{X}^T\\boldsymbol{X}\n",
    "\\quad\\text{leading to}\\quad\n",
    "\\frac{1}{N}\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{v}\n",
    "=\\lambda\\boldsymbol{v}.\n",
    "$$\n",
    "\n",
    "Earlier, given the SVD \n",
    "$\\boldsymbol{B} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$,\n",
    "we saw that putting\n",
    "$\\boldsymbol{A} = \\boldsymbol{B}^T\\boldsymbol{B}$ (symmetric) and \n",
    "$\\boldsymbol{D} = \\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}$ led to\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}^T\\boldsymbol{B}\n",
    "= \\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "\\qquad\\text{becoming}\\qquad\n",
    "\\boldsymbol{A}=\\boldsymbol{V}\\boldsymbol{D}\\boldsymbol{V}^T.\n",
    "$$\n",
    "\n",
    "Therefore, for PCA we could also obtain the SVD of $\\boldsymbol{X}$ and\n",
    "use the right singular vectors. The eigenvalues will be the squares of\n",
    "the singular values divided by $N$.\n",
    "\n",
    "Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# re-solve the eigenvalue problem\n",
    "lmda, V = np.linalg.eig(S)\n",
    "print(f'evals = {lmda} and V = ')\n",
    "print(V)\n",
    "# take the SVD of X\n",
    "U, Sig, VT = np.linalg.svd(X)\n",
    "print(f'singular values Sigma = {Sig}')\n",
    "print(f'Sigma^2/N = {Sig*Sig/N} and V = ')\n",
    "print(VT.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how to do PCA with `sklearn`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print('Here is the explained variance as ratios...')\n",
    "print(f'XV ratio = {pca.explained_variance_ratio_}')\n",
    "print(f'Here are the singular values = {pca.singular_values_}')\n",
    "print('the eigenvalues are squares of the singular values divided by N')\n",
    "print(f'eigenvalues = {pca.singular_values_**2/N}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The principal components can be compared with the\n",
    "eigenvectors from above.\n",
    "\n",
    "Beware: they are given to us in **rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# the principal components are stored as row vectors, so transpose\n",
    "B = pca.components_.T\n",
    "print('Principal Components (transposed) B = \\n', B)\n",
    "print('Compare with our earlier V = \\n', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Do you remember that above we used $\\boldsymbol{X}\\boldsymbol{v}_0$\n",
    "and $\\boldsymbol{X}\\boldsymbol{v}_1$ to get the positions (lengths)\n",
    "along the principal axes of the projected data points?\n",
    "\n",
    "`sklearn` can do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# multiply by sqrt(2) to tidy the output.\n",
    "Xf = pca.fit_transform(X)\n",
    "print('np.sqrt(2)*Xf = \\n', np.sqrt(2)*Xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Technically, the principal components give us a new **basis** for the \n",
    "data space. These **transformed** coordinates gives us the coordinates\n",
    "of the data in the new basis. Let's see this in pictures..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are the original data points in the original feature basis.\n",
    "This is where each axis is labelled with the feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4)); plt.gca().set_aspect('equal')\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are the data points in the PCA basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4)); plt.gca().set_aspect('equal')\n",
    "plt.plot(Xf[:,0], Xf[:,1], '.', color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is immediately apparent that the data has become more *one dimensional*.\n",
    "\n",
    "- An issue though is that the axes are no longer easily interpreted.\n",
    "\n",
    "- This is relevant to the **explainability agenda** in AI and Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reflection\n",
    "\n",
    "That was a long journey - and we didn't even derive the results, we just quoted\n",
    "and illustrated them. This, again, is because we are doing *just enough*\n",
    "to make *progress at pace*.\n",
    "\n",
    "We're now going to embark on a much more realistic (well, in 2D at least)\n",
    "example of how this works. We'll go faster because all the work has been done.\n",
    "\n",
    "## Standard Example\n",
    "\n",
    "The following example is used a lot in account of PCA. \n",
    "\n",
    "The idea is to generate and plot a lozenge of Gaussian distributed data.\n",
    "It will have unequal variances (otherwise it would be a circle). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate this number of sample points\n",
    "Ns=50 \n",
    "# The Gaussian lozenge will be centered with non-unit covariance\n",
    "mean = [0, 0]\n",
    "cov = [[30, 15], [15, 15]]\n",
    "# generate Ns random points (x,y)\n",
    "x, y = np.random.multivariate_normal(mean, cov, Ns).T\n",
    "# reshape them to columns and stack them next to each other \n",
    "X = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\n",
    "# We can see it if the data matrix is small - otherwise little point\n",
    "if Ns < 8: print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's look at the empirical column means - they wont be exactly zero \n",
    "print(f'Column means 1, {X[:,0].mean()} and 2, {X[:,1].mean()}')\n",
    "# so let's center this sample data\n",
    "X[:,0] -= X[:,0].mean()\n",
    "X[:,1] -= X[:,1].mean()\n",
    "print(f'Centered column means 1, {X[:,0].mean()} and 2, {X[:,1].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's plot our data set\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.xlim(-20,20)\n",
    "plt.ylim(-20,20)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# perform the PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(f'XV ratio = {pca.explained_variance_ratio_}')\n",
    "print(f'sing vals = {pca.singular_values_}')\n",
    "# the component are stored as row vectors, so transpose\n",
    "V = pca.components_.T\n",
    "print('V = \\n', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# project X to the singular components\n",
    "Z1 = X @ V[:,[0]] @ V[:,[0]].T\n",
    "Z2 = X @ V[:,[1]] @ V[:,[1]].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# re-plot, and include all the projected data\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')\n",
    "# plot the projections - these illustrate the directions\n",
    "plt.plot(Z1[:,0], Z1[:,1], '.', color='r')\n",
    "plt.plot(Z2[:,0], Z2[:,1], '.', color='g')\n",
    "# Now loop over each point and dot-line the projection onto v0 and v1\n",
    "for k in range(Ns):\n",
    "  plt.plot([X[k,0], Z1[k,0]], [X[k,1],Z1[k,1]], ':', color='r')\n",
    "  plt.plot([X[k,0], Z2[k,0]], [X[k,1],Z2[k,1]], ':', color='g')\n",
    "# zoom in or out with this...\n",
    "window=10; plt.xlim(-window,window); plt.ylim(-window,window); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what the data looks like in the new coordinate system\n",
    "Xf = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# here is the original...\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.xlim(-20,20)\n",
    "plt.ylim(-20,20)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.plot(X[:,0], X[:,1], '.', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# here is the transformed data\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.xlim(-20,20)\n",
    "plt.ylim(-20,20)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.plot(Xf[:,0], Xf[:,1], '.', color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "We covered *just enough*, to make *progress at pace*. We looked at\n",
    "\n",
    "- How the SVD and eigenvalue decomposition are related.\n",
    "- How this becomes relevant to the data covariance matrix.\n",
    "- PCA and its use in variance maximization.\n",
    "\n",
    "Now we can start putting all of this material to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Technical Notes, Production and Archiving\n",
    "\n",
    "Ignore the material below. What follows is not relevant to the material being taught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Production Workflow\n",
    "\n",
    "- Finalise the notebook material above\n",
    "- Clear and fresh run of entire notebook\n",
    "- Create html slide show:\n",
    "  - `jupyter nbconvert --to slides 10_pca.ipynb `\n",
    "- Set `OUTPUTTING=1` below\n",
    "- Comment out the display of web-sourced diagrams\n",
    "- Clear and fresh run of entire notebook\n",
    "- Comment back in the display of web-sourced diagrams\n",
    "- Clear all cell output\n",
    "- Set `OUTPUTTING=0` below\n",
    "- Save\n",
    "- git add, commit and push to FML\n",
    "- copy PDF, HTML etc to web site\n",
    "  - git add, commit and push\n",
    "- rebuild binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some of this originated from\n",
    "\n",
    "<https://stackoverflow.com/questions/38540326/save-html-of-a-jupyter-notebook-from-within-the-notebook>\n",
    "\n",
    "These lines create a back up of the notebook. They can be ignored.\n",
    "\n",
    "At some point this is better as a bash script outside of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NBROOTNAME=10_pca\n",
    "OUTPUTTING=0\n",
    "\n",
    "if [ $OUTPUTTING -eq 1 ]; then\n",
    "  jupyter nbconvert --to html $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.html ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.html\n",
    "  mv -f $NBROOTNAME.html ./formats/html/\n",
    "\n",
    "  jupyter nbconvert --to pdf $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.pdf ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.pdf\n",
    "  mv -f $NBROOTNAME.pdf ./formats/pdf/\n",
    "\n",
    "  jupyter nbconvert --to script $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.py ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.py\n",
    "  mv -f $NBROOTNAME.py ./formats/py/\n",
    "else\n",
    "  echo 'Not Generating html, pdf and py output versions'\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
