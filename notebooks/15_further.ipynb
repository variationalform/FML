{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Going Further\n",
    "\n",
    "#### *variationalform* <https://variationalform.github.io/>\n",
    "\n",
    "#### *Just Enough: progress at pace*\n",
    "\n",
    "<https://variationalform.github.io/>\n",
    "\n",
    "<https://github.com/variationalform>\n",
    "\n",
    "Simon Shaw\n",
    "<https://www.brunel.ac.uk/people/simon-shaw>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<p>\n",
    "This work is licensed under CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)\n",
    "\n",
    "<p>\n",
    "Visit <a href=\"http://creativecommons.org/licenses/by-sa/4.0/\">http://creativecommons.org/licenses/by-sa/4.0/</a> to see the terms.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>This document uses python</td>\n",
    "<td>\n",
    "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>and also makes use of LaTeX </td>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/LaTeX_logo.svg/320px-LaTeX_logo.svg.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>in Markdown</td> \n",
    "<td>\n",
    "<img src=\"https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What this is about...\n",
    "\n",
    "... Some of the things we could have covered but didn't, and a review.\n",
    "\n",
    "- Some more Deep Learning\n",
    "\n",
    "- Cross-Entropy and SoftMax\n",
    "\n",
    "- A review of topics\n",
    "\n",
    "- High Dimensional Volume and Distance\n",
    "\n",
    "- The *curse of dimensionality*\n",
    "\n",
    "- Privacy, Ethics, explainability\n",
    "\n",
    "- Software choices, careers\n",
    "\n",
    "Our emphasis throughout has been on *doing* rather than *proving*:\n",
    "*just enough: progress at pace*\n",
    "\n",
    "This is an opportunity to think about some of the bigger contextual issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assigned Reading\n",
    "\n",
    "For this material you are recommended Chapter 1.5 of [UDL] \n",
    "and Chapter 12 of [MLFCES].\n",
    "\n",
    "- UDL: Understanding Deep Learning, by Simon J.D. Prince. PDF draft available here:\n",
    "  <https://udlbook.github.io/udlbook/>\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists, by Andreas Lindholm,\n",
    "  Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön. Cambridge University Press. \n",
    "  <http://smlbook.org>.\n",
    "- IML: Interpretable Machine Learning: A Guide for Making Black Box Models Explainable,\n",
    "  by Christoph Molnar. <https://christophm.github.io/interpretable-ml-book/>.\n",
    "- FDS: Foundations of Data Science, Avrim Blum, John Hopcroft, and Ravindran Kannan\n",
    "<https://www.cs.cornell.edu/jeh/book.pdf>.\n",
    "\n",
    "[FDS] is explicitly referred to below.\n",
    "\n",
    "These can be accessed legally and without cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Tensorflow Playground\n",
    "\n",
    "Tensorflow is a widely used deep learning library. \n",
    "\n",
    "This web page <http://playground.tensorflow.org> allows you to configure your\n",
    "own neural net.\n",
    "\n",
    "You can play with the hyperparameters, and see the effect for various \n",
    "classification problems.\n",
    "\n",
    "PyTorch is similarly widely used. There are also other software libraries but\n",
    "these are ones we seem (at the moment at least) to hear most about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy\n",
    "\n",
    "We used the TSE cost when we created our MNIST neural net classifier. An\n",
    "often used alternative - which is sometimes claimed as superior for \n",
    "classification problems - is the **cross entropy** cost.\n",
    "\n",
    "Let $X$ be a discrete random variable taking values $X_1,\\ X_2,\\ldots$, with \n",
    "corresponding probabilities $p_1 = P(X=X_1),\\ p_2 = P(X=X_2),\\ldots$.\n",
    "\n",
    "We define the **entropy** of $X$ as\n",
    "\n",
    "$$\n",
    "H(X)\n",
    "= -\\sum_{k=1,2,\\ldots} P(X = X_k) \\log_a(P(X = X_k))\n",
    "= -\\sum_{k=1,2,\\ldots} p_k \\log_a(p_k)\n",
    "$$\n",
    "\n",
    "for a logarithmic base $a$. When $a=2$ we often write $\\lg(z) = \\log_2(x)$ and \n",
    "measure entropy in *bits*. When $a=e$ we have, of course,\n",
    "$\\ln(z) = \\log_e(z)$, and entropy is then measured in *nats*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is a measure of the amount of **uncertainty** or **surprise** contained \n",
    "in the random variable. The larger the entropy, the more uncertainty there is. \n",
    "\n",
    "Consider a fair coin toss where $X \\in \\{H,T\\}$ with $p_1, p_2 = 1/2$.\n",
    "Then, in bits (don't get your $H$'s confused),\n",
    "\n",
    "$$\n",
    "H(X)\n",
    "= -\\sum_{k=1,2,\\ldots} p_k \\log_2(p_k)\n",
    "= -\\sum_{k=1,2} \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right)\n",
    "= 2\\times \\frac{1}{2} \\log_2(2) = 1.\n",
    "$$\n",
    "\n",
    "Next, consider a biased coin such that $X=H$ with unit probability.\n",
    "The entropy now is\n",
    "\n",
    "$$\n",
    "H(X)\n",
    "= -\\sum_{k=1,2,\\ldots} p_k \\log_2(p_k)\n",
    "= -1\\times\\log_2(1) + \\lim_{t\\downarrow 0} t\\log_2(t) = 0 + 0\n",
    "$$\n",
    "\n",
    "because $t\\log_2(t)\\to 0$ as $t\\downarrow 0$. Uncertainty is maximal in the\n",
    "first case, but in the second there is only certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information\n",
    "\n",
    "The background to this concept is that as $-\\log_a(P(X)) = \\log_a(1/P(X))$\n",
    "decreases as $P(X)$ increases it can be used to measure the amount of **information**\n",
    "or (perhaps more intuitively) **surprise** in observing the event $X$.\n",
    "\n",
    "If $X$ has very low probability then we're very surprised when it happens and the\n",
    "information, or surprisal, in that observation is high:\n",
    "$\\log_a(1/P(X))\\to\\infty$ as $P(X)\\downarrow 0$.\n",
    "\n",
    "On the other hand, if $P(X)=1$ then $\\log_a(1/P(X))=0$ and there is no (new)\n",
    "information or surprise when $X$ happens.\n",
    "\n",
    "So, writing information as $I(X) =-\\log_a(P(X))$, we see that the entropy is no more\n",
    "than the expected value, or average, of $I(X)$ under the given probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Entropy\n",
    "\n",
    "Let the vectors $\\boldsymbol{t} = (t_1, t_2, \\ldots, t_N)^T$ and\n",
    "$\\boldsymbol{y} = (y_1, y_2, \\ldots, y_N)^T$ each represent discrete probability \n",
    "mass functions (i.e. each component in $[0,1]$, they sum to $1$) \n",
    "**for the same event**.\n",
    "\n",
    "This may seem strange - how can an event have two\n",
    "different probabilities? For us, the idea here is that $\\boldsymbol{t}$\n",
    "represents some kind of **ground truth** for the event, while\n",
    "$\\boldsymbol{y}$ is an approximation to $\\boldsymbol{t}$. \n",
    "\n",
    "As a specific example we could have $\\boldsymbol{t} = (0.2, 0.1, 0.6, 0.1)^T$\n",
    "and $\\boldsymbol{y} = (0.1, 0.05, 0.65, 0.2)^T$. \n",
    "\n",
    "The expected value of the information contained in $\\boldsymbol{y}$ taken\n",
    "relative to the probability distribution of $\\boldsymbol{t}$ is called\n",
    "the **cross entropy** ($\\boldsymbol{y}$ relative to $\\boldsymbol{t}$).\n",
    "It is given by\n",
    "\n",
    "$$\n",
    "\\mathscr{F}(\\boldsymbol{t},\\boldsymbol{y})\n",
    "= - \\sum_{j=1,2,\\ldots} t_j \\log_a(y_j).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternatively, we might have $\\boldsymbol{t} = (0, 0, 1, 0)^T$ and\n",
    "$\\boldsymbol{y} = (0.1, 0.05, 0.65, 0.2)^T$ as a different example.\n",
    "\n",
    "Why and how might this happen? Well, we've seen this where $\\boldsymbol{t}$\n",
    "represents a one-hot encoding of the training/test labels and $\\boldsymbol{y}$\n",
    "is a neural net's output approximation to $\\boldsymbol{t}$.\n",
    "\n",
    "If we interpret the outputs as probabilities then we can end up with the\n",
    "situation just described, where the network attempts to approximate\n",
    "$\\boldsymbol{t}$ with $\\boldsymbol{y}$. Then\n",
    "\n",
    "$$\n",
    "\\mathscr{F}(\\boldsymbol{t},\\boldsymbol{y})\n",
    "= - \\sum_{j=1,2,\\ldots} t_j \\log_a(y_j)\n",
    "= - \\log_a(y_3).\n",
    "$$\n",
    "\n",
    "(because $t_3=1$ and $t_k=0$ if $k\\ne 3$ (in code indices start at zero).\n",
    "This tends to zero **from above** as $y_3\\uparrow t_3 = 1$ and gives us\n",
    "a new way to ascribe an **error** or **loss/cost** to the network.                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outputs as Probabilities\n",
    "\n",
    "The difficulty with what we have just discovered though is that we\n",
    "interpreted $\\boldsymbol{y} = (0.1, 0.05, 0.65, 0.2)^T$ as a probability\n",
    "mass function.\n",
    "\n",
    "In fact we have so safeguards in place in our neural net to make sure that\n",
    "this is justified. The sigmoid function forces each element of \n",
    "$\\boldsymbol{y}$ to be within the interval $(0,1)$ but doesn't force the values to\n",
    "sum to one. For example, \n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = (0.1, 0.05, 0.65, 0.2)^T\n",
    "\\qquad\\text{ and }\\qquad\n",
    "\\boldsymbol{y} = (0.13, 0.25, 0.75, 0.31)^T\n",
    "$$\n",
    "\n",
    "are - at least in principle - equally possible. The second doesn't sum to one.\n",
    "\n",
    "We get around this by swapping **sigmoid** for **softmax**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Softmax\n",
    "\n",
    "Let $\\boldsymbol{x}\\in\\mathbb{R}^n$ be an arbitrary vector. The **softmax function**,\n",
    "written $\\mathrm{SoftMax}\\colon\\mathbb{R}^n\\to\\mathbb{R}^n$ is defined by\n",
    "\n",
    "$$\n",
    "\\mathrm{SoftMax}(\\boldsymbol{x}) := \n",
    "\\frac{1}{\\displaystyle\\sum_{i=1}^n e^{x_i}}\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "e^{x_1} \\\\ e^{x_2} \\\\ \\vdots \\\\ e^{x_n}\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "In other words, the $j$-th component of the $n$-vector\n",
    "$\\mathrm{SoftMax}(\\boldsymbol{x})$ is given by\n",
    "\n",
    "$$\n",
    "\\big(\\mathrm{SoftMax}(\\boldsymbol{x})\\big)_j := \n",
    "\\frac{e^{x_j}}{e^{x_1}+e^{x_2}+\\cdots+e^{x_n}}.\n",
    "$$\n",
    "\n",
    "This function can be used on an output vector to create a probability mass function.\n",
    "Each component is within $(0,1)$ and they sum to one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Calculus for Learning\n",
    "\n",
    "To use these concepts and replace TSE cost with XE (cross entropy) cost\n",
    "we use $\\mathrm{SoftMax}$ as a final layer activation function, and\n",
    "for ground truth and outputs\n",
    "$\\boldsymbol{t}_k, \\boldsymbol{y}_k\\in\\mathbb{R}^d$, \n",
    "we define the cost as\n",
    "\n",
    "$$\n",
    "\\mathcal{E}(\\boldsymbol{W}_1,\\boldsymbol{W}_2,\\boldsymbol{W}_3,\n",
    "\\boldsymbol{b}_1,\\boldsymbol{b}_2,\\boldsymbol{b}_3)\n",
    "= \\sum_{k=1}^{N_{\\mathrm{train}}} \n",
    "\\mathscr{F}(\\boldsymbol{t}_k,\\boldsymbol{y}_k)\n",
    "$$\n",
    " \n",
    "$$\n",
    "\\text{for the loss}\\quad\n",
    "\\mathscr{F}_k := \\mathscr{F}(\\boldsymbol{t}_k,\\boldsymbol{y}_k) :=\n",
    "-\\sum_{j=1}^d t_{kj}\\ln(y_{kj})\n",
    "\\quad\\text{instead of (TSE)}\\quad\n",
    "\\mathscr{F}_k = \\Vert\\boldsymbol{t}_k-\\boldsymbol{y}_k\\Vert_2^2.\n",
    "$$\n",
    "\n",
    "And, as before, we just write $\\mathcal{E}$ for brevity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The backprop calculus we went over before still applies here except that instead \n",
    "of $\\boldsymbol{S} = -2\\boldsymbol{A}\\boldsymbol{e}$ at the final layer we\n",
    "would need to calculate \n",
    "\n",
    "$$\n",
    "\\boldsymbol{S} = \\frac{\\partial\\mathscr{F}}{\\partial \\boldsymbol{n}}\n",
    "\\quad\\text{with}\\quad\n",
    "\\boldsymbol{n} = \\boldsymbol{W}^T\\hat{\\boldsymbol{a}}+\\boldsymbol{b}.\n",
    "$$\n",
    "\n",
    "\n",
    "Note that we have dropped the subscript $k$ for clarity, and then \n",
    "component $c$ of this is\n",
    "\n",
    "$$\n",
    "S_c = -\\frac{\\partial}{\\partial n_c}\n",
    "\\sum_{j=1}^d t_{j}\\ln(y_{j})\n",
    "=\n",
    "-\\sum_{j=1}^d \\frac{t_{j}}{y_{j}}\n",
    "\\frac{\\partial y_{j}}{\\partial n_c}\n",
    "\\quad\\text{with}\\quad\n",
    "n_c = \\sum_r W_{rc}\\hat{a}_r+b_c\n",
    "$$\n",
    "\n",
    "and $\\boldsymbol{y}=\\sigma(\\boldsymbol{n})$ with $\\sigma$ \n",
    "being $\\mathrm{SoftMax}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, using $\\boldsymbol{y}=\\sigma(\\boldsymbol{n})$ with $\\sigma$ \n",
    "being $\\mathrm{SoftMax}$, we calculate,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial y_{j}}{\\partial n_c}\n",
    "& = \\frac{\\partial}{\\partial n_c}\n",
    "\\frac{e^{n_j}}{e^{n_1}+e^{n_2}+\\cdots+e^{n_d}}  =\n",
    "\\frac{\\left(\\sum_k e^{n_k}\\right)\\left(\\frac{\\partial}{\\partial n_c} e^{n_j}\\right) - e^{n_j}e^{n_c}}{\\left(\\sum_k e^{n_k}\\right)^2}\n",
    "\\\\\n",
    "& = \\left\\{\\begin{array}{ll}\n",
    "\\frac{e^{n_c}}{\\left(\\sum_k e^{n_k}\\right)}\n",
    "-\\left(\n",
    "\\frac{e^{n_c}}{\\sum_k e^{n_k}}\n",
    "\\right)^2\n",
    "&\\text{if }j = c;\n",
    "\\\\\n",
    "-\\frac{e^{n_j}}{\\sum_k e^{n_k}}\\times\n",
    "\\frac{e^{n_c}}{\\sum_k e^{n_k}}\n",
    "&\\text{if }j \\ne c,\n",
    "\\end{array}\\right.\n",
    "\\\\\n",
    "& = \\left\\{\\begin{array}{ll}\n",
    "\\sigma(n_c)\\big(1-\\sigma(n_c)\\big)\n",
    "&\\text{if }j = c;\n",
    "\\\\\n",
    "-\\sigma(n_c)\\sigma(n_j)\n",
    "&\\text{if }j \\ne c,\n",
    "\\end{array}\\right.\n",
    "\\\\\n",
    "& = \\left\\{\\begin{array}{ll}\n",
    "y_c\\big(1-y_c\\big)\n",
    "&\\text{if }j = c;\n",
    "\\\\\n",
    "-y_c y_j\n",
    "&\\text{if }j \\ne c.\n",
    "\\end{array}\\right.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore\n",
    "\n",
    "$$\n",
    "S_c = -\\sum_{j=1}^d \\frac{t_{j}}{y_{j}}\n",
    "\\frac{\\partial y_{j}}{\\partial n_c}\n",
    "= -\\sum_{j=1}^d \\frac{t_{j}}{y_{j}}\\left\\{\\begin{array}{ll}\n",
    "y_c\\big(1-y_c\\big)\n",
    "&\\text{if }j = c;\n",
    "\\\\\n",
    "-y_c y_j\n",
    "&\\text{if }j \\ne c,\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "which simplifies to,\n",
    "\n",
    "$$\n",
    "S_c = \n",
    "-\\frac{t_c}{y_c}\\times y_c\\big(1-y_c\\big)\n",
    "-\\sum_{j=1\\atop j\\ne c}^d \\frac{t_{j}}{y_{j}}\\big(-y_c y_j\\big)\n",
    "=\n",
    "t_c(y_c-1)\n",
    "+y_c\\sum_{j=1\\atop j\\ne c}^d t_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One more push and we get,\n",
    "\n",
    "$$\n",
    "S_c = \n",
    "-t_c\n",
    "+y_c\\sum_{j=1}^d t_j = -(-t_c-y_c) = -e_c\n",
    "$$\n",
    "\n",
    "for $\\boldsymbol{e}=\\boldsymbol{t}-\\boldsymbol{y}$ and because \n",
    "$\\boldsymbol{t}$ is **one hot**.\n",
    "\n",
    "In summary, to use **cross entropy** cost we put \n",
    "$\\boldsymbol{S} = -\\boldsymbol{e}$ at the final layer and use\n",
    "$\\mathrm{SoftMax}$ as the final layer activation function.\n",
    "\n",
    "The **SAWS** algortihm is then applied just as before to \n",
    "backprop:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_{i-1}\n",
    "=\\boldsymbol{A}_{i-1}\\boldsymbol{W}_i\\boldsymbol{S}_i.\n",
    "$$\n",
    "\n",
    "We'll leave the implementation as a homework - here's the big picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Forward and Backward Propagation ('backprop') Algorithm - Learning from Data\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{array}{rl}\n",
    "\\text{forward} &\\text{prop} \n",
    "\\\\\\ \\\\\n",
    "\\boldsymbol{a}_0 & = \\boldsymbol{x},\n",
    "\\\\\n",
    "\\boldsymbol{n}_1 & = \\boldsymbol{W}_1^T\\boldsymbol{a}_0+\\boldsymbol{b}_1,\n",
    "\\\\\n",
    "\\boldsymbol{a}_1 & = \\sigma_1(\\boldsymbol{n}_1),\n",
    "\\\\\n",
    "\\boldsymbol{n}_2 & = \\boldsymbol{W}_2^T\\boldsymbol{a}_1+\\boldsymbol{b}_2,\n",
    "\\\\\n",
    "\\boldsymbol{a}_2 & = \\sigma_2(\\boldsymbol{n}_2),\n",
    "\\\\\n",
    "\\boldsymbol{n}_3 & = \\boldsymbol{W}_3^T\\boldsymbol{a}_2+\\boldsymbol{b}_3,\n",
    "\\\\\n",
    "\\boldsymbol{a}_3 & = \\sigma_\\mathrm{SoftMax}(\\boldsymbol{n}_3),\n",
    "\\\\\n",
    "\\boldsymbol{y} & = \\boldsymbol{a}_3.\n",
    "\\end{array}\n",
    "\\qquad &\\qquad\n",
    "\\begin{array}{rl}\n",
    "\\text{back} &\\text{prop with XE}\n",
    "\\\\\\ \\\\\n",
    "\\boldsymbol{e}_k & = \\boldsymbol{t}_k-\\boldsymbol{y}_k,\n",
    "\\\\\n",
    "\\boldsymbol{S}_3 & = -\\boldsymbol{e}_k,\n",
    "\\\\\n",
    "\\boldsymbol{W}_3 & \\leftarrow \\boldsymbol{W}_3\n",
    "- \\alpha \\boldsymbol{a}_2 \\boldsymbol{S}_3^T\n",
    "\\text{ and }\n",
    "\\boldsymbol{b}_3 \\leftarrow \\boldsymbol{b}_3\n",
    "- \\alpha \\boldsymbol{S}_3\n",
    "\\\\\n",
    "\\boldsymbol{S}_2 & = \\boldsymbol{A}_2\\boldsymbol{W}_3\\boldsymbol{S}_3\n",
    "\\\\\n",
    "\\boldsymbol{W}_2 & \\leftarrow \\boldsymbol{W}_2\n",
    "- \\alpha \\boldsymbol{a}_1 \\boldsymbol{S}_2^T\n",
    "\\text{ and }\n",
    "\\boldsymbol{b}_2 \\leftarrow \\boldsymbol{b}_2\n",
    "- \\alpha \\boldsymbol{S}_2\n",
    "\\\\\n",
    "\\boldsymbol{S}_1 & = \\boldsymbol{A}_1\\boldsymbol{W}_2\\boldsymbol{S}_2\n",
    "\\\\\n",
    "\\boldsymbol{W}_1 & \\leftarrow \\boldsymbol{W}_1\n",
    "- \\alpha \\boldsymbol{a}_0 \\boldsymbol{S}_1^T\n",
    "\\text{ and }\n",
    "\\boldsymbol{b}_1 \\leftarrow \\boldsymbol{b}_1\n",
    "- \\alpha \\boldsymbol{S}_1\n",
    "\\end{array}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### High Dimensional Volume\n",
    "\n",
    "We have seen many examples of data represented as vectors in high dimensional\n",
    "space. The most extreme example we worked with were the MNIST digits, each of\n",
    "which was represented as a vector of $28^2$ pixel values.\n",
    "\n",
    "It is interesting to contemplate how high dimensional space behaves. It is\n",
    "a bit counter intuitive.\n",
    "\n",
    "Consider a hypercube with unit side lengths and consider what happens when \n",
    "we form cartesian prodcuts of the unit interval $[0,1]$ to reach higher and\n",
    "higher dimensions.\n",
    "\n",
    "For example, $[0,1]^1=[0,1]$ is a line segment, $[0,1]^2=[0,1]\\times [0,1]$\n",
    "is a square, $[0,1]^3=[0,1]\\times [0,1]\\times [0,1]$ a cube.\n",
    "\n",
    "Continuing, $[0,1]^d=[0,1]\\times[0,1]\\times\\cdots\\times[0,1]$ ($d$-times) is\n",
    "a hypercube in $\\mathbb{R}^d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The volume of a hypercube is just the $d$-th power of its side length:\n",
    "$d^2$ for a square, for example.\n",
    "\n",
    "Consider a hypercube, $C_1$, with unit side length, $1$, and a\n",
    "shrunk hypercube, $C_2$, with side length $1-\\epsilon$,\n",
    "for some small $\\epsilon > 0$.\n",
    "\n",
    "Look at the volume ratio:\n",
    " \n",
    "$$\n",
    "\\frac{\\mathrm{Volume}(C_2)}{\\mathrm{Volume}(C_1)}\n",
    "= \n",
    "\\frac{(1-\\epsilon)^d}{1^d}\n",
    "= \n",
    "\\left(1-\\frac{\\epsilon d}{d}\\right)^d \\to \\exp(-\\epsilon d)\n",
    "\\quad\\text{as}\\quad d\\to\\infty.\n",
    "$$\n",
    "\n",
    "This nice illustration comes from [FDS, Chapter 2.3]\n",
    "\n",
    "> FDS: Foundations of Data Science, Avrim Blum, John Hopcroft, and Ravindran Kannan\n",
    "<https://www.cs.cornell.edu/jeh/book.pdf>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you're wondering about that limit, recall the binomial expansion,\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(1+\\frac{x}{n}\\right)^n \n",
    "& = 1 + x + \\frac{n(n-1)}{2!}\\left(\\frac{x}{n}\\right)^2\n",
    "+ \\frac{n(n-1)(n-2)}{3!}\\left(\\frac{x}{n}\\right)^3 + \\cdots,\n",
    "\\\\\n",
    "& = 1 + x + \\frac{n(n-1)}{n^2}\\frac{x^2}{2!}\n",
    "+ \\frac{n(n-1)(n-2)}{n^3}\\frac{x^3}{3!} + \\cdots,\n",
    "\\\\\n",
    "& = 1 + x + \\left(1-\\frac{1}{n}\\right)\\frac{x^2}{2!}\n",
    "+ \\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)\\frac{x^3}{3!} + \\cdots,\n",
    "\\\\\n",
    "\\\\\n",
    "& \\to 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots,\n",
    "\\\\\n",
    "& = e^x\\quad\\text{ as }n\\to\\infty.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the volume ratio $\\exp(-\\epsilon d)$ for various $\\epsilon$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dvals = np.arange(1,1000)\n",
    "eps = 0.01; plt.semilogx(dvals, np.exp(-eps*dvals), label=f'eps={eps}')\n",
    "eps = 0.05; plt.semilogx(dvals, np.exp(-eps*dvals), label=f'eps={eps}')\n",
    "eps = 0.10; plt.semilogx(dvals, np.exp(-eps*dvals), label=f'eps={eps}')\n",
    "plt.legend(); plt.xlabel('d'); plt.ylabel('exp(-eps * d)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Comments? Think about $k$-Nearest Neighbours in high dimensions.\n",
    "In $100$ dimensions, most of the volume (and data? and neighbours?) \n",
    "would be in the $5\\%$ shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distance\n",
    "\n",
    "Denote the vector stemming from the origin to a corner of the\n",
    "unit hypercube by $\\boldsymbol{1}$ and the vector\n",
    "connecting to the corresponding shrunk corner by\n",
    "$\\boldsymbol{1}-\\boldsymbol{1}\\epsilon$.\n",
    "\n",
    "The ends of these vectors are connected by the vector\n",
    "$\\boldsymbol{1}-(\\boldsymbol{1}-\\boldsymbol{1}\\epsilon)=\\boldsymbol{1}\\epsilon$,\n",
    "and this has $p$-norm length,\n",
    "\n",
    "$$\n",
    "\\Vert\\boldsymbol{1}\\epsilon\\Vert_p = \\left(\n",
    "\\sum_{n=1}^d\\vert\\epsilon\\vert^p\\right)^{1/p} = \\epsilon d^{1/p}.\n",
    "$$ \n",
    "\n",
    "This tells us that the thickness of the shell behaves like \n",
    "$d^{1/p}$ which grows more and more slowly as $p\\to\\infty$.\n",
    "\n",
    "Counterintuitive? High dimensional space is a bit like that...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Curse of Dimensionality\n",
    "\n",
    "This generic refers to the fact that often things get **harder** as we move to\n",
    "higher and higher dimensions. \n",
    "\n",
    "Consider $k$-NN with the 2-norm in $\\mathbb{R}^d$. If there are $N$ data points \n",
    "then when a new point arrives we have to calculate the $N$ distances to each\n",
    "neighbour.\n",
    "\n",
    "In $\\mathbb{R}^d$ the two norm requires $d$ squares to be calculated for each\n",
    "data point.\n",
    "\n",
    "This is a total of $Nd^2$ operations - and has a quadratic effect on computer time. \n",
    "\n",
    "There's actually a lot more to it - it relates again to the weirdness of high\n",
    "dimensional space. Here's an interesting read:\n",
    "\n",
    "<https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anonymity and Privacy\n",
    "\n",
    "This discussion is based on the material here:\n",
    "\n",
    " - <https://www.johndcook.com/blog/2018/12/07/simulating-zipcode-sex-birthdate/>\n",
    " - <https://techscience.org/a/2015092903/>\n",
    " \n",
    "### How Anonymous is Anonymised Data?\n",
    "\n",
    "- A university collects answers to personal questions from all of its students.\n",
    "- Each student's answer has their name, date of birth, gender and department.\n",
    "- On average a department has 250 students in each year.\n",
    "- We assume the UK setup where students attend for three years.\n",
    "- The names are erased: how anonymous are the resulting data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "| Name          | D.o.B.   | Gender | Department  |\n",
    "| ------------- | -------- | ------ | ----------- |\n",
    "| Ringo Starr   | 26/07/43 |   M    | Music       |\n",
    "| Al Gebra      | 12/08/05 |   F    | Maths       |\n",
    "| Sandie Shaw   | 16/02/38 |   F    | Puppetry    |\n",
    "| Michael Mouse | 17/04/92 |   F    | Computing   |\n",
    "| Mr Pink       | 4/12/56  |   M    | Criminology |\n",
    "| L.O. Gear     | 11/9/23  |   F    | Automotive  |\n",
    "| Donkey Kong   | 23/10/73 |   M    | Video Games |\n",
    "|     :         |     :    |   :    |     :       |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```\n",
    "| Name          | D.o.B.   | Gender | Department  |\n",
    "| ------------- | -------- | ------ | ----------- |\n",
    "|               | 26/07/43 |   M    | Music       |\n",
    "|               | 12/08/05 |   F    | Maths       |\n",
    "|               | 16/02/38 |   F    | Puppetry    |\n",
    "|               | 17/04/92 |   F    | Computing   |\n",
    "|               | 4/12/56  |   M    | Criminology |\n",
    "|               | 11/9/23  |   F    | Automotive  |\n",
    "|               | 23/10/73 |   M    | Video Games |\n",
    "|               |     :    |   :    |     :       |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The names are erased - can one line identify the person?\n",
    "\n",
    "- Is DoB, Gender and Department enough information to identify the person?\n",
    "\n",
    "How anonymous is a dataset like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We assume that all students are born within a three year window, and that each\n",
    "department has 750 students across its three years.\n",
    "\n",
    "- There are $365\\times 3 = 1095$ possible birthdays\n",
    "\n",
    "- For each there are at least $2$ possible genders\n",
    "\n",
    "- So, for a given department, there are $d = 1095\\times 2 = 2190$\n",
    "possible entries among $N=750$ students.\n",
    "\n",
    ">** Can you see why anonymity might not be assured?**\n",
    "\n",
    "- Think about a line of $d=2190$ empty buckets.\n",
    "\n",
    "- Now throw $N=750$ balls at random into the buckets.\n",
    "\n",
    "- Most will stay empty. Some will have just one ball - the 'loners'.\n",
    "\n",
    "The proportion of buckets having just one ball estimates the probabilty\n",
    "that line of anonymised data occurs just once in the department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computational Simulation - Planning the Code\n",
    "\n",
    "- We are going to generate a list of $d = 2190$ zeros.\n",
    "\n",
    "- We'll then generate $N=750$ random integers $z\\in\\{1,2,\\ldots,2190\\}$.\n",
    "\n",
    "- For each $z$ we'll add one to the $z^{\\mathrm{th}}$ item in the list,\n",
    "$d$.\n",
    "\n",
    "- In the end, the $n^{\\mathrm{th}}$ item in the list, $d$, tells us\n",
    "how many students share that same data.\n",
    "\n",
    "- We want to find the 'loners' - the buckets with only one item in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "d = 365*3*2\n",
    "N = 750\n",
    "buckets = np.zeros(d)\n",
    "\n",
    "for _ in range(N):\n",
    "    z = randrange(d)\n",
    "    buckets[z] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Look at how many buckets get more than one item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(buckets, range(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's estimate the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loners = len(buckets[buckets==1])\n",
    "print('Probability that anonymous data occurs only once: ', loners/N)\n",
    "print('Nearly exact probability that anonymous data occurs only once: ', np.exp(-N/d))\n",
    "\n",
    "print(f'number of buckets = {buckets.shape[0]}')\n",
    "print(f'number of items in buckets = {sum(buckets)}')\n",
    "print(f'number of non-empty buckets {len(buckets[buckets!=0])}')\n",
    "\n",
    "for k in range(5):\n",
    "  print(f'> number of buckets with {k} items {len(buckets[buckets==k])}')\n",
    "print(f'> number of buckets with >4 items {len(buckets[buckets>4])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is about a $70\\%$ probability that a student can be identified from\n",
    "this anonymised data.\n",
    "\n",
    "As indicated above, a 'near exact' solution is $\\exp(-N/d) \\approx 71\\%$.\n",
    "\n",
    "My main reference for this was John D Cook:\n",
    "\n",
    "<http://www.johndcook.com/blog/2018/12/07/simulating-zipcode-sex-birthdate/>\n",
    "\n",
    "Which itself references the paper **Only You, Your Doctor, and Many Others May Know**,\n",
    "by Latanya Sweeney: <https://techscience.org/a/2015092903/>\n",
    "\n",
    ">**THNK ABOUT**: as a data scientist you may be consulted on how to curate data.\n",
    "Bear this experiment in mind. Anonymity isn't as straightforward as it may seem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Homework \n",
    "\n",
    "Here is some more code. Have a play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loners2 = len(buckets[buckets==2])\n",
    "print('Probability that anonymous data occurs at most twice: ', (loners+2*loners2)/N)\n",
    "\n",
    "loners3 = len(buckets[buckets==3])\n",
    "print('Probability that anonymous data occurs at most three times: ', (loners+2*loners2+3*loners3)/N)\n",
    "\n",
    "loners4 = len(buckets[buckets==4])\n",
    "print('Probability that anonymous data occurs at most four times: ', (loners+2*loners2+3*loners3+4*loners4)/N)\n",
    "\n",
    "# a check\n",
    "print( len(buckets[buckets==1])+2*len(buckets[buckets==2])+3*len(buckets[buckets==3])+4*len(buckets[buckets==4]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ethics\n",
    "\n",
    "The discussion above touches on the more general topic of \n",
    "ethics and the responsible use of data, and AI.\n",
    "\n",
    "There's a free-to-view MOOC here if you're interested in this:\n",
    "\n",
    "<https://ethics-of-ai.mooc.fi/chapter-1/1-a-guide-to-ai-ethics>\n",
    "\n",
    "It's a big topic. \n",
    "\n",
    "We already explored an issue around **fairness** with the example\n",
    "based on the discussion in Chapter 12 of our reference book [MLFCES]:\n",
    "\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists,\n",
    "by Andreas Lindholm, Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön.\n",
    "Cambridge University Press. <http://smlbook.org>\n",
    "\n",
    "Other topics touched on there include a discussion of **misleading claims\n",
    "about performance** (in five years time we'll be able to ...) and \n",
    "**explainability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explainable AI\n",
    "\n",
    "This is regarded as a very serious topic. AI's may soon (already are?) make lots of\n",
    "important decisions (life or death or financial or societal or ...).\n",
    "\n",
    "Some of those decisions may be poor. How can\n",
    "we make judgements about whether the mistake is justified or not?\n",
    "\n",
    "A self-driving car may have to choose between hitting a child that ran into the\n",
    "road, or driving its passengers off the road into the neightbouring sea.\n",
    "\n",
    "How can we prepare an AI for that? How can we probe its decision after the fact?\n",
    "\n",
    "It may not matter if your social media app wrongly face-tagged your best friend\n",
    "as your sister, but what about in financial and medical scenarios? What about\n",
    "in high-hazard zones? \n",
    "\n",
    "**Would you travel in a self-flying aeroplane?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A rather advanced treatment can be found in [IML]\n",
    "\n",
    "**Interpretable Machine Learning:**\n",
    "**A Guide for Making Black Box Models Explainable**, by\n",
    "Christoph Molnar,\n",
    "\n",
    "and available here: <https://christophm.github.io/interpretable-ml-book/>.\n",
    "\n",
    "We can't even begin to engage with this material in any depth, but think\n",
    "about what you have learned in the last couple of weeks...\n",
    "\n",
    "You can completely explain a linear regression model. Typically, you provide\n",
    "the data, solve the normal equations, thus minimising cost, and arrive at\n",
    "a polynomial you can plot.\n",
    "\n",
    "Consider your MNIST classifying neural network. When it mis-classifies can you\n",
    "even begin to explain why? Which weight(s) or bias(es) caused the error? Why\n",
    "they have that (wrong?) value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Responsible AI\n",
    "\n",
    "Chapter 13 of **A Hands-On Introduction to Machine Learning, by Chirag Shah**\n",
    "\n",
    "<https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview>\n",
    "\n",
    "also has some excellent content on these ideas. Unfortunately this book is not \n",
    "free like the main references that we have been using. The kind of\n",
    "topics to think about are\n",
    "\n",
    "- dataset bias: when an ML tool is trained on a dataset then the \n",
    "AI that tool demonstrates is likely to inherit flaws in the data set.\n",
    "\n",
    "This is obvious if the data is wrong because the training will be too.\n",
    "\n",
    "More serious though is that many datasets are biased. White males may\n",
    "for example dominate the dataset. The ML tool may then learn to be \n",
    "racist and mysogynistic.\n",
    "\n",
    "This isn't just hypothetical - there is a well known case related to\n",
    "Amazon's use of AI in hiring:\n",
    "<https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chapter 1.5 of [UDL], 'Ethics', is freely available and gives a very \n",
    "useful summary. It touches on **Bias and Fairness** and **Explainability**\n",
    "and also on:\n",
    "\n",
    "- **Weaponizing AI:** humans seem not to be able to resist creating more and\n",
    "more devesatating weapons. What about an armed AI drone that knows no fear?\n",
    "\n",
    "- **Concentrating power:** do the rich nations reap all the reards of AI without\n",
    "bringing the rest along with them? Is that moral? Is it de-stabilising?\n",
    "\n",
    "- **Existential risk:** will we keep control?\n",
    "\n",
    "Stuart Russell in **Human Compatible** discusses a\n",
    "coffee making robot with a cost function based on keeping\n",
    "humans happy with coffee deliveries to their desk.\n",
    "\n",
    "It learns that **it can't make the coffee if I'm dead**, and so\n",
    "it must stay alive to keep its cost function optimized. The next step is to learn\n",
    "that it **must not** allow itself to be switched off... That requires defence...\n",
    "\n",
    "\n",
    "Russell's book is not free, but well worth it:\n",
    "<https://people.eecs.berkeley.edu/~russell/hc.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Software Choices and Careers\n",
    "\n",
    "I am quite limited in what I can say about this.\n",
    "\n",
    "- My understanding is that `python` is dominant in data science.\n",
    "\n",
    "- I also believe that SQL is an essential skill for data acquisition and manipulation.\n",
    "\n",
    "- I suggest getting good at version control: `git` with remote storage seems to dominate.\n",
    "\n",
    "- Visualization has always been important and always will be: `seaborn` is a good start,\n",
    "`R` is also good - take a look at what is out there.\n",
    "\n",
    "- Get an online profile. **Linked In** for example, post your projects there, connect to\n",
    "your online `git` repositories.\n",
    "\n",
    "- Participate in kaggle (<https://www.kaggle.com>) and Hugging face (<https://huggingface.co>).\n",
    "\n",
    "- I don't think commercial `Data Scientists` or `Analysts` exist without context: pick your area\n",
    "(science, health, finance, sociological, economic, ...) and get to know your way around it.\n",
    "\n",
    "- **ALWAYS ALWAYS ALWAYS ALWAYS** cite your sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "For the last time, we covered *just enough*, to make *progress at pace*.\n",
    "We looked at\n",
    "\n",
    "- XE, SoftMax and backprop\n",
    "- Privacy\n",
    "- Ethics, explainability and responsible AI\n",
    "\n",
    "Again, and as usual, there is much much more. There always is... \n",
    "\n",
    "For example: bagging, boosting, dropout and regularization... - just a few.\n",
    "\n",
    "**Thanks for taking this journey with me.**\n",
    "\n",
    "**Best Wishes and Good Luck.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Technical Notes, Production and Archiving\n",
    "\n",
    "Ignore the material below. What follows is not relevant to the material being taught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Production Workflow\n",
    "\n",
    "- Finalise the notebook material above\n",
    "- Clear and fresh run of entire notebook\n",
    "- Create html slide show:\n",
    "  - `jupyter nbconvert --to slides 15_further.ipynb `\n",
    "- Set `OUTPUTTING=1` below\n",
    "- Comment out the display of web-sourced diagrams\n",
    "- Clear and fresh run of entire notebook\n",
    "- Comment back in the display of web-sourced diagrams\n",
    "- Clear all cell output\n",
    "- Set `OUTPUTTING=0` below\n",
    "- Save\n",
    "- git add, commit and push to FML\n",
    "- copy PDF, HTML etc to web site\n",
    "  - git add, commit and push\n",
    "- rebuild binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some of this originated from\n",
    "\n",
    "<https://stackoverflow.com/questions/38540326/save-html-of-a-jupyter-notebook-from-within-the-notebook>\n",
    "\n",
    "These lines create a back up of the notebook. They can be ignored.\n",
    "\n",
    "At some point this is better as a bash script outside of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NBROOTNAME=15_further\n",
    "OUTPUTTING=0\n",
    "\n",
    "if [ $OUTPUTTING -eq 1 ]; then\n",
    "  jupyter nbconvert --to html $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.html ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.html\n",
    "  mv -f $NBROOTNAME.html ./formats/html/\n",
    "\n",
    "  jupyter nbconvert --to pdf $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.pdf ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.pdf\n",
    "  mv -f $NBROOTNAME.pdf ./formats/pdf/\n",
    "\n",
    "  jupyter nbconvert --to script $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.py ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.py\n",
    "  mv -f $NBROOTNAME.py ./formats/py/\n",
    "else\n",
    "  echo 'Not Generating html, pdf and py output versions'\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
