{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix Decompositions\n",
    "\n",
    "#### *variationalform* <https://variationalform.github.io/>\n",
    "\n",
    "#### *Just Enough: progress at pace*\n",
    "\n",
    "<https://variationalform.github.io/>\n",
    "\n",
    "<https://github.com/variationalform>\n",
    "\n",
    "Simon Shaw\n",
    "<https://www.brunel.ac.uk/people/simon-shaw>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<p>\n",
    "This work is licensed under CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)\n",
    "\n",
    "<p>\n",
    "Visit <a href=\"http://creativecommons.org/licenses/by-sa/4.0/\">http://creativecommons.org/licenses/by-sa/4.0/</a> to see the terms.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>This document uses python</td>\n",
    "<td>\n",
    "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>and also makes use of LaTeX </td>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/LaTeX_logo.svg/320px-LaTeX_logo.svg.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>in Markdown</td> \n",
    "<td>\n",
    "<img src=\"https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What this is about:\n",
    "\n",
    "You will be introduced to ...\n",
    "\n",
    "- Methods for decomposing matrices in to alternative forms.\n",
    "- Eigenvalues and Eigenvectors of square matrices.\n",
    "- The Singular Value Decomposition (SVD) for non-square matrices\n",
    "- Using `numpy` to compute these decompositions.\n",
    "\n",
    "As usual our emphasis will be on *doing* rather than *proving*:\n",
    "*just enough: progress at pace*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assigned Reading\n",
    "\n",
    "For this worksheet you should read Chapter 7 of [FCLA], and\n",
    "Chapters 4 of [MML].\n",
    "\n",
    "- MML: Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong.\n",
    "  Cambridge University Press. <https://mml-book.github.io>.\n",
    "- FCLA: A First Course in Linear Algebra, by Ken Kuttler, \n",
    "  <https://math.libretexts.org/Bookshelves/Linear_Algebra/A_First_Course_in_Linear_Algebra_(Kuttler)>\n",
    " \n",
    "All of the above can be accessed legally and without cost.\n",
    "\n",
    "There are also these useful references for coding:\n",
    "\n",
    "- PT: `python`: <https://docs.python.org/3/tutorial>\n",
    "- NP: `numpy`: <https://numpy.org/doc/stable/user/quickstart.html>\n",
    "- MPL: `matplotlib`: <https://matplotlib.org>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "Consider this matrix and vector,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\n",
    "= \\left(\\begin{array}{rrr}\n",
    " 3 & -2 & 4 \\\\\n",
    "-6 & 6 & -11 \\\\\n",
    " 6 & 2 & 5 \\\\\n",
    "\\end{array}\\right)\n",
    "\\qquad\\text{ and }\\qquad\n",
    "\\boldsymbol{x}\n",
    "= \\left(\\begin{array}{r}\n",
    "-2 \\\\\n",
    " 3 \\\\\n",
    " 1 \\\\\n",
    "\\end{array}\\right)\n",
    "\\qquad\\text{ then }\\qquad\n",
    "\\boldsymbol{B}\\boldsymbol{x}\n",
    "= \\left(\\begin{array}{r}\n",
    "-8 \\\\\n",
    " 19 \\\\\n",
    "-1 \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "and notice that $\\boldsymbol{B}$ *mixes up* $\\boldsymbol{x}$ to such an\n",
    "extent that $\\boldsymbol{B}\\boldsymbol{x}$ bears no relationship to the\n",
    "original $\\boldsymbol{x}$. This isn't so surprising when you think about\n",
    "it.\n",
    "\n",
    "We can do this in `python` using `numpy` as follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "B = np.array( [[3, -2, 4],[-6, 6, -11],[ 6, 2, 5 ]])\n",
    "x = np.array([[-2], [3], [1]])\n",
    "f = B.dot(x)\n",
    "print('f = \\n', f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the other hand, consider this matrix with a different vector,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\n",
    "= \\left(\\begin{array}{rrr}\n",
    " 3 & -2 & 4 \\\\\n",
    "-6 & 6 & -11 \\\\\n",
    " 6 & 2 & 5 \\\\\n",
    "\\end{array}\\right)\n",
    "\\qquad\\text{ and }\\qquad\n",
    "\\boldsymbol{w}\n",
    "= \\left(\\begin{array}{r}\n",
    "-2 \\\\\n",
    " 5 \\\\\n",
    " 2 \\\\\n",
    "\\end{array}\\right).\n",
    "$$ This time,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\\boldsymbol{w}\n",
    "= \\left(\\begin{array}{rrr}\n",
    " 3 & -2 & 4 \\\\\n",
    "-6 & 6 & -11 \\\\\n",
    " 6 & 2 & 5 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{r}\n",
    "-2 \\\\\n",
    " 5 \\\\\n",
    " 2 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{r}\n",
    "-8 \\\\\n",
    " 20 \\\\\n",
    " 8 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "4\\left(\\begin{array}{r}\n",
    "-2 \\\\\n",
    " 5 \\\\\n",
    " 2 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "4\\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "So $\\boldsymbol{B}\\boldsymbol{w}=4\\boldsymbol{w}$, and all\n",
    "$\\boldsymbol{B}$ does is magnify $\\boldsymbol{w}$ to be $4$ times\n",
    "longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here it is again,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\\boldsymbol{w}\n",
    "= \\left(\\begin{array}{rrr}\n",
    " 3 & -2 & 4 \\\\\n",
    "-6 & 6 & -11 \\\\\n",
    " 6 & 2 & 5 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{r}\n",
    "-2 \\\\\n",
    " 5 \\\\\n",
    " 2 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{r}\n",
    "-8 \\\\\n",
    " 20 \\\\\n",
    " 8 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "4\\left(\\begin{array}{r}\n",
    "-2 \\\\\n",
    " 5 \\\\\n",
    " 2 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "4\\boldsymbol{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Exercise:\n",
    "use `python` to show that $\\frac{1}{4}\\boldsymbol{B}\\boldsymbol{w}-\\boldsymbol{w}=\\boldsymbol{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "B = np.array( [[3, -2, 4],[-6, 6, -11],[ 6, 2, 5 ]])\n",
    "w = np.array([[-2], [5], [2]])\n",
    "f = 0.25*B.dot(w)\n",
    "print('f - w = \\n', f-w) \n",
    "# or many other variants, such as\n",
    "print('result = \\n', 0.25*B.dot(w)-w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\boldsymbol{B}\\boldsymbol{w}\n",
    "= \\left(\\begin{array}{rrr}\n",
    " 3 & -2 & 4 \\\\\n",
    "-6 & 6 & -11 \\\\\n",
    " 6 & 2 & 5 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{r}\n",
    "-2 \\\\\n",
    " 5 \\\\\n",
    " 2 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{r}\n",
    "-8 \\\\\n",
    " 20 \\\\\n",
    " 8 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "4\\left(\\begin{array}{r}\n",
    "-2 \\\\\n",
    " 5 \\\\\n",
    " 2 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "4\\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "This seems more surprising. Here $4$ is called an *eigenvalue* ('own\n",
    "value') of $\\boldsymbol{B}$ and $\\boldsymbol{w}$ is the corresponding\n",
    "*eigenvector*.\n",
    "\n",
    "In general, for any square matrix $\\boldsymbol{B}$ the problem of\n",
    "finding scalars $\\lambda$ and vectors $\\boldsymbol{v}$ such that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\\boldsymbol{v}=\\lambda\\boldsymbol{v}\n",
    "$$ is called an *eigenvalue problem*. The following facts are known to\n",
    "be true:\n",
    "\n",
    "> **The Eigenvalue Theorem.** Every square matrix of dimension $n$ has\n",
    "> $n$ eigenvalue-eigenvector pairs,\n",
    "> $(\\lambda_1,\\boldsymbol{v}_1), (\\lambda_2,\\boldsymbol{v}_2),\\ldots, (\\lambda_n,\\boldsymbol{v}_n)$.\n",
    "> The eigenvalues need not be distinct, and the eigenvector lengths are\n",
    "> arbitrary. **A matrix which has one or more zero eigenvalues is not\n",
    "> invertible.** On the other hand, **If the eigenvalues of a matrix are\n",
    "> all non-zero then that matrix is invertible**, and it has **full\n",
    "> rank**. The determinant of a matrix is the product of its eigenvalues.\n",
    "\n",
    "**NOTE:**\n",
    "if $\\boldsymbol{B}\\boldsymbol{v}=\\lambda\\boldsymbol{v}$ then it can be shown\n",
    "that we need $\\det(\\boldsymbol{B}-\\lambda\\boldsymbol{I})=0$. This is not practically\n",
    "useful, but is of central importance for theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1\n",
    "\n",
    "For $\\boldsymbol{B}$ above we have that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\\boldsymbol{v}=\\lambda\\boldsymbol{v}\n",
    "$$\n",
    "\n",
    "for\n",
    "$(\\lambda_1,\\boldsymbol{v}_1), (\\lambda_2,\\boldsymbol{v}_2), (\\lambda_3,\\boldsymbol{v}_3)$\n",
    "given by\n",
    "\n",
    "$$\n",
    "9 \\text{ with }\n",
    "\\left(\\begin{array}{r}\n",
    " 17 \\\\\n",
    "-45 \\\\   \n",
    " 3 \\\\\n",
    "\\end{array}\\right),\n",
    "\\qquad\\qquad\n",
    "1 \\text{ with }\n",
    "\\left(\\begin{array}{r}\n",
    "-1 \\\\\n",
    " 1 \\\\\n",
    " 1\n",
    "\\end{array}\\right)\n",
    "\\qquad\\text{ and }\\qquad\n",
    "4 \\text{ with }\n",
    "\\left(\\begin{array}{r}\n",
    "  -2 \\\\\n",
    "   5 \\\\\n",
    "   2\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "We have already seen the case $\\lambda = 4$ and\n",
    "$\\boldsymbol{v}=(-2,5,2)^T$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The eigenvectors are not unique - they can be multiplied by an arbitrary\n",
    "(non-zero) scalar and they remain eigenvectors. For that reason it is\n",
    "usual to *normalize* an eigenvector by dividing through by its length,\n",
    "as given by the (Euclidean, Pythagorean) $2$-norm, $\\Vert\\cdot\\Vert_2$.\n",
    "For example, for $\\boldsymbol{v}=(-2,5,2)^T$ we have\n",
    "\n",
    "$$\n",
    "\\Vert\\boldsymbol{v}\\Vert_2 = \\sqrt{(-2)^2+5^2+2^2} = \\surd{33} \n",
    "$$\n",
    "\n",
    "which means that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v} = \\frac{1}{\\sqrt{33}}\\left(\\begin{array}{r}\n",
    "-2 \\\\ 5 \\\\ 2\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{r}\n",
    "  -0.348155311911396 \\\\\n",
    "   0.870388279778489 \\\\\n",
    "      0.348155311911395\n",
    "\\end{array}\\right)\n",
    "$$ is also an eigenvector for the eigenvalue $\\lambda = 4$.\n",
    "\n",
    "> **THINK ABOUT:** An eigenpair satisfies\n",
    "> $\\boldsymbol{B}\\boldsymbol{v}=\\lambda\\boldsymbol{v}$. Choose a\n",
    "> non-zero real number $\\alpha$ and write\n",
    "> $\\boldsymbol{w}=\\alpha\\boldsymbol{v}$. Is it true that\n",
    "> $\\boldsymbol{B}\\boldsymbol{w}=\\lambda\\boldsymbol{w}$? Can you see why\n",
    "> eigenvectors are not unique in length?\n",
    "\n",
    "> **THINK ABOUT:** If we choose $\\alpha = \\Vert\\boldsymbol{v}\\Vert_2^{-1}$\n",
    "> above what can you say about the value of $\\Vert\\boldsymbol{w}\\Vert_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we normalize each of the eigenvectors above with their own length we\n",
    "get something like this (the decimals may go on for ever - why?):\n",
    "\n",
    "$$\n",
    "9 \\text{ with }\n",
    "\\left(\\begin{array}{r}\n",
    " 0.3527\\ldots \\\\\n",
    "-0.9336\\ldots \\\\   \n",
    " 0.0622\\ldots \\\\\n",
    "\\end{array}\\right),\n",
    "\\qquad\\qquad\n",
    "1 \\text{ with }\n",
    "\\left(\\begin{array}{r}\n",
    "-0.5773\\ldots \\\\\n",
    " 0.5773\\ldots \\\\\n",
    " 0.5773\\ldots\n",
    "\\end{array}\\right)\n",
    "\\qquad\\text{ and }\\qquad\n",
    "4 \\text{ with }\n",
    "\\left(\\begin{array}{r}\n",
    "  -0.3481\\ldots \\\\\n",
    "   0.8703\\ldots \\\\\n",
    "   0.3481\\ldots\n",
    "\\end{array}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use `numpy` to calculate the eigensystem for $\\boldsymbol{B}$.\n",
    "It goes like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w, V = np.linalg.eig(B)\n",
    "print(w)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that two quantities are returned, `w` and `V`. The eigenvalues are\n",
    "collected in `w` and the corresponding eigenvectors are the columns of `V`.\n",
    "\n",
    "Note that these columns of `V` agree with our calculations above for the\n",
    "normalized eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Eigen-System\n",
    "\n",
    "As indicated by this computation,\n",
    "we can stack the length-normalized eigenvectors together next to each\n",
    "other, with the eigenvalues on the leading diagonal of an otherwise\n",
    "zero matrix. We also make sure that the eigenvectors appear in the same\n",
    "order as the corresponding eigenvalues and then use the fact that\n",
    "$\\boldsymbol{B}\\boldsymbol{v}=\\lambda \\boldsymbol{v}$ for each\n",
    "eigen-pair. Then entire eigen-system can then be represented in one\n",
    "equation. For example,\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\left(\\begin{array}{rrr}\n",
    " 3 & -2 & 4 \\\\\n",
    "-6 & 6 & -11 \\\\\n",
    " 6 & 2 & 5 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{rrr}\n",
    " 0.3527\\ldots & -0.5773\\ldots &  -0.3481\\ldots \\\\\n",
    "-0.9336\\ldots &  0.5773\\ldots &   0.8703\\ldots \\\\\n",
    " 0.0622\\ldots &  0.5773\\ldots &   0.3481\\ldots \\\\\n",
    "\\end{array}\\right)\n",
    "\\\\\\ \\\\\n",
    "\\qquad {}=\n",
    "\\left(\\begin{array}{rrr}\n",
    " 0.3527\\ldots & -0.5773\\ldots &  -0.3481\\ldots \\\\\n",
    "-0.9336\\ldots &  0.5773\\ldots &   0.8703\\ldots \\\\\n",
    " 0.0622\\ldots &  0.5773\\ldots &   0.3481\\ldots \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{rrr}\n",
    " 9 & 0 & 0 \\\\\n",
    " 0 & 1 & 0 \\\\\n",
    " 0 & 0 & 4 \\\\\n",
    "\\end{array}\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We can write this as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}\\boldsymbol{V} = \\boldsymbol{V}\\boldsymbol{D}\n",
    "$$\n",
    "\n",
    "where the columns of $\\boldsymbol{V}$ are the eigenvectors of\n",
    "$\\boldsymbol{B}$, and the diagonal matrix $\\boldsymbol{D}$ has the\n",
    "eigenvalues on the leading diagonal. The left-to-right order of the\n",
    "eigenvalues in $\\boldsymbol{D}$ matches the order that the eigenvectors\n",
    "appear in $\\boldsymbol{V}$.\n",
    "\n",
    "All three of these matrices are square and they each have the same\n",
    "dimension as $\\boldsymbol{B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, suppose that $\\det(\\boldsymbol{V})\\ne 0$, then\n",
    "$\\boldsymbol{V}^{-1}$ exists and we can (pre-)multiply both sides of\n",
    "$\\boldsymbol{B}\\boldsymbol{V} = \\boldsymbol{V}\\boldsymbol{D}$ by\n",
    "$\\boldsymbol{V}^{-1}$ and get,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{V}^{-1}\\boldsymbol{B}\\boldsymbol{V} =\n",
    "\\boldsymbol{V}^{-1}\\boldsymbol{V}\\boldsymbol{D} = \\boldsymbol{D}.\n",
    "$$\n",
    "\n",
    "We see that this has produced a diagonal matrix\n",
    "$\\boldsymbol{V}^{-1}\\boldsymbol{B}\\boldsymbol{V}$ that is similar to\n",
    "$\\boldsymbol{B}$ in the sense that it has the same eigenvalues (why are they\n",
    "the same? What can you say about the eigenvalues of diagonal matrices?).\n",
    "Such an operation is called a *similarity transformation*.\n",
    "\n",
    "On the other hand, we can (post-)multiply both sides of\n",
    "$\\boldsymbol{B}\\boldsymbol{V} = \\boldsymbol{V}\\boldsymbol{D}$ by\n",
    "$\\boldsymbol{V}^{-1}$ and get,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B} =\n",
    "\\boldsymbol{B}\\boldsymbol{V}\\boldsymbol{V}^{-1} =\n",
    "\\boldsymbol{V}\\boldsymbol{D}\\boldsymbol{V}^{-1}.\n",
    "$$\n",
    "\n",
    "One reason why this is useful is that we can now easily raise\n",
    "$\\boldsymbol{B}$ to powers. For example,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B}^2 =\n",
    "(\\boldsymbol{V}\\boldsymbol{D}\\boldsymbol{V}^{-1})\n",
    "(\\boldsymbol{V}\\boldsymbol{D}\\boldsymbol{V}^{-1})\n",
    "=\\boldsymbol{V}\\boldsymbol{D}^2\\boldsymbol{V}^{-1}.\n",
    "$$ and so on.\n",
    "\n",
    "However, to do this we needed to assume that\n",
    "$\\det(\\boldsymbol{V})\\ne 0$, and this need not be the case. Matrices for\n",
    "which this is true are called *diagonalizable*, and matrices for which\n",
    "it isn't true are called *defective*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigen-systems of Symmetric Matrices\n",
    "\n",
    "The eigenvalues and eigenvectors of a general square matrix could be\n",
    "**complex numbers** (which we aren't going to be too concerned with),\n",
    "and for large matrices the inverse $\\boldsymbol{V}^{-1}$ could be hard\n",
    "to find explicitly.\n",
    "\n",
    "However, in the special case of a symmetric matrix $\\boldsymbol{A}$, the\n",
    "eigensystem $\\boldsymbol{A}\\boldsymbol{v}=\\lambda\\boldsymbol{A}$ is made\n",
    "up exclusively of *real numbers*.\n",
    "\n",
    "Furthermore, the matrix of normalized eigenvectors, $\\boldsymbol{V}$, is\n",
    "an orthogonal matrix. This means that\n",
    "$\\boldsymbol{V}^{-1}=\\boldsymbol{V}^T$ - which is very easy to\n",
    "calculate once $\\boldsymbol{V}$ is known.\n",
    "\n",
    "This is called the *Spectral Theorem* - see [MML, Theorem 4.15]\n",
    "\n",
    "> **Spectral Theorem (for matrices)**\n",
    "> If $\\boldsymbol{A}$ is real and symmetric (hence square) then its eigenvalues\n",
    "> are all real and its eigenvector matrix $\\boldsymbol{V}$ can be taken\n",
    "> as *orthogonal* so that $\\boldsymbol{V}^{-1}=\\boldsymbol{V}^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see an example of this.\n",
    "\n",
    "$$\n",
    "\\text{if }\n",
    "\\boldsymbol{A} = \\left(\\begin{array}{rrr}\n",
    " 3 & -2 & 4  \\\\\n",
    "-2 &  6 & 2  \\\\\n",
    " 4 &  2 & 5\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "then (with some rounding),\n",
    "\n",
    "$$\n",
    "\\boldsymbol{D} \\approx \\left(\n",
    "\\begin{array}{rrr}\n",
    "-1.217 &  0     & 0 \\\\\n",
    " 0     &  8.217 & 0 \\\\\n",
    " 0     &  0     & 7  \n",
    "\\end{array}\n",
    "\\right) \\quad\\text{ and }\\quad \\boldsymbol{V} \\approx \\left(\n",
    "\\begin{array}{rrr}\n",
    " .726 & .522 & -.447 \\\\\n",
    " .363 & .261 &  .894 \\\\\n",
    "-.584 & .812 &  0 \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll take a look at how to do that in the lab.\n",
    "\n",
    "We will verify that $\\boldsymbol{V}^T\\boldsymbol{V}=\\boldsymbol{I}$\n",
    "up to rounding error.\n",
    "\n",
    "We'll also confirm that $\\boldsymbol{A}\\boldsymbol{V}=\\boldsymbol{V}\\boldsymbol{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Eigen-Decomposition\n",
    "\n",
    "Next, by post-multiplying by $\\boldsymbol{V}^{-1}$\n",
    "we note that we can also write\n",
    "$\\boldsymbol{A}\\boldsymbol{V}=\\boldsymbol{V}\\boldsymbol{D}$ \n",
    "as $\\boldsymbol{A}=\\boldsymbol{V}\\boldsymbol{D}\\boldsymbol{V}^T$\n",
    "which, in expanded form, is,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A} =\n",
    "(\\boldsymbol{v}_1\\ \\boldsymbol{v}_2\\ \\boldsymbol{v}_3\\ \\ldots)\n",
    "\\left(\\begin{array}{rrrr}\n",
    "\\lambda_1     \\\\\n",
    "& \\lambda_2   \\\\\n",
    "& & \\lambda_3 \\\\\n",
    "& & & \\ \\ \\ddots\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{rrrr}\n",
    "\\boldsymbol{v}_1^T\\\\ \\boldsymbol{v}_2^T\\\\ \\boldsymbol{v}_3^T\\\\ \\vdots\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "(we haven't shown all the zero elements). Then simplifying this we get\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A} =\n",
    "(\\boldsymbol{v}_1\\ \\boldsymbol{v}_2\\ \\boldsymbol{v}_3\\ \\ldots)\n",
    "\\left(\\begin{array}{cccc}\n",
    "\\lambda_1\\boldsymbol{v}_1^T \\\\\n",
    "\\lambda_2\\boldsymbol{v}_2^T \\\\\n",
    "\\lambda_3\\boldsymbol{v}_3^T \\\\ \\vdots\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\lambda_1\\boldsymbol{v}_1\\boldsymbol{v}_1^T\n",
    "+\n",
    "\\lambda_2\\boldsymbol{v}_2\\boldsymbol{v}_2^T\n",
    "+\n",
    "\\lambda_3\\boldsymbol{v}_3\\boldsymbol{v}_3^T\n",
    "+ \\cdots\n",
    "= \n",
    "\\sum_{k=1}^n\n",
    "\\lambda_k\\boldsymbol{v}_k\\boldsymbol{v}_k^T.\n",
    "$$\n",
    "\n",
    "Let's see this in python for a specific example. We start by getting the eigen-system for\n",
    "$\\boldsymbol{A}$ as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[3,-2,4],[-2,6,2],[4,2,5]])\n",
    "w, V = np.linalg.eig(A)\n",
    "D=np.diag(w)\n",
    "print('D = \\n', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at each decomposed term in turn. First, for the $k=1$ term,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print( D[0,0]*V[:,0:1]*V[:,0:1].T )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's think about what is going on here. We are printing out the quantity\n",
    "\n",
    "```python\n",
    "D[0,0]*V[:,0:1]*V[:,0:1].T\n",
    "```\n",
    "\n",
    "In this, the `D[0,0]` factor is just the eigenvalue from the top left\n",
    "(first row, first column) of the $\\boldsymbol{D}$ matrix. \n",
    "\n",
    "Then, next, `V[:,0:1]` is a `numpy` **slice**.\n",
    "\n",
    "In this type of expression `[c,a:b]` means take the elements in row `c` \n",
    "that occupy columns `a` through to `b-1`. The expression `[:,a:b]` means\n",
    "take **all of** the rows. Remember that column and row numbering starts at zero\n",
    "in `numpy`.\n",
    "\n",
    "So, `V[:,0:1]` says take the first column of `V` - a column vector, \n",
    "and `V[:,0:1].T` says take the transpose of the first solumn of `V`.\n",
    "\n",
    "It is important to note that we have to write our slicing\n",
    "expressions in the form  `V[:,0:1]` rather than `V[:,0]`,\n",
    "otherwise we lose the shape. See e.g.\n",
    "<https://stackoverflow.com/questions/29635501/row-vs-column-vector-return-during-numpy-array-slicing>\n",
    "\n",
    "Here is a demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('V = \\n', V)\n",
    "print('V[:,0:1] = \\n', V[:,0:1])\n",
    "print('V[:,0] = \\n', V[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Therefore, for $k=1$ we can write $\\lambda_k\\boldsymbol{v}_k\\boldsymbol{v}_k^T$\n",
    "in code as `D[0,0]*V[:,0:1]*V[:,0:1].T`.\n",
    "\n",
    "The full reconstruction of $\\boldsymbol{A}$ is then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n The reconstruction of A ...')\n",
    "print(D[0,0]*V[:,0:1]*V[:,0:1].T + D[1,1]*V[:,1:2]*V[:,1:2].T + D[2,2]*V[:,2:3]*V[:,2:3].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implication: approximation of matrices\n",
    "\n",
    "This is quite a big deal, it means in effect that we can break a square\n",
    "symmetric matrix into pieces and consider as many or as few of those\n",
    "pieces as we wish.\n",
    "\n",
    "This point of view really suggests an approximation scheme.\n",
    "If $\\boldsymbol{A}$ is $n\\times n$ and symmetric then,  \n",
    "\n",
    "$$\n",
    "\\boldsymbol{A} =\n",
    "\\sum_{k=1}^n\n",
    "\\lambda_k\\boldsymbol{v}_k\\boldsymbol{v}_k^T\n",
    "\\qquad\\text{ which suggests that }\\qquad\n",
    "\\boldsymbol{A} \\approx\n",
    "\\sum_{k=1}^m\n",
    "\\lambda_k\\boldsymbol{v}_k\\boldsymbol{v}_k^T\n",
    "$$\n",
    "\n",
    "for $m < n$. We would want to sort the eigenvalues so that the\n",
    "most dominant ones come first in this sum, which is the same as saying\n",
    "\n",
    "$$\n",
    "\\vert\\lambda_1\\vert \\ge\n",
    "\\vert\\lambda_2\\vert \\ge\n",
    "\\vert\\lambda_3\\vert \\ge  \\cdots\n",
    "$$\n",
    "\n",
    "We'll look more closely at this in the workshop session where we will see \n",
    "the effect on the amount of data storage we can save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Building on these examples we can infer that if a large matrix can be well\n",
    "approximated by just a few terms in the eigen-expansion then the amount of\n",
    "storage required in computer memory can be vastly reduced.\n",
    "\n",
    "This is very useful. But it only applies to symmetric square matrices.\n",
    "\n",
    "We can extend it to non-symmetric matrices by introducing complex\n",
    "numbers but we can't extend this to non-square matrices because they\n",
    "don't have eigenvalues.\n",
    "\n",
    "Fortunately, there is another - even more powerful - tool at our\n",
    "disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVD: The Singular Value Decomposition\n",
    "\n",
    "Only square matrices have eigenvalues, and not all square matrices are\n",
    "diagonalizable via the similarity transform. For general matrices\n",
    "(containing real numbers for us) there is a tool called the **SVD** - the\n",
    "*Singular Value Decomposition*.\n",
    "\n",
    "Let $\\boldsymbol{K}$ be an $n$-row by $m$-column matrix of real numbers.\n",
    "\n",
    "Then $\\boldsymbol{K} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$ -\n",
    "this is called the *Singular Value Decomposition* of $\\boldsymbol{K}$.\n",
    "In this:\n",
    "\n",
    "-   $\\boldsymbol{U}$ is an $n\\times n$ *orthogonal square* matrix\n",
    "-   $\\boldsymbol{\\Sigma}$ is an $n\\times m$ *rectangular diagonal*\n",
    "    matrix\n",
    "-   $\\boldsymbol{V}^T$ is an $m\\times m$ *orthogonal square* matrix\n",
    "\n",
    "The entries on the diagonal of $\\boldsymbol{\\Sigma}$ are called the\n",
    "*singular values* of $\\boldsymbol{K}$ and the number of non-zero\n",
    "singular values gives the rank of $\\boldsymbol{K}$.\n",
    "\n",
    "The columns of $\\boldsymbol{U}$ (resp. $\\boldsymbol{V}$) are called the\n",
    "left (resp. right) singular vectors of $\\boldsymbol{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see an example of\n",
    "$\\boldsymbol{K} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$.\n",
    "\n",
    "$$\n",
    "\\text{If }\n",
    "\\boldsymbol{K}=\\left(\\begin{array}{rrr}\n",
    "1  &  2 & 5 \\\\\n",
    "5  & -6 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "\\text{ then }\n",
    "\\boldsymbol{U}=\\left(\\begin{array}{rr}\n",
    "  -0.06213\\ldots  & 0.99806\\ldots \\\\\n",
    "   0.99806\\ldots  & 0.06213\\ldots \\\\\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}=\\left(\\begin{array}{rrr}\n",
    "7.88191\\ldots &                 0 & 0 \\\\\n",
    "0                 & 5.46584\\ldots & 0 \\\\\n",
    "\\end{array}\\right)\n",
    "\\text{ and }\n",
    "\\boldsymbol{V}=\\left(\\begin{array}{rrr}\n",
    " 0.62525\\ldots & 0.23944\\ldots &-0.74278\\ldots \\\\\n",
    "-0.77553\\ldots & 0.29699\\ldots &-0.55708\\ldots \\\\\n",
    " 0.08720\\ldots & 0.92437\\ldots & 0.37139\\ldots \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "If we use these we can indeed check that\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&\n",
    "\\scriptsize\n",
    "\\left(\\begin{array}{rr}\n",
    "  -0.062\\ldots  & 0.998\\ldots \\\\\n",
    "   0.998\\ldots  & 0.062\\ldots \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{rrr}\n",
    "7.881\\ldots &                 0 & 0 \\\\\n",
    "0                 & 5.465\\ldots & 0 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{rrr}\n",
    " 0.625\\ldots & 0.239\\ldots &-0.742\\ldots \\\\\n",
    "-0.775\\ldots & 0.296\\ldots &-0.557\\ldots \\\\\n",
    " 0.087\\ldots & 0.924\\ldots & 0.371\\ldots \\\\\n",
    "\\end{array}\\right)^T\n",
    "\\\\\n",
    "&\\qquad{} =\n",
    "\\left(\\begin{array}{rrr}\n",
    "1  &  2 & 5 \\\\\n",
    "5  & -6 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "as required. We aren't going to do it by hand though, that's what\n",
    "computers are for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "K = np.array([[1,2,5],[5,-6,1]])\n",
    "U, S, VT = np.linalg.svd(K)\n",
    "print(U)\n",
    "print(S)\n",
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note two things:\n",
    "\n",
    "- `np.linalg.svd` returns $\\boldsymbol{V}^T$, not $\\boldsymbol{V}$.  \n",
    "- The shape of `S` doesn't agree with $\\boldsymbol{\\Sigma}$.\n",
    "\n",
    "We'll need to pad `S` - and then we can check the reconstruction $\\boldsymbol{K} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$.\n",
    "\n",
    "The padding is a bit awkward - here it is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "S = np.hstack(( np.diag(S), np.zeros((2,1)) ))\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can check the reconstruction $\\boldsymbol{K} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$.\n",
    "\n",
    "Note that we can also use `@` to perform matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(K - U @ S @ VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is zero (to machine precision) as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is a great deal that can be said about the SVD, but we're going to\n",
    "stay narrowly focussed and explore its value in data science and machine\n",
    "learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the following observation. If we denote the $n$-th\n",
    "column of $\\boldsymbol{U}$ by $\\boldsymbol{u}_n$, and the $n$-th column\n",
    "of $\\boldsymbol{V}$ by $\\boldsymbol{v}_n$, then the statement\n",
    "$\\boldsymbol{K} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$\n",
    "becomes (we saw something very similar to this above with eigenvalues),\n",
    "\n",
    "$$\n",
    "\\boldsymbol{K}\n",
    "=\n",
    "(\\boldsymbol{u}_1\\ \\boldsymbol{u}_2\\ \\cdots\\ \\boldsymbol{u}_n)\n",
    "\\left(\\begin{array}{rrrr}\n",
    "\\sigma_1 & & & \\\\\n",
    " & \\sigma_2 & & \\\\\n",
    " & & \\ddots & \\\\\n",
    " & & & \\sigma_n\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{r}\n",
    "\\boldsymbol{v}_1^T \\\\\n",
    "\\boldsymbol{v}_2^T \\\\\n",
    "\\vdots\\  \\\\\n",
    "\\boldsymbol{v}_n^T \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "(\\boldsymbol{u}_1\\ \\boldsymbol{u}_2\\ \\cdots\\ \\boldsymbol{u}_n)\n",
    "\\left(\\begin{array}{r}\n",
    "\\sigma_1\\boldsymbol{v}_1^T \\\\\n",
    "\\sigma_2\\boldsymbol{v}_2^T \\\\\n",
    "\\vdots\\  \\\\\n",
    "\\sigma_n\\boldsymbol{v}_n^T \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "(we haven't shown all the zero elements of $\\boldsymbol{\\Sigma}$).\n",
    "Simplifying this further then gives,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{K}\n",
    "=\n",
    "\\sigma_1\\boldsymbol{u}_1\\boldsymbol{v}_1^T\n",
    "+ \\sigma_2\\boldsymbol{u}_2\\boldsymbol{v}_2^T\n",
    "+ \\cdots\n",
    "+ \\sigma_n\\boldsymbol{u}_n\\boldsymbol{v}_n^T\n",
    "$$\n",
    "\n",
    "> **THINK ABOUT**: $\\boldsymbol{u}\\boldsymbol{v}^T$ is a rank 1 matrix -\n",
    "> why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The SVD Theorem\n",
    "\n",
    "Here are the full details. For $\\boldsymbol{B}\\in\\mathbb{R}^{m\\times n}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "=\\sum_{j=1}^{p} \\sigma_j \\boldsymbol{u}_j\\boldsymbol{v}_j^T\n",
    "$$\n",
    "\n",
    "where: $\\boldsymbol{U}\\in\\mathbb{R}^{m\\times m}$,\n",
    "$\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{m\\times n}$, $\\boldsymbol{V}\\in\\mathbb{R}^{n\\times n}$\n",
    "and $p=\\min\\{m,n\\}$.\n",
    "\n",
    "Note that $\\boldsymbol{\\Sigma}=\\text{diag}(\\sigma_1,\\ldots,\\sigma_p)$\n",
    "and we can always arrange this so that $\\sigma_1\\ge\\sigma_2\\ge\\cdots\\ge\\sigma_p \\ge 0$.\n",
    "\n",
    "$\\boldsymbol{B}$ is real here (we aren't interested in complex matrices),\n",
    "then $\\boldsymbol{U}$ and $\\boldsymbol{V}$\n",
    "are real and *orthogonal*.\n",
    "\n",
    "If $\\sigma_r\\ne 0$ and $\\sigma_p= 0$ for all $p>r$ then\n",
    "$r$ is the rank of $\\boldsymbol{B}$.\n",
    "\n",
    "Note storage for $\\boldsymbol{B}$ is $mn$. That for the SVD is \n",
    "$r(m+n+1)$. The ratio is\n",
    "\n",
    "$$\n",
    "\\frac{r(m+n+1)}{mn} \n",
    "= \\frac{r}{n}\n",
    "+ \\frac{r}{m} \n",
    "+ \\frac{r}{mn} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Thin SVD\n",
    "\n",
    "We saw above that the zero columns were missing from $\\boldsymbol{\\Sigma}$\n",
    "when we used `numpy` to calculate the SVD. This is called the \n",
    "*Thin SVD*.\n",
    "\n",
    "Here we still have $\\boldsymbol{B}\\in\\mathbb{R}^{m\\times n}$ but with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{B} = \\boldsymbol{U}_1\\boldsymbol{\\Sigma}_1\\boldsymbol{V}^T\n",
    "=\\sum_{j=1}^{n} \\sigma_j \\boldsymbol{u}_j\\boldsymbol{v}_j^T\n",
    "$$\n",
    "\n",
    "where: $\\boldsymbol{U}_1\\in\\mathbb{R}^{m\\times n}$,\n",
    "$\\boldsymbol{\\Sigma}_1\\in\\mathbb{R}^{n\\times n}$.\n",
    "\n",
    "It is called the *thin SVD* because we drop the values that make no\n",
    "contribution (i.e. the zeros).\n",
    "\n",
    "Don't worry too much about this. We'll let `numpy` do the hard work for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HOMEWORK - very important\n",
    "\n",
    "\n",
    "In the lab we are going to see how the SVD can be used to compress data. \n",
    "\n",
    "We'll use **image compression** as an example.\n",
    "\n",
    "Take a good quality jpeg colour photo (e.g. on your phone) of something vivid,\n",
    "detailed and colourful and save it on your account (One Drive, for example)\n",
    "so that your Jupyter notebook in Anaconda can use it.\n",
    "\n",
    "We are going to use the SVD to compress the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review\n",
    "\n",
    "- we understand that matrices can be decomposed into multiplicative factors.\n",
    "- we saw how the spectral theorem allows us to approximate square \n",
    "symmteric matrices using the eigen-system.\n",
    "- we have seen how rectangular matrices can be similarly expanded in terms\n",
    "of the singular value decomposition.\n",
    "- we saw how we can access this functionality using `numpy` in `python`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Technical Notes, Production and Archiving\n",
    "\n",
    "Ignore the material below. What follows is not relevant to the material being taught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Production Workflow\n",
    "\n",
    "- Finalise the notebook material above\n",
    "- Clear and fresh run of entire notebook\n",
    "- Create html slide show:\n",
    "  - `jupyter nbconvert --to slides 7_decomp.ipynb `\n",
    "- Set `OUTPUTTING=1` below\n",
    "- Comment out the display of web-sourced diagrams\n",
    "- Clear and fresh run of entire notebook\n",
    "- Comment back in the display of web-sourced diagrams\n",
    "- Clear all cell output\n",
    "- Set `OUTPUTTING=0` below\n",
    "- Save\n",
    "- git add, commit and push to FML\n",
    "- copy PDF, HTML etc to web site\n",
    "  - git add, commit and push\n",
    "- rebuild binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some of this originated from\n",
    "\n",
    "<https://stackoverflow.com/questions/38540326/save-html-of-a-jupyter-notebook-from-within-the-notebook>\n",
    "\n",
    "These lines create a back up of the notebook. They can be ignored.\n",
    "\n",
    "At some point this is better as a bash script outside of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NBROOTNAME='7_decomp'\n",
    "OUTPUTTING=0\n",
    "\n",
    "if [ $OUTPUTTING -eq 1 ]; then\n",
    "  jupyter nbconvert --to html $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.html ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.html\n",
    "  mv -f $NBROOTNAME.html ./formats/html/\n",
    "\n",
    "  jupyter nbconvert --to pdf $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.pdf ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.pdf\n",
    "  mv -f $NBROOTNAME.pdf ./formats/pdf/\n",
    "\n",
    "  jupyter nbconvert --to script $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.py ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.py\n",
    "  mv -f $NBROOTNAME.py ./formats/py/\n",
    "else\n",
    "  echo 'Not Generating html, pdf and py output versions'\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
