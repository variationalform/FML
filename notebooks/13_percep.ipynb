{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrons\n",
    "\n",
    "#### *variationalform* <https://variationalform.github.io/>\n",
    "\n",
    "#### *Just Enough: progress at pace*\n",
    "\n",
    "<https://variationalform.github.io/>\n",
    "\n",
    "<https://github.com/variationalform>\n",
    "\n",
    "Simon Shaw\n",
    "<https://www.brunel.ac.uk/people/simon-shaw>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<p>\n",
    "This work is licensed under CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)\n",
    "\n",
    "<p>\n",
    "Visit <a href=\"http://creativecommons.org/licenses/by-sa/4.0/\">http://creativecommons.org/licenses/by-sa/4.0/</a> to see the terms.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>This document uses python</td>\n",
    "<td>\n",
    "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>and also makes use of LaTeX </td>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/LaTeX_logo.svg/320px-LaTeX_logo.svg.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>in Markdown</td> \n",
    "<td>\n",
    "<img src=\"https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What this is about:\n",
    "\n",
    "\n",
    "The perceptron\n",
    "\n",
    "- weighted inputs\n",
    "- activation thresholds and outputs\n",
    "- decision boundaries\n",
    "- multi-layer perceptrons\n",
    "- feed-forward artificial neural network\n",
    "\n",
    "As usual our emphasis will be on *doing* rather than *proving*:\n",
    "*just enough: progress at pace*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assigned Reading\n",
    "\n",
    "For this material you are recommended Chapter 3 of [UDL], \n",
    "then Chapter 3 of [NND], and Chapter 6 of [MLFCES], \n",
    "\n",
    "- UDL: Understanding Deep Learning, by Simon J.D. Prince. PDF draft available here:\n",
    "<https://udlbook.github.io/udlbook/>\n",
    "- NND: Neural Network Design by Martin T. Hagan, Howard B. Demuth, Mark Hudson Beale, Orlando De Jesús. <https://hagan.okstate.edu/nnd.html> and <https://hagan.okstate.edu/NNDesign.pdf>\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists, by Andreas Lindholm,\n",
    "  Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön. Cambridge University Press. \n",
    "  <http://smlbook.org>.\n",
    "\n",
    "These can be accessed legally and without cost.\n",
    "\n",
    "There are also these useful references for coding:\n",
    "\n",
    "- PT: `python`: <https://docs.python.org/3/tutorial>\n",
    "- NP: `numpy`: <https://numpy.org/doc/stable/user/quickstart.html>\n",
    "- MPL: `matplotlib`: <https://matplotlib.org>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "In the last two sessions we moved from classification to regression and then\n",
    "we moved back to classification again.\n",
    "\n",
    "We have focussed on cases where the data is **linearly separable**. This important\n",
    "property allowed us to use **logistic regression** and **support vector machines** to\n",
    "construct **linear decision boundaries**.\n",
    "\n",
    "In this session we will continue to work with linear decision boundaries and\n",
    "investigate the **perceptron** and **feed forward neural networks**.\n",
    "\n",
    "The main purpose of this session is to get insight into how perceptrons\n",
    "can **carve up high dimensional space** with hyperplanes, thus creating many \n",
    "compartments within which data classes can be isolated.\n",
    "This will set us up nicely for our study of **deep neural networks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Percepton - and a Shallow Neural Network\n",
    "\n",
    "A perceptron is a computing unit that accepts a vector of numerical inputs,\n",
    "$\\boldsymbol{x}$, and forms a linear combination of them using a vector of\n",
    "**weights**, $\\boldsymbol{W}$. A numerical bias, $b$, may then be added to\n",
    "form a real number. \n",
    "\n",
    "This real number is then **thresholded** with an activation function. This\n",
    "activation function seeks to determine how significant its input signal is\n",
    "and either kills it or passes it some form of it through.\n",
    "\n",
    "The whole system is an attempt to (very crudely) mimic the neuronal connections\n",
    "in the brain.\n",
    "\n",
    "Chapters 2 and 3 of [UDL, Chapter 3] <https://udlbook.github.io/udlbook/> are useful\n",
    "background reading, but what we do here is self-contained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here it is - schematically. The input vector\n",
    "$\\boldsymbol{x}=(x_1,x_2,x_3)^T$, and weights,\n",
    "$\\boldsymbol{W}=(w_1,w_2,w_3)^T$, are **linearly combined**.\n",
    "An optional bias, $b$, is added and the result, $n$ \n",
    "is fed into an activation function,\n",
    "$\\sigma\\colon\\mathbb{R}\\to\\mathbb{R}$ to produce the output $y$.\n",
    "\n",
    "$$\n",
    "\\begin{array}{lr}\n",
    "\\begin{array}{ll}\n",
    "n & = w_1 x_1 + w_2 x_2 + w_3 x_3 + b, \\\\\n",
    "  & = \\boldsymbol{W}^T\\boldsymbol{x} + b, \\\\\n",
    "y & = \\sigma(n).\n",
    "\\end{array}\n",
    "& \\qquad\\qquad\n",
    "\\begin{array}{rlcll}\n",
    "x_1 & \\boldsymbol{w}&          &             &\\\\ \n",
    "    & \\searrow      &          & \\sigma(n)   &\\\\ \n",
    "x_2 & \\rightarrow   & n\\!\\!\\!\\!\\!\\bigcirc & \\longrightarrow & y \\\\ \n",
    "    & \\nearrow      & \\uparrow &             & \\\\ \n",
    "x_3 &               & b        &             & \\\\ \n",
    "    &               &          &             & \\\\ \n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Compactly: $y = \\sigma(\\boldsymbol{W}^T\\boldsymbol{x} + b)$.\n",
    "Let's see why this is so useful - it really comes down the the \n",
    "activation function. Without that this isn't such a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Heaviside Unit Step function\n",
    "\n",
    "This is defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(x) = \\left\\{\\begin{array}{ll}\n",
    "1    &\\text{if } x > 0; \\\\\n",
    "x_2  &\\text{if } x = 0; \\\\\n",
    "0    &\\text{if } x < 0.\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "It represents a switch being thrown as the input increases through zero,\n",
    "with the output signal moving from zero to one. The value $x_2$ at the \n",
    "discontinuity is usually arbitrary. We have included it because `numpy`\n",
    "allows it with this syntax:\n",
    "\n",
    "\n",
    "```\n",
    "np.heaviside(x,x2)\n",
    "```\n",
    "\n",
    "We will usually take $x_2=0$. For details see here:\n",
    "<https://numpy.org/doc/stable/reference/generated/numpy.heaviside.html>\n",
    "\n",
    "The Heaviside function can be used as an **activation function** - let's see it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.matlib  # a new one, needed below for repmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_vals = np.arange(-20, 20.1, 0.1)\n",
    "y_vals_1 = np.heaviside(x_vals, 0)\n",
    "y_vals_2 = np.heaviside(x_vals - 5, 0)\n",
    "y_vals_3 = 2*np.heaviside(x_vals + 8, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4)); plt.gca().set_aspect(10)\n",
    "plt.plot(x_vals, y_vals_1, color='blue', label='H(x)')\n",
    "plt.plot(x_vals, y_vals_2, color='red', label='H(x-5)')\n",
    "plt.plot(x_vals, y_vals_3, color='green', label='2H(x+8)')\n",
    "plt.xlabel('x'); plt.ylabel('Heaviside');\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** the vertical lines are an artefact of the plotting - they aren't part of the\n",
    "function's graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A simple feed forward neural net\n",
    "\n",
    "Let's use this concrete example: two inputs\n",
    "$\\boldsymbol{x}=(x_1,x_2)^T$, with weights,\n",
    "$\\boldsymbol{W}=(w_1,w_2)^T=(3,2)^T$ and optional bias,\n",
    "$b=-1$. The activation function is the Heaviside function.\n",
    "\n",
    "This gives the output,\n",
    "\n",
    "$$\n",
    "y = \\mathcal{H}(n) \\quad\\text{ for }\\quad\n",
    "n = \\boldsymbol{W}^T\\boldsymbol{x}+b\n",
    "= 3 x_1 + 2 x_2 - 1.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "y = \\left\\{\\begin{array}{ll}\n",
    "1    &\\text{if } n > 0; \\\\\n",
    "0    &\\text{if } n \\le 0,\n",
    "\\end{array}\\right.\n",
    "\\qquad\\Longrightarrow\\qquad\n",
    "y = \\left\\{\\begin{array}{ll}\n",
    "1    &\\text{if } 3 x_1 + 2 x_2 - 1 > 0; \\\\\n",
    "0    &\\text{if } 3 x_1 + 2 x_2 - 1 \\le 0.\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "This can be a **binary classifier**. Can you see why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have just seen that with the given weights and bias,\n",
    "\n",
    "$$\n",
    "y = \\mathcal{H}(n)\\text{ for }\n",
    "n = \\boldsymbol{W}^T\\boldsymbol{x}+b\n",
    "= 3 x_1 + 2 x_2 - 1\n",
    "\\ \\Rightarrow\\ y = \\left\\{\\begin{array}{ll}\n",
    "1    &\\text{if } 3 x_1 + 2 x_2 - 1 > 0; \\\\\n",
    "0    &\\text{if } 3 x_1 + 2 x_2 - 1 \\le 0.\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "In this $3 x_1 + 2 x_2 - 1 = 0$ represents a straight line in the $(x_1,x_2)$\n",
    "plane. In the usual form of $y=mx+c$, this line has equation $x_2 = (1-3x_1)/2$.\n",
    "That is: gradient $-3/2$, and intercept $1/2$.\n",
    "\n",
    "A point $(x_1,x_2)=(a,b)$ above this line has $3 x_1 + 2 x_2 - 1 > 0$ while a point\n",
    "on or below the line has $3 x_1 + 2 x_2 - 1 \\le 0$.\n",
    "\n",
    "Therefore - **this line could be a decision boundary**. Any other line could also,\n",
    "we would just need different choices for the weights and bias.\n",
    "\n",
    "Let's set this up and do some calculations with $(x_1,x_2)=(3,1)$ and\n",
    "$(x_1,x_2)=(-2,-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$y = \\mathcal{H}(n)$ for $\\displaystyle n=(3,2){x_1 \\choose x_2}-1$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set up our column vector of weights, and the bias\n",
    "W = np.array([[3,2]]).T\n",
    "b = -1\n",
    "# find y for input x = (3,1)\n",
    "X = np.array([[3,1]]).T\n",
    "y = np.heaviside(W.T@X-b,0)\n",
    "print('For input x = ( 3, 1), y = ', y)\n",
    "# find y for input x = (-2,-1)\n",
    "X = np.array([[-2,-1]]).T\n",
    "y = np.heaviside(W.T@X-b,0)\n",
    "print('For input x = (-2,-1), y = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also illustrate this graphically. We plot the line and \n",
    "then the two input points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_vals = np.arange(-5, 5.1)\n",
    "y_vals = (1-3*x_vals)/2\n",
    "plt.plot(x_vals, y_vals, color='black', label='decision boundary')\n",
    "plt.plot( 3, 1, 'x', color='blue',label='(x1,x2)=( 3, 1)')\n",
    "plt.plot(-2,-1, 'x', color='red', label='(x1,x2)=(-2,-1)')\n",
    "plt.xlabel('x_1'); plt.ylabel('x_2');\n",
    "plt.annotate('above', [ 0, 3], color='blue')\n",
    "plt.annotate('below', [-2,-4], color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the predictions on a grid of equally spaced points in the \n",
    "$x_1$ and $x_2$ directions.\n",
    "\n",
    "If we colour the predictions according to $y=0$ or $y=1$ then this\n",
    "decision boundary will naturally emerge.\n",
    "\n",
    "We will create a grid that ranges in both directions over\n",
    "\n",
    "$$\n",
    "-7 \\le x_1, \\ x_2 \\le 9 \\qquad\\text{ in steps of } s=0.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "s = .5\n",
    "x1 = np.arange(-7, 9+s, s)\n",
    "x2 = np.arange(-7, 9+s, s)\n",
    "N = x1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# allocate input and output variables\n",
    "X = np.zeros([2,1])\n",
    "y = np.zeros([N,N])\n",
    "# and our grid points in each direction (note the transpose for x2)\n",
    "X1grid = np.matlib.repmat(x1,N,1)      # repeats x1 N times\n",
    "X2grid = np.matlib.repmat(x2,N,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# loop over the grid, vertically for each horizontal point\n",
    "for i in range(N):\n",
    "  for j in range(N):\n",
    "    X[0,0] = X1grid[i,j] # x1[i]\n",
    "    X[1,0] = X2grid[i,j] # x2[j]\n",
    "    n = W.T @ X + b\n",
    "    # make a prediction and assign to y_{ij} for x1[i], x2[j]\n",
    "    tmp = np.heaviside(W.T @ X + b, 0)\n",
    "    y[i,j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# determine the locations where the output is zero\n",
    "indx = (y < 0.5)\n",
    "# flatten this matrix into a vector, along with the grid points\n",
    "indx= indx.flatten()\n",
    "X1grid = X1grid.flatten()\n",
    "X2grid = X2grid.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the zero outputs in red\n",
    "plt.scatter(X1grid[indx], X2grid[indx], 2, color='red')\n",
    "# and plot the unit outputs in blue\n",
    "plt.scatter(X1grid[~indx], X2grid[~indx], 2, color='blue')\n",
    "# plot the decision boubdary from the weights and bias in black\n",
    "plt.plot(x_vals, y_vals, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Take a breath - this simple idea, using just this,\n",
    "\n",
    "$$\n",
    "y = \\mathcal{H}(n)\\text{ for }\n",
    "n = \\boldsymbol{W}^T\\boldsymbol{x}+b\n",
    "= 3 x_1 + 2 x_2 - 1\n",
    "\\ \\Rightarrow\\ y = \\left\\{\\begin{array}{ll}\n",
    "1    &\\text{if } 3 x_1 + 2 x_2 - 1 > 0; \\\\\n",
    "0    &\\text{if } 3 x_1 + 2 x_2 - 1 \\le 0.\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "is the basis of a hugely powerful sub-domain technology in \n",
    "data science and artificial intelligence. \n",
    "\n",
    "We used this idea to classify the input into two classes.\n",
    "\n",
    "Let's now push this idea forward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Four Classes\n",
    "\n",
    "Consider this,\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{r}\n",
    "y_1 \\\\ y_2\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\mathcal{H}\n",
    "\\left(\n",
    "\\left(\\begin{array}{rr}\n",
    "1 & 2 \\\\ -6 & 4\n",
    "\\end{array}\\right)^T\n",
    "\\left(\\begin{array}{r}\n",
    "x_1 \\\\ x_2 \n",
    "\\end{array}\\right)\n",
    "+\n",
    "\\left(\\begin{array}{r}\n",
    "1 \\\\ 2\n",
    "\\end{array}\\right)\n",
    "\\right)\n",
    "=\n",
    "\\left(\\begin{array}{l}\n",
    "\\mathcal{H}(x_1 - 6x_2 + 1)\n",
    "\\\\\n",
    "\\mathcal{H}(2 x_1 + 4 x_2 + 2)\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "Note that the activation function is applied element-by-element.\n",
    "\n",
    "We can write this in a more generic way as follows:\n",
    "$\\boldsymbol{y} =\n",
    "\\sigma(\\boldsymbol{n})$ for\n",
    "$\\boldsymbol{n}=\\boldsymbol{W}^T\\boldsymbol{x}+\\boldsymbol{b}$.\n",
    "\n",
    "Here we have the matrix of weights\n",
    "$\\boldsymbol{W}={\\ \\ \\ 1\\ \\ 2 \\choose-6\\ \\ 4}$\n",
    "and bias vector $\\boldsymbol{b}={1 \\choose 2}$.\n",
    "\n",
    "The possible outputs are:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} =\n",
    "\\left(\\begin{array}{r}\n",
    "0 \\\\ 0\n",
    "\\end{array}\\right), \\quad\n",
    "\\left(\\begin{array}{r}\n",
    "1 \\\\ 0\n",
    "\\end{array}\\right), \\quad\n",
    "\\left(\\begin{array}{r}\n",
    "0 \\\\ 1\n",
    "\\end{array}\\right) \\quad\\text{ and }\\quad\n",
    "\\left(\\begin{array}{r}\n",
    "1 \\\\ 1\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "With this we can consider **four** classes:\n",
    "$C_1$, $C_2$, $C_3$ and $C_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With \n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{r}\n",
    "y_1 \\\\ y_2\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{l}\n",
    "\\mathcal{H}(x_1 - 6x_2 + 1)\n",
    "\\\\\n",
    "\\mathcal{H}(2 x_1 + 4 x_2 + 2)\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "the decision boundaries are given by $x_1 - 6x_2 + 1=0$\n",
    "and $2 x_1 + 4 x_2 + 2=0$.\n",
    "\n",
    "This neural network will determine whether or not a given\n",
    "point is above or below the line $x_1 - 6x_2 + 1=0$ in\n",
    "the first component of $\\boldsymbol{y}$.\n",
    "\n",
    "And it will determine whether or not that\n",
    "point is above or below the line $2 x_1 + 4 x_2 + 2=0$ in\n",
    "the second component of $\\boldsymbol{y}$.\n",
    "\n",
    "It will then decide to which of the four classes the input\n",
    "belongs.\n",
    "\n",
    "Let's see a possible classification scheme..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x_vals = np.arange(-5, 5.1)\n",
    "plt.plot(x_vals,-(1+x_vals)/2, color='black')\n",
    "plt.plot(x_vals, (1+x_vals)/6, color='black')\n",
    "plt.annotate('C1', [-4, 0], color='blue')\n",
    "plt.annotate('C2', [-2,-2], color='magenta')\n",
    "plt.annotate('C3', [ 2,0], color='red')\n",
    "plt.annotate('C4', [-1, 1], color='green');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's make predictions on a background grid just as before. By coloring \n",
    "each prediction according to the four-way scheme above we will see the \n",
    "four classes and the separating decision boundaries naurally emerge.\n",
    "\n",
    "We will create a grid for $-20 \\le x_1, x_2 \\le 20$ in unit steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# here is the grid\n",
    "s = 1\n",
    "x1 = np.arange(-20, 20+s, s)\n",
    "x2 = np.arange(-20, 20+s, s)\n",
    "N = x1.shape[0]\n",
    "X1grid = np.matlib.repmat(x1,N,1)\n",
    "X2grid = np.matlib.repmat(x2,N,1).T\n",
    "\n",
    "# here is the neural network definition\n",
    "W = np.array([[1,2],[-6,4]])\n",
    "b = np.array([[1,2]]).T\n",
    "# two variable, output and input\n",
    "y = np.zeros([2,N,N])\n",
    "X = np.zeros([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# traverse the grid, store a prediction for each point\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        X[0] = X1grid[i,j]\n",
    "        X[1] = X2grid[i,j]\n",
    "        tmp = np.heaviside(W.T @ X + b, 0)\n",
    "        y[:,i,j] = tmp[:,0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X1grid = X1grid.flatten()\n",
    "X2grid = X2grid.flatten()\n",
    "\n",
    "indx0 = (y[0,:] < 0.5)\n",
    "indx0= indx0.flatten()\n",
    "indx1 = (y[1,:] < 0.5)\n",
    "indx1= indx1.flatten()\n",
    "indx00 = np.logical_and( indx0, indx1)\n",
    "indx11 = np.logical_and(~indx0,~indx1)\n",
    "indx01 = np.logical_and( indx0,~indx1)\n",
    "indx10 = np.logical_and(~indx0, indx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X1grid[indx00], X2grid[indx00], 2, color='blue')\n",
    "plt.scatter(X1grid[indx11], X2grid[indx11], 2, color='red')\n",
    "plt.scatter(X1grid[indx01], X2grid[indx01], 2, color='green')\n",
    "plt.scatter(X1grid[indx10], X2grid[indx10], 2, color='magenta')\n",
    "plt.plot(x1,-(1+x1)/2, color='black')\n",
    "plt.plot(x1, (1+x1)/6, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Artificial Neural Network\n",
    "\n",
    "Perceptrons can be vertically stacked and **fully connected**, and \n",
    "they can also be multi-layered horizontally. \n",
    "\n",
    "An input signal on the left **input layer** is propagated through\n",
    "the network by repeating the **weighting**, **bias** and **activation**\n",
    "steps.\n",
    "\n",
    "This eventually results in an **output** on the right-most layer.\n",
    "\n",
    "This is called **feeding forward**. \n",
    "\n",
    "Layers of stacked perceptrons between the input and output layers \n",
    "are called **hidden layers**.\n",
    "\n",
    "Networks without hidden layers are often termed **shallow**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The diagrams that are present in the Jupyter notebook and slides version of this document\n",
    "are not included in the PDF version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a simple 2-input/2-output shallow network (i.e. with no hidden layers).\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"./gfx/ann_1.png\" style=\"height:300px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "This is a useful notation system...\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_0 & = \\boldsymbol{x},\n",
    "\\\\\n",
    "\\boldsymbol{n}_1 & = \\boldsymbol{W}_1^T\\boldsymbol{a}_0+\\boldsymbol{b}_1,\n",
    "\\\\\n",
    "\\boldsymbol{a}_1 & = \\sigma_1(\\boldsymbol{n}_1),\n",
    "\\\\\n",
    "\\boldsymbol{y} & = \\boldsymbol{a}_1.\n",
    "\\end{align*}\n",
    "    \n",
    "Note that the weight matrices are square.\n",
    "    \n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a 3-input/3-output network with one hidden layer.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"./gfx/ann_2.png\" style=\"height:300px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "The notation system is now easily adjusted...\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_0 & = \\boldsymbol{x},\n",
    "\\\\\n",
    "\\boldsymbol{n}_1 & = \\boldsymbol{W}_1^T\\boldsymbol{a}_0+\\boldsymbol{b}_1,\n",
    "\\\\\n",
    "\\boldsymbol{a}_1 & = \\sigma_1(\\boldsymbol{n}_1),\n",
    "\\\\\n",
    "\\boldsymbol{n}_2 & = \\boldsymbol{W}_2^T\\boldsymbol{a}_1+\\boldsymbol{b}_2,\n",
    "\\\\\n",
    "\\boldsymbol{a}_2 & = \\sigma_2(\\boldsymbol{n}_2),\n",
    "\\\\\n",
    "\\boldsymbol{y} & = \\boldsymbol{a}_2.\n",
    "\\end{align*}\n",
    "\n",
    "The weight matrices are still square.    \n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is more general network with one hidden layer.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"./gfx/ann_3.png\" style=\"height:300px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_0 & = \\boldsymbol{x},\n",
    "\\\\\n",
    "\\boldsymbol{n}_1 & = \\boldsymbol{W}_1^T\\boldsymbol{a}_0+\\boldsymbol{b}_1,\n",
    "\\\\\n",
    "\\boldsymbol{a}_1 & = \\sigma_1(\\boldsymbol{n}_1),\n",
    "\\\\\n",
    "\\boldsymbol{n}_2 & = \\boldsymbol{W}_2^T\\boldsymbol{a}_1+\\boldsymbol{b}_2,\n",
    "\\\\\n",
    "\\boldsymbol{a}_2 & = \\sigma_2(\\boldsymbol{n}_2),\n",
    "\\\\\n",
    "\\boldsymbol{y} & = \\boldsymbol{a}_2.\n",
    "\\end{align*}\n",
    "\n",
    "In this general case the weight matrices are no longer square.    \n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a more general network with two hidden layers.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"./gfx/ann_4.png\" style=\"height:500px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_0 & = \\boldsymbol{x},\n",
    "\\\\\n",
    "\\boldsymbol{n}_1 & = \\boldsymbol{W}_1^T\\boldsymbol{a}_0+\\boldsymbol{b}_1,\n",
    "\\\\\n",
    "\\boldsymbol{a}_1 & = \\sigma_1(\\boldsymbol{n}_1),\n",
    "\\\\\n",
    "\\boldsymbol{n}_2 & = \\boldsymbol{W}_2^T\\boldsymbol{a}_1+\\boldsymbol{b}_2,\n",
    "\\\\\n",
    "\\boldsymbol{a}_2 & = \\sigma_2(\\boldsymbol{n}_2),\n",
    "\\\\\n",
    "\\boldsymbol{n}_3 & = \\boldsymbol{W}_3^T\\boldsymbol{a}_2+\\boldsymbol{b}_3,\n",
    "\\\\\n",
    "\\boldsymbol{a}_3 & = \\sigma_3(\\boldsymbol{n}_3),\n",
    "\\\\\n",
    "\\boldsymbol{y} & = \\boldsymbol{a}_3.\n",
    "\\end{align*}\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "This assemblage of perceptons is called an **artificial neural network**.\n",
    "It is a very crude imitation of the neuronal connections in the brain.\n",
    "\n",
    "Note that the weight matrix and bias vector have dimensions determined \n",
    "by the size of the layers they connect.\n",
    "\n",
    "The **architecture** possibilities for a neural network are **endless**,\n",
    "and there are also variants on the basic scheme introduced above.\n",
    "These are,\n",
    "\n",
    "- Convolutional Neural Networks\n",
    "\n",
    "- Recurrent Neural Networks\n",
    "\n",
    "- ... And lots more! It's a vibrant field!\n",
    "\n",
    "The feed forward algortithm, for $L$ layers (not including the input layer) is\n",
    "\n",
    "\\begin{align*}\n",
    "& \\boldsymbol{a}_0 = \\boldsymbol{x},\n",
    "\\\\\n",
    "& \\text{for } k = 1,2,\\ldots,L,\n",
    "\\\\\n",
    "&\\qquad \\boldsymbol{n}_k = \\boldsymbol{W}_k^T\\boldsymbol{a}_{k-1}+\\boldsymbol{b}_k,\n",
    "\\\\\n",
    "&\\qquad \\boldsymbol{a}_k = \\sigma_k(\\boldsymbol{n}_k),\n",
    "\\\\\n",
    "&\\boldsymbol{y} = \\boldsymbol{a}_L.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "The heaviside function is **not differentiable** - at least not in the usual\n",
    "and useful sense. \n",
    "\n",
    "Other frequently used activation functions are the **sigmoid** (or **logistic**)\n",
    "function , and the **ReLU** - the **Rectified Linear Unit**.\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}(x) = \\frac{1}{1+\\exp(-x)}\n",
    "\\qquad\\text{and}\\qquad\n",
    "\\mathrm{ReLU}(x) = \\max\\{0,x\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# here are some useful python function definitions of these\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xvals = np.arange(-5,5+0.1,0.1)\n",
    "plt.plot(xvals, sigmoid(xvals), color='blue', label='sigmoid')\n",
    "plt.plot(xvals, ReLU(xvals), color='red', label='ReLU')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The sigmoid we have seen before (logistic regression). The ReLU is new to\n",
    "us. Both of these are in frequent and common use in neural network development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's an example of how to use them with our previous data\n",
    "$\\boldsymbol{W}={\\ \\ \\ 1\\ \\ 2 \\choose-6\\ \\ 4}$\n",
    "and $\\boldsymbol{b}={1 \\choose 2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,-3]]).T\n",
    "W = np.array([[1,2],[-6,4]])\n",
    "b = np.array([[1,2]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = W.T @ X + b\n",
    "ys = sigmoid(n)\n",
    "yR = ReLU(n)\n",
    "print(f'y = sigmoid(n) = \\n{ys}\\ny = ReLU(n) = \\n{yR}, ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "We covered *just enough*, to make *progress at pace*. We looked at\n",
    "\n",
    "- Perceptrons...\n",
    "  - weights, biases and various activation functions\n",
    "- Vertically stacked fully connected perceptrons\n",
    "- multi-layer perceptrons with hidded layers\n",
    "- classification by partitioning the plane\n",
    "\n",
    "This led us to **artificial neural networks** and will next take\n",
    "us on to **deep learning**.\n",
    "\n",
    "There are two things to bring along with us:\n",
    "\n",
    "1. In the above we knew the weights and biases - they gave us the classes.\n",
    "\n",
    "2. We have only worked in two dimensions.\n",
    "\n",
    "**In practice our data lies in higher dimensional space**. In such cases\n",
    "**the data, bias and output vectors, and the weight matrices, are also\n",
    "high dimensional**. Further, when we get our data set, **we will not know\n",
    "the weights and biases in advance** and we have to **devise ways to\n",
    "_machine learn_ them**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Technical Notes, Production and Archiving\n",
    "\n",
    "Ignore the material below. What follows is not relevant to the material being taught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Production Workflow\n",
    "\n",
    "- Finalise the notebook material above\n",
    "- Clear and fresh run of entire notebook\n",
    "- Create html slide show:\n",
    "  - `jupyter nbconvert --to slides 13_percep.ipynb `\n",
    "- Set `OUTPUTTING=1` below\n",
    "- Comment out the display of web-sourced diagrams\n",
    "- Clear and fresh run of entire notebook\n",
    "- Comment back in the display of web-sourced diagrams\n",
    "- Clear all cell output\n",
    "- Set `OUTPUTTING=0` below\n",
    "- Save\n",
    "- git add, commit and push to FML\n",
    "- copy PDF, HTML etc to web site\n",
    "  - git add, commit and push\n",
    "- rebuild binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some of this originated from\n",
    "\n",
    "<https://stackoverflow.com/questions/38540326/save-html-of-a-jupyter-notebook-from-within-the-notebook>\n",
    "\n",
    "These lines create a back up of the notebook. They can be ignored.\n",
    "\n",
    "At some point this is better as a bash script outside of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NBROOTNAME=13_percep\n",
    "OUTPUTTING=0\n",
    "\n",
    "if [ $OUTPUTTING -eq 1 ]; then\n",
    "  jupyter nbconvert --to html $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.html ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.html\n",
    "  mv -f $NBROOTNAME.html ./formats/html/\n",
    "\n",
    "  jupyter nbconvert --to pdf $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.pdf ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.pdf\n",
    "  mv -f $NBROOTNAME.pdf ./formats/pdf/\n",
    "\n",
    "  jupyter nbconvert --to script $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.py ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.py\n",
    "  mv -f $NBROOTNAME.py ./formats/py/\n",
    "else\n",
    "  echo 'Not Generating html, pdf and py output versions'\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
