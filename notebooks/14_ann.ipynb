{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Networks and Deep Learning\n",
    "\n",
    "#### *variationalform* <https://variationalform.github.io/>\n",
    "\n",
    "#### *Just Enough: progress at pace*\n",
    "\n",
    "<https://variationalform.github.io/>\n",
    "\n",
    "<https://github.com/variationalform>\n",
    "\n",
    "Simon Shaw\n",
    "<https://www.brunel.ac.uk/people/simon-shaw>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<p>\n",
    "This work is licensed under CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)\n",
    "\n",
    "<p>\n",
    "Visit <a href=\"http://creativecommons.org/licenses/by-sa/4.0/\">http://creativecommons.org/licenses/by-sa/4.0/</a> to see the terms.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>This document uses python</td>\n",
    "<td>\n",
    "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>and also makes use of LaTeX </td>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/LaTeX_logo.svg/320px-LaTeX_logo.svg.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>in Markdown</td> \n",
    "<td>\n",
    "<img src=\"https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What this is about:\n",
    "\n",
    "- Artificial Neural Networks\n",
    "\n",
    "- Deep Learning\n",
    "\n",
    "- MNIST digit recognition\n",
    "\n",
    "As usual our emphasis will be on *doing* rather than *proving*:\n",
    "*just enough: progress at pace*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assigned Reading\n",
    "\n",
    "For this material you are recommended Chapter 3 of [UDL], \n",
    "then Chapter 3 of [NND], and Chapter 6 of [MLFCES].\n",
    "\n",
    "- UDL: Understanding Deep Learning, by Simon J.D. Prince. PDF draft available here:\n",
    "<https://udlbook.github.io/udlbook/>\n",
    "- NND: Neural Network Design by Martin T. Hagan, Howard B. Demuth, Mark Hudson Beale, Orlando De Jesús. <https://hagan.okstate.edu/nnd.html> and <https://hagan.okstate.edu/NNDesign.pdf>\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists, by Andreas Lindholm,\n",
    "  Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön. Cambridge University Press. \n",
    "  <http://smlbook.org>.\n",
    "- DL: Deep Learning, by Ian Goodfellow, Yoshua Bengio and Aaron Courville <https://www.deeplearningbook.org>\n",
    "\n",
    "These can be accessed legally and without cost.\n",
    "\n",
    "There are also these useful references for coding:\n",
    "\n",
    "- PT: `python`: <https://docs.python.org/3/tutorial>\n",
    "- NP: `numpy`: <https://numpy.org/doc/stable/user/quickstart.html>\n",
    "- MPL: `matplotlib`: <https://matplotlib.org>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The [DL] book has rapidly become something of a classic - it's rich in content but\n",
    "might not be an easy introductory read: that will depend on the individual though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "In the last section of this course we are going to take a look at the \n",
    "mathematical formulation of artificial neural networks.\n",
    "\n",
    "We will be building on the feed forward algorithm we met in the section\n",
    "on perceptrons.\n",
    "\n",
    "We know (or at least imagine) that, given the weights and biases, \n",
    "the network carves up the output space into compartments that can\n",
    "be used for classification.\n",
    "\n",
    "But we need to start with training data, and use this to determine the\n",
    "weights and biases. We will cover the essentials of:\n",
    "\n",
    "- cost, error, loss\n",
    "- gradient descent\n",
    "- hyperparameters\n",
    "- back propagation\n",
    "- activation functions\n",
    "\n",
    "We start by looking at the data we are going to use: \n",
    "**The MNIST data set of handwritten digits**.\n",
    "\n",
    "This will all be done manually - we wont use `sklearn` for this section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# our usual imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MNIST Data Set of Handwritten Digits\n",
    "\n",
    "The original source of these digitized images is here:\n",
    "<http://yann.lecun.com/exdb/mnist/>\n",
    "\n",
    "This format isn't particularly easy to work with, so this site,\n",
    "<https://pjreddie.com/projects/mnist-in-csv/>, makes two \n",
    "CSV files available:\n",
    "\n",
    "- `MNIST_train.csv` - $60,000$ handwritten digit images, for training\n",
    "- `MNIST_test.csv`  - $10,000$ handwritten digit images, for testing\n",
    "\n",
    "Further, for testing and development it is useful to have\n",
    "small data sets, and so Rashid for his book \n",
    "**Make Your Own Neural Network**, at\n",
    "<https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork>,\n",
    "made these two smaller sets,\n",
    "\n",
    "- `MNIST_train_100.csv` - $100$, for training\n",
    "- `MNIST_test_10.csv` - $10$, for testing\n",
    "\n",
    "This book was also used for these notes. Note that the test set here is not *exhaustive*\n",
    "in that not all labels are included. This means that you'll get errors below for the confusion mamtrix if you use this one.\n",
    "\n",
    "There are also these (home made), for intermediate use:\n",
    "\n",
    "- `MNIST_train_1000.csv` - $1000$, for training\n",
    "- `MNIST_test_100.csv` - $100$, for testing    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get the data - you may need to need to grab it and unzip it from\n",
    "brightspace (or use binder).\n",
    "\n",
    "We'll make it easy to choose which data set with a `choice` variable.\n",
    "\n",
    "**NOTE:** `MNIST_train.csv` is not available in this git repo, or the binder environment\n",
    "because it is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "choice = 2\n",
    "if choice == 0:\n",
    "  df_train = pandas.read_csv(r'./data/MNIST/MNIST_train.csv', header=None)\n",
    "  df_test  = pandas.read_csv(r'./data/MNIST/MNIST_test.csv', header=None)\n",
    "elif choice == 1:\n",
    "  df_train = pandas.read_csv(r'./data/MNIST/MNIST_train_1000.csv', header=None)\n",
    "  df_test  = pandas.read_csv(r'./data/MNIST/MNIST_test_100.csv', header=None)\n",
    "elif choice == 2:\n",
    "  df_train = pandas.read_csv(r'./data/MNIST/MNIST_train_100.csv', header=None)\n",
    "  df_test  = pandas.read_csv(r'./data/MNIST/MNIST_test_100.csv', header=None)\n",
    "else:\n",
    "  df_train = pandas.read_csv(r'./data/MNIST/MNIST_train_100.csv', header=None)\n",
    "  df_test  = pandas.read_csv(r'./data/MNIST/MNIST_test_10.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# it will take a bit of work to see what these data files hold\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This doesn't look too promising ... but we'll get there. \n",
    "\n",
    "The first column contains the labels. The remaining columns contain \n",
    "$28^2$ pixel values for the digitized image of the label.\n",
    "\n",
    "Let's push on ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We assign the pixel values to X_train and X_test\n",
    "X_train = df_train.iloc[:, 1:].values\n",
    "X_test  = df_test.iloc[:, 1:].values\n",
    "N_train = X_train.shape[0]\n",
    "N_test  = X_test.shape[0]\n",
    "print(f'N_train = {N_train}, N_test = {N_test}')\n",
    "\n",
    "# And we assign the first column labels 0,1,2,...,9 to ... \n",
    "train_labels = df_train.iloc[:, 0].values\n",
    "test_labels  = df_test.iloc[:, 0].values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are the first 10 labels in the training set... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(train_labels[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And now the first ten labels in the test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(test_labels[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We choose the third row (indexed as 2) in the training set to demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's choose the third row (indexed as 2) \n",
    "row = 2\n",
    "plt.imshow( X_train[row,:].reshape(28,28) , cmap='Greys', interpolation='None')\n",
    "plt.title(f'Digitized Image With Label {train_labels[row]}')\n",
    "print(f'There are 28x28 = {28*28} pixel values: 0,1,...,255')\n",
    "print('0 is white, 255 is black 2,3,...,254 are grays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# scale pixel values to [0,1] - this is recommended. \n",
    "X_train = X_train/255\n",
    "X_test  = X_test/255\n",
    "plt.imshow(X_train[row,:].reshape(28,28) , cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our Neural Network\n",
    "\n",
    "We want a neural network that will accept a digitized image as input\n",
    "\n",
    "Each image yields $28^2=784$ inputs, one for each pixel value.\n",
    "\n",
    "There are $10$ possible outputs - corresponding to the digits \n",
    "$\\{0,1,2,3,4,5,6,7,8,9\\}$.\n",
    "\n",
    "Our network will have $10$ outputs. For a given input we want the \n",
    "output to be all zeros except for one which is unity. This \n",
    "non-zero will be in the position of the label.\n",
    "\n",
    "So, if the label is $7$ then we want the output to be \n",
    "$(0,0,0,0,0,0,0,1,0,0)^T$. (Note the transpose - column vectors only.)\n",
    "\n",
    "This is called **one hot encoding**. Let's set up the output (label) data for\n",
    "the training and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# make every entry zero to begin with ...\n",
    "y_train = np.zeros((10, N_train))\n",
    "y_test = np.zeros((10, N_test))\n",
    "print(f'Shape of: y_train = {y_train.shape}, y_test = {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "We will one-hot encode the labels ready for implementation in a neural\n",
    "network. There are 10 possible output values corresponding to the labels\n",
    "$\\{0,1,2,3,4,5,6,7,8,9\\}$.\n",
    "\n",
    "We want to use the `train_labels` and `test_labels` data from above\n",
    "to create `y_train` and `y_test`.\n",
    "\n",
    "These will be matrices with each column having $10$ entries. \n",
    "\n",
    "In `y_train` there will be as many columns as there are training data points\n",
    "(i.e. `N_train`). And in `y_test` there will be as many columns as there\n",
    "are test data points (i.e. `N_test`).\n",
    "\n",
    "Each column contains zeros except for a single one in the position\n",
    "(0,1,2,...,9) corresponding to the label (0,1,2,...,9) for that column.\n",
    "\n",
    "Above we saw that with `choice = 2`, the third data point in the training set\n",
    "had label $4$.\n",
    "\n",
    "Hence the third column of `y_train` will be $(0,0,0,0,1,0,0,0,0,0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We loop through the two data sets, grab each label in turn, and set\n",
    "that position equal to unity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for k in range(N_train):\n",
    "  label = train_labels[k]\n",
    "  y_train[label,k] = 1\n",
    "\n",
    "for k in range(N_test):\n",
    "  label = test_labels[k]\n",
    "  y_test[label,k] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can plot the sums to see how many of each label there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(range(10),y_train.sum(axis=1), label='y train')\n",
    "plt.bar(range(10),y_test.sum(axis=1), label='y test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And, because we like to always be checking our work, we can add up all the\n",
    "ones in each of these and make sure there are the same sumber as\n",
    "`N_train` and `N_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# add up all the ones - across both dimensions\n",
    "print(y_train.sum())\n",
    "print(y_test.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that we've always insisted that our features vary along the \n",
    "columns, with our observations listed down the rows. \n",
    "\n",
    "That's what we have done for `X_train` and `X_test`. Each row is an observation\n",
    "of a handwritten digit. Each column - pixel value - is a feature.\n",
    "\n",
    "**NOTE** the term **one hot** is an electrical analogy wherein one terminal\n",
    "is considered *hot* and the other *cold* (i.e. *on* and *off*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our Neural Network Architecture\n",
    "\n",
    "We have an input layer of $28^2=784$ nodes (neurons) accepting\n",
    "a pixel value per node, and an output layer of $10$ neurons capable of\n",
    "yielding a one-hot encoded output.\n",
    "\n",
    "We'll choose **two hidden layers** with $500$ nodes on the first\n",
    "and $200$ on the second. These are all **hyperparameters** - we have to choose\n",
    "them and build them into our design. Once chosen they remain fixed.\n",
    "\n",
    "We have already seen this type of network, along with the feed forward\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is an artificial neural network with two hidden layers.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"./gfx/ann_4.png\" style=\"height:500px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_0 & = \\boldsymbol{x},\n",
    "\\\\\n",
    "\\boldsymbol{n}_1 & = \\boldsymbol{W}_1^T\\boldsymbol{a}_0+\\boldsymbol{b}_1,\n",
    "\\\\\n",
    "\\boldsymbol{a}_1 & = \\sigma_1(\\boldsymbol{n}_1),\n",
    "\\\\\n",
    "\\boldsymbol{n}_2 & = \\boldsymbol{W}_2^T\\boldsymbol{a}_1+\\boldsymbol{b}_2,\n",
    "\\\\\n",
    "\\boldsymbol{a}_2 & = \\sigma_2(\\boldsymbol{n}_2),\n",
    "\\\\\n",
    "\\boldsymbol{n}_3 & = \\boldsymbol{W}_3^T\\boldsymbol{a}_2+\\boldsymbol{b}_3,\n",
    "\\\\\n",
    "\\boldsymbol{a}_3 & = \\sigma_3(\\boldsymbol{n}_3),\n",
    "\\\\\n",
    "\\boldsymbol{y} & = \\boldsymbol{a}_3.\n",
    "\\end{align*}\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DEEP LEARNING\n",
    "\n",
    "The addition of extra **hidden** layers between the input and output \n",
    "layers gives rise to the **deep** in **DEEP LEARNING**.\n",
    "\n",
    "We'll see where the **learning** fits in shortly. We'lll need to\n",
    "**learn** the values of the weights and biases.\n",
    "\n",
    "For the moment though we initialise our weights with fairly small random \n",
    "numbers, and set our biases to be zero. We'll use the training data to\n",
    "learn better values.\n",
    "\n",
    "We initialise our network architecture along with the weights and biases as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "inn = 784  # number of nodes on input layer\n",
    "h1n = 500  # number of nodes on first hidden layer\n",
    "h2n = 200  # number of nodes on second hidden layer\n",
    "onn = 10   # number of nodes on output layer\n",
    "\n",
    "# weights and biases\n",
    "W1 = 0.5 - np.random.rand(inn,h1n) # weights connecting input to first hidden\n",
    "W2 = 0.5 - np.random.rand(h1n,h2n) # weights connecting first to second hidden\n",
    "W3 = 0.5 - np.random.rand(h2n,onn) # weights connecting second hidden to output\n",
    "b1 = np.zeros([h1n,1])             # bias on first hidden\n",
    "b2 = np.zeros([h2n,1])             # bias on second hidden\n",
    "b3 = np.zeros([onn,1])             # bias on output\n",
    "\n",
    "print(f'W1 shape: {W1.shape}, W2 shape: {W2.shape}, W3 shape: {W3.shape}')\n",
    "print(f'b1 shape: {b1.shape}, b2 shape: {b2.shape}, b3 shape: {b3.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Feed Forward Algorithm\n",
    "\n",
    "We have already seen this. The feed forward algortithm,\n",
    "for $L$ layers (not including the input layer) is,\n",
    "\n",
    "\\begin{align*}\n",
    "& \\boldsymbol{a}_0 = \\boldsymbol{x},\n",
    "\\\\\n",
    "& \\text{for } k = 1,2,\\ldots,L,\n",
    "\\\\\n",
    "&\\qquad \\boldsymbol{n}_k = \\boldsymbol{W}_k^T\\boldsymbol{a}_{k-1}+\\boldsymbol{b}_k,\n",
    "\\\\\n",
    "&\\qquad \\boldsymbol{a}_k = \\sigma_k(\\boldsymbol{n}_k),\n",
    "\\\\\n",
    "&\\boldsymbol{y} = \\boldsymbol{a}_L.\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma$ is the activation function. We have seen the Heaviside\n",
    "function for this, as well as the sigmoid and the ReLU. \n",
    "\n",
    "We'll define them using python functions. We will also need their\n",
    "derivatives (and that's the main reason for not using the Heaviside step\n",
    "function - it isn't differentiable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1/(1+np.exp(-x))\n",
    "def ReLU(x):\n",
    "  return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Diff_sigmoid(x):\n",
    "  return sigmoid(x)*(1-sigmoid(x))\n",
    "def Diff_ReLU(x):\n",
    "  return np.heaviside(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot them - just to remember what they look like\n",
    "xvals = np.arange(-5,5+0.1,0.1)\n",
    "plt.plot(xvals, sigmoid(xvals), color='blue', label='sigmoid')\n",
    "xvals = np.arange(-5,2+0.1,0.1)\n",
    "plt.plot(xvals, ReLU(xvals), color='red', label='ReLU')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feeding Forward - Forward Propagation\n",
    "\n",
    "Here is a typical forward pass through the network. We choose a random\n",
    "integer, $k$ from $\\{0,1,2,\\ldots,N_{\\mathrm{train}}\\}$, and use that to select a training point at random. Then\n",
    "we apply the algorithm from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = np.random.randint(0, X_train.shape[0])\n",
    "a0 = X_train[k,:].T # a0 = X_train[[k],:].T #a0 = X_train[k,:].reshape(-1,1)\n",
    "# feed into first hidden layer and activate\n",
    "n1 = W1.T @ a0 + b1\n",
    "a1 = sigmoid(n1)\n",
    "# feed into second hidden layer and activate\n",
    "n2 = W2.T @ a1 + b2\n",
    "a2 = sigmoid(n2)\n",
    "# feed into output layer and activate\n",
    "n3 = W3.T @ a2 + b3\n",
    "a3 = sigmoid(n3)\n",
    "# produce output\n",
    "y = a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**BUT THIS IS USELESS** - the weights are random and the biases are zero.\n",
    "Whatever the input, the output will be random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning - Artificial Intelligence (AI)\n",
    "\n",
    "We want to use the training data to **learn** better values of the\n",
    "weights and biases.\n",
    "\n",
    "By **better** we mean that given an input with label, say, $5$, the \n",
    "output should be $(0,0,0,0,0,1,0,0,0,0)^T$ - a one-hot encoding of the\n",
    "label corresponding to the input.\n",
    "\n",
    "In practice our weights and biases may never be **perfect**, and\n",
    "so we might not get perfect one-hot outputs. \n",
    "\n",
    "But we would be happy with, for example,\n",
    "\n",
    "$$\n",
    "(0.01,0.04,0.1,0.34,0.07,0.89,0.02,0.11,0.21,0.04)^T\n",
    "$$\n",
    "\n",
    "Here $0.89\\approx 1$ in the index $5$ position and the other values\n",
    "we treat as $\\approx 0$.\n",
    "\n",
    "If it works then we have created AI for digit recognition. This could\n",
    "be used for ANPR, handwriting, and it's not much of a conceptual step\n",
    "to move to digitized photos (face tagging) and voice\n",
    "(Alexa, Hi Siri, OK Google)... \n",
    "\n",
    "No human needed... \n",
    "\n",
    "But we need **better** weights and biases: we get them by setting up a\n",
    "cost and minimizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost: Total Squared Error (TSE)\n",
    "\n",
    "There are other choices, but we have seen this one before and so will use it. \n",
    "\n",
    "For a given training point, indexed by $k$ say, we have two outputs. One\n",
    "we call the ground truth, and is stored in `y_train`. We will call this\n",
    "$\\boldsymbol{t}_k$ for **truth**. It is perfectly one-hot encoded.\n",
    "\n",
    "The other output is the prediction from the network which is given by \n",
    "$\\boldsymbol{y}_k = \\boldsymbol{a}_L$ in the feed-forward algorithm above.\n",
    "This will not be perfectly one-hot encoded but as we have seen above \n",
    "we can be happy with just picking the maximum value and using it as\n",
    "an approximation to a one-hot encoding.\n",
    "\n",
    "We define the TSE cost as\n",
    "\n",
    "$$\n",
    "\\mathcal{E}(\\boldsymbol{W}_1,\\boldsymbol{W}_2,\\boldsymbol{W}_3,\n",
    "\\boldsymbol{b}_1,\\boldsymbol{b}_2,\\boldsymbol{b}_3)\n",
    "= \\sum_{k=1}^{N_{\\mathrm{train}}} \n",
    "\\mathscr{F}(\\boldsymbol{t}_k,\\boldsymbol{y}_k)\n",
    "$$\n",
    "\n",
    "$$\\text{for the loss}\\quad\n",
    "\\mathscr{F}_k := \\mathscr{F}(\\boldsymbol{t}_k,\\boldsymbol{y}_k) :=\n",
    "\\Vert\\boldsymbol{t}_k-\\boldsymbol{y}_k\\Vert_2^2.\n",
    "$$\n",
    "\n",
    "Normally we just write $\\mathcal{E}$ for brevity, but we have to always bear in\n",
    "mind that it depends on all the weights and all the biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Size of the Task\n",
    "\n",
    "We want to choose all the weights and biases so as to minimize the error. \n",
    "\n",
    "Recall the size of the weight matrices and bias vectors.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{W}_1 \\in\\mathbb{R}^{784\\times500},\n",
    "\\ \\boldsymbol{W}_2 \\in\\mathbb{R}^{500\\times200},\n",
    "\\ \\boldsymbol{W}_3 \\in\\mathbb{R}^{200\\times10},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{b}_1 \\in\\mathbb{R}^{500},\n",
    "\\ \\boldsymbol{b}_2 \\in\\mathbb{R}^{200},\n",
    "\\ \\boldsymbol{b}_3 \\in\\mathbb{R}^{10}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Wvals = W1.size + W2.size + W3.size\n",
    "bvals = b1.size + b2.size + b3.size\n",
    "print('Number of values to optimize = ', Wvals + bvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are nearly half a million values to optimize in the three\n",
    "full weight matrices, and the three bias vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "If you have ever studied multivariable calculus and looked at\n",
    "optimization problems you might have seen examples of how to optimize\n",
    "a function of two variables. For example,\n",
    "\n",
    "$$\n",
    "f(x,y) = 3\\,{\\mathrm{e}}^{-{\\left(y+1\\right)}^2-x^2}\\,{\\left(x-1\\right)}^2\n",
    "-\\frac{{\\mathrm{e}}^{-{\\left(x+1\\right)}^2-y^2}}{3}\n",
    "+{\\mathrm{e}}^{-x^2-y^2}\\,\\left(10\\,x^3-2\\,x+10\\,y^5\\right).\n",
    "$$\n",
    "\n",
    "You may remember that we need to calculate the gradient, $\\nabla f$, set it to zero,\n",
    "$\\nabla f=\\boldsymbol{0}$, solve this for the optimal points,\n",
    "and then check the Hessian for the nature of these optimal points. \n",
    "\n",
    "That isn't an option here. Instead the usual choice for minimizing\n",
    "the cost is **Stochastic Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent in Outline\n",
    "\n",
    "The idea is to consider $\\nabla f$ and note that this gives the\n",
    "direction in which $f$ increases most rapidly. \n",
    "\n",
    "Therefore $-\\nabla f$ tells us in which direction $f$ decreases most rapidly.\n",
    "\n",
    "That's what we want - we want to get to the minimum. *The bottom of the hill*\n",
    "\n",
    "We choose a point, $\\boldsymbol{x}_0$ say, and then move to a new\n",
    "point by **moving down the gradient**. We then iterate:\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{x}_1 & = \\boldsymbol{x}_0 - \\alpha\\nabla f(\\boldsymbol{x}_0),\n",
    "\\\\\n",
    "\\boldsymbol{x}_2 & = \\boldsymbol{x}_1 - \\alpha\\nabla f(\\boldsymbol{x}_1),\n",
    "\\\\\n",
    "\\boldsymbol{x}_3 & = \\boldsymbol{x}_2 - \\alpha\\nabla f(\\boldsymbol{x}_2),\n",
    "\\\\\n",
    "\\vdots & \\qquad\\vdots\\qquad \\vdots\n",
    "\\end{align}\n",
    "\n",
    "Once we find a $k$ for which $\\nabla f(\\boldsymbol{x}_k)=\\boldsymbol{0}$ (at least\n",
    "approximately) we stop - we'll be at a minimum (approximately). Note that \n",
    "it might not be a **global minimum**, and it could even be a **saddle point**.\n",
    "\n",
    "Here, and in this context, $\\alpha$ is called the **learning rate**. It is another **hyperparameter**. We choose it - it does not get learned by the algorithm.\n",
    "\n",
    "Let's see some examples of how this might work in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**NOTE** these diagrams will not show up in the PDF version of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent Demonstrations - Global and Local Minima\n",
    "\n",
    "Diagrams will not show in PDF version.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "        <img src=\"./gfx/GradientDescent/GlobalMin.gif\" style=\"height:300px\"/>\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"./gfx/GradientDescent/LocalMin.gif\" style=\"height:300px\"/>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent Demonstrations - Saddle Points\n",
    "\n",
    "Diagrams will not show in PDF version.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "        <img src=\"./gfx/GradientDescent/SaddlePoint1.gif\" style=\"height:300px\"/>\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"./gfx/GradientDescent/SaddlePoint2.gif\" style=\"height:300px\"/>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient (Descent?)\n",
    "\n",
    "The Gradient Descent process described above is too computationally expensive\n",
    "for training big neural nets. \n",
    "\n",
    "Ours has nearly **half a million** independent variables, but that is pretty\n",
    "modest by some standards. \n",
    "\n",
    "This number of independent variables tells us how many gradient component\n",
    "directions there are. \n",
    "\n",
    "Also `N_train` tells us how many loss terms there are in the cost function - these \n",
    "would need to be simultaneously minimized. \n",
    "\n",
    "A variant, called **Stochastic Gradient Descent**, or **SGD** for short, \n",
    "is often used to save computer time and resources.\n",
    "\n",
    "We'll use this in its simplest form (so-called **mini-batch** approaches exist) \n",
    "which is to just pick at random, and without replacement, one loss at a time\n",
    "and update with that.\n",
    "\n",
    "Some people call this **Stochastic Gradient** instead of\n",
    "**Stochastic Gradient Descent** because it is not guaranteed that descent\n",
    "actually occurs at a given step.\n",
    "\n",
    "Like so much of what we have done, there is so much more we could be saying\n",
    "on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 1\n",
    "\n",
    "We're going to just outline the main steps. A deep understanding of this \n",
    "is beyond our scope.\n",
    "\n",
    "Consider the weight matrix connecting the second hidden layer to the output layer:\n",
    "$\\boldsymbol{W}_3$ and let $w_{rc}$ denote the entry in row $r$ and column $c$. \n",
    "Also, consider $\\boldsymbol{b}_3$ with $b_r$ in row $r$. \n",
    "\n",
    "We choose a loss term at random - the $k$-th term say, $\\mathscr{F}_k$ - and\n",
    "then an SGD update is then written like this\n",
    "\n",
    "$$\n",
    "w_{rc} \\leftarrow w_{rc} -\\alpha\\frac{\\partial\\mathscr{F}_k}{\\partial w_{rc}} \n",
    "\\qquad\\text{ and }\\qquad\n",
    "b_{r} \\leftarrow b_{r} -\\alpha\\frac{\\partial\\mathscr{F}_k}{\\partial b_{r}} \n",
    "$$\n",
    "\n",
    "where $\\leftarrow$ means *is replaced by*.\n",
    "Now, $\\mathscr{F}_k = \\Vert\\boldsymbol{t}_k-\\boldsymbol{y}_k\\Vert_2^2$ and so,\n",
    "concentrating on the weights,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathscr{F}_k}{\\partial w_{rc}}\n",
    "= \n",
    "\\frac{\\partial}{\\partial w_{rc}}\n",
    "\\Vert\\boldsymbol{t}_k-\\boldsymbol{y}_k\\Vert_2^2\n",
    "= \n",
    "\\frac{\\partial}{\\partial w_{rc}}\n",
    "\\sum_{\\ell=1}^{10}\n",
    "(t_{\\ell k}- y_{\\ell k})^2.\n",
    "$$\n",
    "\n",
    "In this $\\boldsymbol{t}_k$ is a constant, so we need to think about $y_{\\ell k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 2\n",
    "\n",
    "We recall the forward prop algorithm: \n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a}_0 & = \\boldsymbol{x},\n",
    "\\\\\n",
    "\\boldsymbol{n}_1 & = \\boldsymbol{W}_1^T\\boldsymbol{a}_0+\\boldsymbol{b}_1,\n",
    "\\\\\n",
    "\\boldsymbol{a}_1 & = \\sigma_1(\\boldsymbol{n}_1),\n",
    "\\\\\n",
    "\\boldsymbol{n}_2 & = \\boldsymbol{W}_2^T\\boldsymbol{a}_1+\\boldsymbol{b}_2,\n",
    "\\\\\n",
    "\\boldsymbol{a}_2 & = \\sigma_2(\\boldsymbol{n}_2),\n",
    "\\\\\n",
    "\\boldsymbol{n}_3 & = \\boldsymbol{W}_3^T\\boldsymbol{a}_2+\\boldsymbol{b}_3,\n",
    "\\\\\n",
    "\\boldsymbol{a}_3 & = \\sigma_3(\\boldsymbol{n}_3),\n",
    "\\\\\n",
    "\\boldsymbol{y} & = \\boldsymbol{a}_3.\n",
    "\\end{align*}\n",
    "\n",
    "In this it is the $k$-th component of $\\boldsymbol{y} = \\boldsymbol{a}_3$\n",
    "that we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 3\n",
    "\n",
    "So, we look at the $k$-th component and calculate...\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathscr{F}_k}{\\partial w_{rc}}\n",
    "= \n",
    "\\frac{\\partial}{\\partial w_{rc}}\n",
    "\\sum_{\\ell=1}^{10}\n",
    "(t_{\\ell k}- y_{\\ell k})^2\n",
    "= \n",
    "-2\\sum_{\\ell=1}^{10}\n",
    "(t_{\\ell k}- y_{\\ell k})\n",
    "\\frac{\\partial y_{\\ell k}}{\\partial w_{rc}}\n",
    "$$\n",
    "\n",
    "But (with $L=10$ outputs and $H=200$ neurons on the second hidden layer),\n",
    "and $\\boldsymbol{a}_2 = (a_1, a_2, \\ldots)^T$,\n",
    "\n",
    "$$\n",
    "y_{\\ell k} = \\sigma(n_{\\ell})\n",
    "\\quad\\text{for}\\quad\n",
    "n_{\\ell} = \\sum_{h=1}^{H} w_{h\\ell}a_h + b_\\ell\n",
    "\\quad\\text{ and }\\quad\n",
    "\\frac{\\partial y_{\\ell k}}{\\partial w_{rc}}\n",
    "= \n",
    "\\sigma'(n_{\\ell})\n",
    "\\frac{\\partial n_{\\ell}}{\\partial w_{rc}}\n",
    "$$\n",
    "\n",
    "with $\\boldsymbol{n}_{3k} = (n_1, n_2, \\ldots)^T$.\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial n_{\\ell k}}{\\partial w_{rc}}\n",
    "= \\frac{\\partial}{\\partial w_{rc}}\\sum_{h=1}^{H} w_{h\\ell}a_h + b_\\ell\n",
    "\\quad\\text{ hence }\\quad\n",
    "\\frac{\\partial n_{\\ell k}}{\\partial w_{rc}}\n",
    "= a_r\\delta_{c\\ell}\n",
    "$$\n",
    "\n",
    "where $\\delta_{ij} = 1$ if $i=j$ and $\\delta_{ij} = 0$ otherwise (called the **Kronecker delta**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 4\n",
    "\n",
    "Putting this together we then get, first, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{\\ell k}}{\\partial w_{rc}}\n",
    "= \n",
    "\\sigma'(n_{\\ell k})\n",
    "\\frac{\\partial n_{\\ell k}}{\\partial w_{rc}}\n",
    "= \n",
    "\\sigma'(n_{\\ell k}) a_r\\delta_{c\\ell}.\n",
    "$$\n",
    "\n",
    "Therefore, for $e_{\\ell k} = t_{\\ell k}- y_{\\ell k}$,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathscr{F}_k}{\\partial w_{rc}}\n",
    "= \n",
    "-2\\sum_{\\ell=1}^{10}\n",
    "(t_{\\ell k}- y_{\\ell k})\n",
    "\\sigma'(n_{\\ell k}) a_r\\delta_{c\\ell}\n",
    "= \n",
    "-2\\sum_{\\ell=1}^{10}\n",
    "e_{\\ell k}\n",
    "\\sigma'(n_{\\ell k}) a_r\\delta_{c\\ell}\n",
    "$$\n",
    "\n",
    "Let $\\displaystyle\\frac{\\partial\\mathscr{F}_k}{\\partial \\boldsymbol{W}_3}$\n",
    "be the matrix with $\\displaystyle\\frac{\\partial\\mathscr{F}_k}{\\partial w_{rc}}$\n",
    "in row $r$ and column $c$. Then, after some manipulations, it can be shown that,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathscr{F}_k}{\\partial \\boldsymbol{W}_3}\n",
    "= \n",
    "\\boldsymbol{a}_2 \\boldsymbol{S}_3^T\n",
    "\\text{ for }\n",
    "\\boldsymbol{S}_3 = -2 \\boldsymbol{A}_3\\boldsymbol{e}_k\n",
    "\\text{ and }\n",
    "\\boldsymbol{A}_3 = \\left(\\begin{array}{llll}\n",
    "\\sigma'(n_1) & 0 & 0 & \\cdots  \\\\\n",
    "0 & \\sigma'(n_2) & 0 & \\cdots  \\\\\n",
    "0 & 0 & \\sigma'(n_3) & \\cdots  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 5\n",
    "\n",
    "With similar manipulations it can also be shown that \n",
    "$\\displaystyle\\frac{\\partial\\mathscr{F}_k}{\\partial \\boldsymbol{b}_3} = \\boldsymbol{S}$.\n",
    "\n",
    "The gradient updates from earlier,\n",
    "\n",
    "$$\n",
    "w_{rc} \\leftarrow w_{rc} -\\alpha\\frac{\\partial\\mathscr{F}_k}{\\partial w_{rc}} \n",
    "\\qquad\\text{ and }\\qquad\n",
    "b_{r} \\leftarrow w_{r} -\\alpha\\frac{\\partial\\mathscr{F}_k}{\\partial b_{r}} \n",
    "$$\n",
    "\n",
    "can now be written in explicit and computable form as,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{W}_3 \\leftarrow \\boldsymbol{W}_3\n",
    "- \\alpha \\boldsymbol{a}_2 \\boldsymbol{S}_3^T\n",
    "\\qquad\\text{ and }\\qquad\n",
    "\\boldsymbol{b}_3 \\leftarrow \\boldsymbol{b}_3\n",
    "- \\alpha \\boldsymbol{S}_3\n",
    "$$\n",
    "\n",
    "These tell us how to update the weights and biases at the output end of the\n",
    "network. \n",
    "\n",
    "But what about $\\boldsymbol{W}_2$, $\\boldsymbol{b}_2$ and \n",
    "$\\boldsymbol{W}_1$, $\\boldsymbol{b}_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 6\n",
    "\n",
    "To update the weights and biases further down towards the start of the network\n",
    "we need \n",
    "\n",
    "$$\n",
    "w_{rc} \\leftarrow w_{rc} -\\alpha\\frac{\\partial\\mathscr{F}_k}{\\partial w_{rc}} \n",
    "\\qquad\\text{ and }\\qquad\n",
    "b_{r} \\leftarrow w_{r} -\\alpha\\frac{\\partial\\mathscr{F}_k}{\\partial b_{r}} \n",
    "$$\n",
    "\n",
    "where $w_{rc}$ and $b_r$ now refer to $\\boldsymbol{W}_2$, $\\boldsymbol{b}_2$ first,\n",
    "and then to $\\boldsymbol{W}_1$, $\\boldsymbol{b}_1$.\n",
    "\n",
    "We could attempt a direct calculation, as above, but this would get quite involved\n",
    "as the number of layers increases. Instead we use the **back propagation**\n",
    "formula:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_{i-1}=\\boldsymbol{A}_{i-1}\\boldsymbol{W}_i\\boldsymbol{S}_i\n",
    "$$\n",
    "\n",
    "which is applied for $i=L, L-1, \\ldots, 2$.\n",
    "\n",
    "The derivation of this is quite involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 7\n",
    "\n",
    "We consider two consecutive layers, $i-1$ and $i$, connected with the weights in\n",
    "$\\boldsymbol{W}_i$ and with biases $\\boldsymbol{b}_i$ added on layer $i$.\n",
    "Assume there are $H$ nodes on layer $i-1$ and $L$ on layer $i$. \n",
    "We use hats to denote quantities on layer $i-1$, and then the key formulae are,\n",
    "\n",
    "$$\n",
    "n_c = \\sum_{\\ell=1}^H\n",
    "w_{\\ell c} \\hat{a}_\\ell + b_c,\n",
    "\\quad\n",
    "\\hat{\\boldsymbol{a}} = \\sigma_{i-1}(\\hat{\\boldsymbol{n}})\n",
    "\\quad\\text{ and }\\quad\n",
    "\\boldsymbol{a} = \\sigma_i(\\boldsymbol{n}).\n",
    "$$\n",
    "\n",
    "Then (and similarly for the biases),\n",
    "\n",
    "$$\n",
    "\\boldsymbol{W} \\leftarrow \\boldsymbol{W} - \\alpha\n",
    "\\frac{\\partial\\mathscr{F}}{\\partial w_{rc}}\n",
    "\\quad\\text{ uses }\\quad\n",
    "\\frac{\\partial\\mathscr{F}}{\\partial w_{rc}}\n",
    "=\n",
    "\\frac{\\partial\\mathscr{F}}{\\partial n_{c}}\n",
    "\\frac{\\partial n_c}{\\partial w_{rc}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{with }\n",
    "\\frac{\\partial n_c}{\\partial w_{rc}}\n",
    "=\n",
    "\\frac{\\partial }{\\partial w_{rc}}\n",
    "\\sum_{\\ell=1}^H\n",
    "w_{\\ell c} \\hat{a}_\\ell + b_c\n",
    "=\n",
    "\\hat{a}_r\n",
    "\\Longrightarrow\n",
    "\\frac{\\partial\\mathscr{F}}{\\partial w_{rc}}\n",
    "= \\hat{a}_r S_c\n",
    "\\text{ with }\n",
    "\\boldsymbol{S} = \\left(\\begin{array}{c}\n",
    "\\partial\\mathscr{F}/\\partial n_1 \\\\\n",
    "\\partial\\mathscr{F}/\\partial n_2 \\\\\n",
    "\\partial\\mathscr{F}/\\partial n_3 \\\\\n",
    "\\vdots\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 8\n",
    "\n",
    "We can conclude from this that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathscr{F}}{\\partial\\boldsymbol{W}_i}\n",
    "= \\hat{\\boldsymbol{a}}\\boldsymbol{S}^T\n",
    "\\quad\\text{ and }\n",
    "\\frac{\\partial\\mathscr{F}}{\\partial\\boldsymbol{b}_i}\n",
    "= \\boldsymbol{S}\n",
    "$$\n",
    "\n",
    "just as earlier at the output layer.\n",
    "\n",
    "A key step is now to introduce the **Jacobian Matrix** \n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\boldsymbol{n}}{\\partial\\hat{\\boldsymbol{n}}}\n",
    "=\n",
    "\\left(\\begin{array}{cccc}\n",
    "\\partial n_1/\\partial\\hat{n}_1 & \\partial n_1/\\partial\\hat{n}_2 & \\partial n_1/\\partial\\hat{n}_3 & \\cdots\n",
    "\\\\ \n",
    "\\partial n_2/\\partial\\hat{n}_1 & \\partial n_2/\\partial\\hat{n}_2 & \\partial n_2/\\partial\\hat{n}_3 & \\cdots\n",
    "\\\\\n",
    "\\partial n_3/\\partial\\hat{n}_1 & \\partial n_3/\\partial\\hat{n}_2 & \\partial n_3/\\partial\\hat{n}_3 & \\cdots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "and then use $\\hat{\\boldsymbol{a}} = \\hat{\\sigma}(\\hat{\\boldsymbol{n}})$ to calculate\n",
    "\n",
    "$$\n",
    "\\frac{\\partial n_c}{\\partial\\hat{n}_r}\n",
    "=\n",
    "\\frac{\\partial }{\\partial\\hat{n}_r}\n",
    "\\sum_{\\ell=1}^H w_{\\ell c} \\hat{a}_\\ell + b_c\n",
    "=\n",
    "w_{rc}\\frac{\\partial\\hat{a}_r}{\\partial\\hat{n}_r}\n",
    "=\n",
    "w_{rc}\\hat{\\sigma}'(\\hat{n}_r).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 9\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\boldsymbol{n}}{\\partial\\hat{\\boldsymbol{n}}}\n",
    "=\n",
    "\\boldsymbol{W}^T\\hat{\\boldsymbol{A}}\n",
    "\\quad\\text{ for }\\quad\n",
    "\\hat{\\boldsymbol{A}} = \\left(\\begin{array}{llll}\n",
    "\\hat{\\sigma}'(\\hat{n}_1) & 0 & 0 & \\cdots  \\\\\n",
    "0 & \\hat{\\sigma}'(\\hat{n}_2) & 0 & \\cdots  \\\\\n",
    "0 & 0 & \\hat{\\sigma}'(\\hat{n}_3) & \\cdots  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "From above we define $\\hat{\\boldsymbol{S}}$ analogously to $\\boldsymbol{S}$ as\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{S}} = \\left(\\begin{array}{c}\n",
    "\\partial\\mathscr{F}/\\partial \\hat{n}_1 \\\\\n",
    "\\partial\\mathscr{F}/\\partial \\hat{n}_2 \\\\\n",
    "\\partial\\mathscr{F}/\\partial \\hat{n}_3 \\\\\n",
    "\\vdots\n",
    "\\end{array}\\right)\n",
    "\\quad\\text{ then }\\quad\n",
    "\\hat{S}_r\n",
    "= \\frac{\\partial\\mathscr{F}}{\\partial\\hat{n}_r}\n",
    "= \\sum_\\ell\\frac{\\partial n_\\ell}{\\partial\\hat{n}_r}\\frac{\\partial\\mathscr{F}}{\\partial n_\\ell}\n",
    "= \\sum_\\ell\\frac{\\partial n_\\ell}{\\partial\\hat{n}_r}S_\\ell\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{and so }\n",
    "\\hat{\\boldsymbol{S}}\n",
    "= \\left(\\frac{\\partial\\boldsymbol{n}}{\\partial\\hat{\\boldsymbol{n}}}\\right)^T\\boldsymbol{S}\n",
    "= \\left(\\boldsymbol{W}^T\\hat{\\boldsymbol{A}}\\right)^T\\boldsymbol{S}\n",
    "= \\hat{\\boldsymbol{A}}\\boldsymbol{W}\\boldsymbol{S}\n",
    "\\text{ because } \\hat{\\boldsymbol{A}}=\\hat{\\boldsymbol{A}}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Calculus of Learning - part 10\n",
    "\n",
    "Layer-by-layer this **recursion** is\n",
    "$\\boldsymbol{S}_{i-1}=\\boldsymbol{A}_{i-1}\\boldsymbol{W}_i\\boldsymbol{S}_i$\n",
    "and is called **back propagation**.\n",
    "\n",
    "\n",
    "From above we have \n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathscr{F}}{\\partial\\boldsymbol{W}_i}\n",
    "= \\boldsymbol{a}_{i-1}\\boldsymbol{S}_i^T\n",
    "\\quad\\text{ and }\n",
    "\\frac{\\partial\\mathscr{F}}{\\partial\\boldsymbol{b}_i}\n",
    "= \\boldsymbol{S}_i\n",
    "$$\n",
    "\n",
    "We have $\\boldsymbol{S}_L = -2\\boldsymbol{A}_i\\boldsymbol{e}$ at the output layer,\n",
    "and we can compute $\\boldsymbol{S}_{L-1}$, $\\boldsymbol{S}_{L-2}$, $\\boldsymbol{S}_{L-3}, \\ldots$,\n",
    "recursively from the **SAWS** backprop recursion \n",
    "$\\boldsymbol{S}_{i-1}=\\boldsymbol{A}_{i-1}\\boldsymbol{W}_i\\boldsymbol{S}_i$\n",
    "\n",
    "We now have eveything we need..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Forward and Backward Propagation ('backprop') Algorithm - Learning from Data\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{array}{rl}\n",
    "\\text{forward} &\\text{prop} \n",
    "\\\\\\ \\\\\n",
    "\\boldsymbol{a}_0 & = \\boldsymbol{x},\n",
    "\\\\\n",
    "\\boldsymbol{n}_1 & = \\boldsymbol{W}_1^T\\boldsymbol{a}_0+\\boldsymbol{b}_1,\n",
    "\\\\\n",
    "\\boldsymbol{a}_1 & = \\sigma_1(\\boldsymbol{n}_1),\n",
    "\\\\\n",
    "\\boldsymbol{n}_2 & = \\boldsymbol{W}_2^T\\boldsymbol{a}_1+\\boldsymbol{b}_2,\n",
    "\\\\\n",
    "\\boldsymbol{a}_2 & = \\sigma_2(\\boldsymbol{n}_2),\n",
    "\\\\\n",
    "\\boldsymbol{n}_3 & = \\boldsymbol{W}_3^T\\boldsymbol{a}_2+\\boldsymbol{b}_3,\n",
    "\\\\\n",
    "\\boldsymbol{a}_3 & = \\sigma_3(\\boldsymbol{n}_3),\n",
    "\\\\\n",
    "\\boldsymbol{y} & = \\boldsymbol{a}_3.\n",
    "\\end{array}\n",
    "\\qquad &\\qquad\n",
    "\\begin{array}{rl}\n",
    "\\text{back} &\\text{prop}\n",
    "\\\\\\ \\\\\n",
    "\\boldsymbol{e}_k & = \\boldsymbol{t}_k-\\boldsymbol{y}_k,\n",
    "\\\\\n",
    "\\boldsymbol{S}_3 & = -2\\boldsymbol{A}_3\\boldsymbol{e}_k,\n",
    "\\\\\n",
    "\\boldsymbol{W}_3 & \\leftarrow \\boldsymbol{W}_3\n",
    "- \\alpha \\boldsymbol{a}_2 \\boldsymbol{S}_3^T\n",
    "\\text{ and }\n",
    "\\boldsymbol{b}_3 \\leftarrow \\boldsymbol{b}_3\n",
    "- \\alpha \\boldsymbol{S}_3\n",
    "\\\\\n",
    "\\boldsymbol{S}_2 & = \\boldsymbol{A}_2\\boldsymbol{W}_3\\boldsymbol{S}_3\n",
    "\\\\\n",
    "\\boldsymbol{W}_2 & \\leftarrow \\boldsymbol{W}_2\n",
    "- \\alpha \\boldsymbol{a}_1 \\boldsymbol{S}_2^T\n",
    "\\text{ and }\n",
    "\\boldsymbol{b}_2 \\leftarrow \\boldsymbol{b}_2\n",
    "- \\alpha \\boldsymbol{S}_2\n",
    "\\\\\n",
    "\\boldsymbol{S}_1 & = \\boldsymbol{A}_1\\boldsymbol{W}_2\\boldsymbol{S}_2\n",
    "\\\\\n",
    "\\boldsymbol{W}_1 & \\leftarrow \\boldsymbol{W}_1\n",
    "- \\alpha \\boldsymbol{a}_0 \\boldsymbol{S}_1^T\n",
    "\\text{ and }\n",
    "\\boldsymbol{b}_1 \\leftarrow \\boldsymbol{b}_1\n",
    "- \\alpha \\boldsymbol{S}_1\n",
    "\\end{array}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our Neural Network - training and testing\n",
    "\n",
    "Here is the basic implementation algortithm for learning and testing\n",
    "\n",
    "- Choose hyperparameters such as learning rate $\\alpha$ and network architecture.\n",
    "\n",
    "- Choose a positive integer $N_{\\mathrm{epochs}}$ as the **number of epochs** to use in \n",
    "training. An **epoch** is a single loop through the whole training set, updating weights\n",
    "and biases for each data point.\n",
    "\n",
    "- For each epoch, loop through the training points by choosing integers\n",
    "$k$ at random from $\\{0,1,2,\\ldots,N_{\\mathrm{train}}-1\\}$ without replacement\n",
    "(in code the indices start at zero).\n",
    "\n",
    "  - Forward prop that training data point and calculate the error\n",
    "    $\\boldsymbol{e}_k = \\boldsymbol{t}_k - \\boldsymbol{y}_k$\n",
    "\n",
    "  - Use the error to initiate the backprop and gradient descent updates.\n",
    "  \n",
    "  - Repeat for the next $k$\n",
    "\n",
    "- At the end of each epoch update the cost $\\mathcal{E}$ for later plotting.\n",
    "\n",
    "- At the end of training, run the test data through one point at a time,\n",
    "and use the approximate one-hot encoding in the outputs $\\boldsymbol{y}$\n",
    "to assess the accuracy of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The random selection is done **without replacement**. We use \n",
    "\n",
    "```\n",
    "ransel = np.random.permutation(N_train)\n",
    "```\n",
    "\n",
    "which gives us an array containing a random permutation of the \n",
    "indices in `[0,1,2,...,N_train-1]`.\n",
    "\n",
    "Here's the code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# select a learning rate for SGD\n",
    "alpha = 0.3\n",
    "# loop through this many epochs\n",
    "N_ep = 50\n",
    "# initialise the TSE cost\n",
    "TSEcost = np.zeros([N_ep,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We train by looping `N_ep` times through the training set.\n",
    "\n",
    "Each training set loop randomly selects a loss term in the total cost with\n",
    "which to learn (update) new values for the weights and biases.\n",
    "\n",
    "In the slides view the next cell will not fully display - it is too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for ep in range(N_ep): \n",
    "  ransel = np.random.permutation(N_train)\n",
    "  for k in range(N_train):\n",
    "    # select a random without replacement\n",
    "    j = ransel[k]\n",
    "    # forward prop\n",
    "    a0 = X_train[[j],:].T \n",
    "    n1 = W1.T @ a0 + b1\n",
    "    a1 = sigmoid(n1)\n",
    "    n2 = W2.T @ a1 + b2\n",
    "    a2 = sigmoid(n2)\n",
    "    n3 = W3.T @ a2 + b3\n",
    "    a3 = sigmoid(n3)\n",
    "    y = a3\n",
    "    # backprop and update\n",
    "    error = y_train[:,[j]] - y\n",
    "    A3 = np.diagflat(Diff_sigmoid(n3))\n",
    "    A2 = np.diagflat(Diff_sigmoid(n2))\n",
    "    A1 = np.diagflat(Diff_sigmoid(n1))\n",
    "    S3 = -2*A3@error\n",
    "    S2 = A2@W3@S3\n",
    "    S1 = A1@W2@S2\n",
    "\n",
    "    W3 = W3 - alpha * a2@S3.T\n",
    "    W2 = W2 - alpha * a1@S2.T\n",
    "    W1 = W1 - alpha * a0@S1.T\n",
    "\n",
    "    b3 = b3 - alpha * S3\n",
    "    b2 = b2 - alpha * S2\n",
    "    b1 = b1 - alpha * S1\n",
    "\n",
    "  # update cost - loop through training set\n",
    "  for j in range(N_train):\n",
    "    a0 = X_train[[j],:].T\n",
    "    n1 = W1.T @ a0 + b1\n",
    "    a1 = sigmoid(n1)\n",
    "    n2 = W2.T @ a1 + b2\n",
    "    a2 = sigmoid(n2)\n",
    "    n3 = W3.T @ a2 + b3\n",
    "    a3 = sigmoid(n3)\n",
    "    y = a3\n",
    "    error = y_train[:,[j]] - y\n",
    "    TSEcost[ep] += (error * error).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is usual to plot the cost against epochs. We want to see the cost\n",
    "reduce to zero, or at least near-zero. If it doesn't then our training\n",
    "is not effective and we would consider adjusting the hyperparameters.\n",
    "These are:\n",
    "\n",
    "- network architecture: number of hidden layers, nodes per layer\n",
    "- learning rate\n",
    "- number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(N_ep), TSEcost)\n",
    "plt.xlabel('epoch'); plt.ylabel('cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing\n",
    "\n",
    "Once it is trained we want to test the network's predictions on unseen data. We'll \n",
    "store all the predictions for a confusion matrix, and also create scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# test - create a matrix to hold the predictions\n",
    "y_pred = np.zeros((10, X_test.shape[0]))\n",
    "print(f'Shape of y_test = {y_test.shape}')\n",
    "print(f'Shape of y_pred = {y_pred.shape}')\n",
    "\n",
    "# create scorecards...\n",
    "success = np.zeros(10)\n",
    "failure = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for k in range(N_test):\n",
    "  # forward prop\n",
    "  a0 = X_test[[k],:].T   # a0 = X_test[k,:].reshape(-1,1)\n",
    "  n1 = W1.T @ a0 + b1\n",
    "  a1 = sigmoid(n1)\n",
    "  n2 = W2.T @ a1 + b2\n",
    "  a2 = sigmoid(n2)\n",
    "  n3 = W3.T @ a2 + b3\n",
    "  a3 = sigmoid(n3)\n",
    "  y_pred[:,[k]] = a3  \n",
    "  if np.argmax(a3) == test_labels[k]:\n",
    "    success[test_labels[k]] += 1\n",
    "  else:\n",
    "    failure[test_labels[k]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(success)\n",
    "print(failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "We have seen these before. We need to determine the positions of the \n",
    "maximum entries in the computed approximate one-hot encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_test_cm = np.zeros(N_test)\n",
    "y_pred_cm = np.zeros(N_test)\n",
    "test_indx_max = np.argmax(y_test, axis=0)\n",
    "pred_indx_max = np.argmax(y_pred, axis=0)\n",
    "for k in range(N_test):\n",
    "  y_test_cm[k] = test_indx_max[k]\n",
    "  y_pred_cm[k] = pred_indx_max[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test_cm, y_pred_cm)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "accsc = accuracy_score(y_test_cm,y_pred_cm)\n",
    "print(\"Accuracy:\", accsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cmplot = ConfusionMatrixDisplay(cm, display_labels=range(10))\n",
    "#plt.figure(figsize=(15, 15))\n",
    "#cmplot.plot()\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "cmplot.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "\n",
    "We got modest accuracy. **Can you do better?**\n",
    "\n",
    "You can obtain the MNIST data from \n",
    "\n",
    "<https://variationalform.github.io/downloads/MNIST.zip>\n",
    "\n",
    "There are some handwritten notes that expand further on the maths of backprop.\n",
    "They are here:\n",
    "\n",
    "<https://variationalform.github.io/downloads/Backprop.pdf>\n",
    "\n",
    "We covered *just enough*, to make *progress at pace*. We looked at\n",
    "\n",
    "- neural network Architecture\n",
    "- training: cost, and backprop\n",
    "- testing and evaluation\n",
    "\n",
    "There is much much more - as usual. \n",
    "\n",
    "Let's finish with a few observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Functions and Cost\n",
    "\n",
    "We used the sigmoid rather than the Heaviside because we needed its derivative\n",
    "in the backprop algorithm. It also gives us output values between zero and one\n",
    "which is useful for the approximate one-hot encoding.\n",
    "\n",
    "However, we could have used the ReLU on the hidden layers. You can try this - you\n",
    "just need to alter those activations in the traiing and testing forward prop, and\n",
    "also in the backprop.\n",
    "\n",
    "There is also a cost function that can be described as a **cross entropy**. This\n",
    "is used with a **softmax** activation on the final layer. We wont cover that here,\n",
    "but be aware that this approach is often claimed to be superior for deep learning\n",
    "classification tasks.\n",
    "\n",
    "Deep Learning can also be used for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Power Consumption\n",
    "\n",
    "There are some live discussions out there regarding the carbon footprint of\n",
    "intensive computation. Machine Learning and AI in particular need a lot \n",
    "of such power. \n",
    "\n",
    "It is worth giving it some thought.\n",
    "\n",
    "Here are the results of a crude experiment\n",
    "\n",
    "On a Mac, in the Terminal app, this command gives details of the battery.\n",
    "\n",
    "```\n",
    "system_profiler SPPowerDataType\n",
    "```\n",
    "\n",
    "With a fully charged battery, the charger was disconnected, the display dimmed,\n",
    "and this notebook was run five times up to the point where the cost is plotted\n",
    "against epochs.\n",
    "\n",
    "The command above reported $6439\\,\\mathrm{mAh}$ at the start, and\n",
    "$6132\\,\\mathrm{mAh}$ after the five runs, and also reported a \n",
    "$12.5\\,\\mathrm{V}$ battery. \n",
    "\n",
    "This was for 50 epochs, with a [784, 500, 200, 10] architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare this energy cost to having a standard $28\\,\\mathrm{W}$ light bulb switched on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print( (6439 - 6132)/5, ' mAh per training cycle')\n",
    "print( (6439 - 6132)/5/1000, ' Ah per training cycle')\n",
    "print( 12.5*(6439 - 6132)/5/1000, ' Wh per training cycle')\n",
    "print( 28 / (12.5*(6439 - 6132)/5/1000), ' training cycles for 1 hour of a 28 watt bulb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Comments? Thoughts? What about those big server farms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Tensorflow Playground\n",
    "\n",
    "Tensorflow is a widely used deep learning library. This web page\n",
    "<http://playground.tensorflow.org> allows you to configure your own \n",
    "neural net, play with the hyper parameters, and see the effect for various \n",
    "classification problems.\n",
    "\n",
    "PyTorch is similarly widely used. There are also others but these are ones\n",
    "we seem to hear most about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Learning in a few lines\n",
    "\n",
    "The code that follows came from \n",
    "<https://twitter.com/svpino/status/1582703127651721217>\n",
    "on 21 Oct 2023.\n",
    "\n",
    "It shows that you can implement deep learning with just a few lines\n",
    "if the problem and data are simple enough.\n",
    "\n",
    "This code learns logic gates. You can read about the **Exclusive OR**\n",
    "in particular in the [DL] book, Chapter 6.\n",
    "\n",
    "This code might not dosplay properly in the slides medium - it's too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1+np.exp(-x))\n",
    "\n",
    "def neural_network(X,y):\n",
    "    learning_rate = 0.1\n",
    "    W1 = np.random.rand(2,4)\n",
    "    W2 = np.random.rand(4,1)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        layer1 = sigmoid(np.dot(X, W1))\n",
    "        output = sigmoid(np.dot(layer1, W2))\n",
    "        error = y-output\n",
    "        delta2 = 2 * error * (output * (1 - output))\n",
    "        delta1 = delta2.dot(W2.T) * (layer1 * (1 - layer1))\n",
    "        W2 += learning_rate * layer1.T.dot(delta2)\n",
    "        W1 += learning_rate * X.T.dot(delta1)\n",
    "    \n",
    "    return np.round(output).flatten()\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "print(  \"OR\", neural_network( X, np.array([[0,1,1,1]]).T ) )\n",
    "print( \"AND\", neural_network( X, np.array([[0,0,0,1]]).T ) )\n",
    "print( \"XOR\", neural_network( X, np.array([[0,1,1,0]]).T ) )\n",
    "print(\"NAND\", neural_network( X, np.array([[1,1,1,0]]).T ) )\n",
    "print( \"NOR\", neural_network( X, np.array([[1,0,0,0]]).T ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Technical Notes, Production and Archiving\n",
    "\n",
    "Ignore the material below. What follows is not relevant to the material being taught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Production Workflow\n",
    "\n",
    "- Finalise the notebook material above\n",
    "- Clear and fresh run of entire notebook\n",
    "- Create html slide show:\n",
    "  - `jupyter nbconvert --to slides 14_ann.ipynb `\n",
    "- Set `OUTPUTTING=1` below\n",
    "- Comment out the display of web-sourced diagrams\n",
    "- Clear and fresh run of entire notebook\n",
    "- Comment back in the display of web-sourced diagrams\n",
    "- Clear all cell output\n",
    "- Set `OUTPUTTING=0` below\n",
    "- Save\n",
    "- git add, commit and push to FML\n",
    "- copy PDF, HTML etc to web site\n",
    "  - git add, commit and push\n",
    "- rebuild binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some of this originated from\n",
    "\n",
    "<https://stackoverflow.com/questions/38540326/save-html-of-a-jupyter-notebook-from-within-the-notebook>\n",
    "\n",
    "These lines create a back up of the notebook. They can be ignored.\n",
    "\n",
    "At some point this is better as a bash script outside of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NBROOTNAME=14_ann\n",
    "OUTPUTTING=0\n",
    "\n",
    "if [ $OUTPUTTING -eq 1 ]; then\n",
    "  jupyter nbconvert --to html $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.html ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.html\n",
    "  mv -f $NBROOTNAME.html ./formats/html/\n",
    "\n",
    "  jupyter nbconvert --to pdf $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.pdf ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.pdf\n",
    "  mv -f $NBROOTNAME.pdf ./formats/pdf/\n",
    "\n",
    "  jupyter nbconvert --to script $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.py ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.py\n",
    "  mv -f $NBROOTNAME.py ./formats/py/\n",
    "else\n",
    "  echo 'Not Generating html, pdf and py output versions'\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
