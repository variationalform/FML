{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $k$-NN's: $k$-Nearest Neighbours\n",
    "\n",
    "#### *variationalform* <https://variationalform.github.io/>\n",
    "\n",
    "#### *Just Enough: progress at pace*\n",
    "\n",
    "<https://variationalform.github.io/>\n",
    "\n",
    "<https://github.com/variationalform>\n",
    "\n",
    "<https://www.brunel.ac.uk/people/simon-shaw>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "<img src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\" style=\"height:18px\"/>\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "<p>\n",
    "This work is licensed under CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)\n",
    "\n",
    "<p>\n",
    "Visit <a href=\"http://creativecommons.org/licenses/by-sa/4.0/\">http://creativecommons.org/licenses/by-sa/4.0/</a> to see the terms.\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>This document uses python</td>\n",
    "<td>\n",
    "<img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>and also makes use of LaTeX </td>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/LaTeX_logo.svg/320px-LaTeX_logo.svg.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "<td>in Markdown</td> \n",
    "<td>\n",
    "<img src=\"https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png\" style=\"height:30px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What this is about:\n",
    "\n",
    "You will be introduced to ...\n",
    "\n",
    "- The penguins data set, data frames, data selection\n",
    "- Data engineering: *mean imputation*, and dropping unknowns \n",
    "- Data bifurcation and trifurcation; calibration; tuning and **hyperparameters**\n",
    "- $k$-Nearest Neighbours - classifying by *nearness*\n",
    "- using the `KNeighborsClassifier` from `sklearn.neighbors`\n",
    "- Confusion Matrices\n",
    "\n",
    "The idea is that by using vectors to represent our data set, we can classify\n",
    "a new data point by finding the nearest data point to it for which the class\n",
    "is known. We then assign the new point with the same class. \n",
    "\n",
    "As usual our emphasis will be on *doing* rather than *proving*:\n",
    "*just enough: progress at pace*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assigned Reading\n",
    "\n",
    "For this worksheet you should read pages 19 - 25 of\n",
    "\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists, by Andreas Lindholm,\n",
    "  Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön. Cambridge University Press. \n",
    "  <http://smlbook.org>.\n",
    "\n",
    "The pages leading up to Page 19 are also highly recommended as an overview of\n",
    "concepts, purpose and uses of Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Penguins: An Example Data Set\n",
    "\n",
    "We bring in our standard imports and then recall the data sets that are\n",
    "available in seaborn. We'll be using the *penguins* data.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Tux.svg/202px-Tux.svg.png\" style=\"height:50px\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# See, for example,\n",
    "#   https://github.com/mwaskom/seaborn-data\n",
    "#   https://blog.enterprisedna.co/how-to-load-sample-datasets-in-python/\n",
    "sns.get_dataset_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some Data-Engineering\n",
    "\n",
    "As we have seen, there are a lot of data sets here that can be used\n",
    "to demonstrate various aspects of, and techniques in, Machine Learning\n",
    "and Data Science, and we'll look at a few of them - and others - as we progress.\n",
    "\n",
    "To start with though we'll be working with the penguins data set. Before\n",
    "we do any machine learning we are going to have to do some **data cleaning**,\n",
    "see e.g.\n",
    "<https://en.wikipedia.org/wiki/Data_cleansing>, to remove some \n",
    "undefined values.\n",
    "\n",
    "This shouldn't be confused with\n",
    "<https://en.wikipedia.org/wiki/Feature_engineering>.\n",
    "\n",
    "Let's grab the penguins data and see what is in it. We load it into a data frame\n",
    "called `dfp`, as in *data frame for penguins*, and then look at the head of the\n",
    "table - the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp = sns.load_dataset('penguins')\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the shape of the data set - how many rows and columns\n",
    "does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_rows, num_columns = dfp.shape\n",
    "print('number of data points (or observations) = ', num_rows)\n",
    "print('number of features (or measurement) = ', num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, the data set contains 344 rows and seven columns. Each row corresponds to a single penguin,\n",
    "and for each row each column corresponds to a feature of that penguin. We can see its species,\n",
    "the island it was found on, its bill length, bill depth, and flipper length - all in millimetres,\n",
    "its body mass in grams, and its gender.\n",
    "\n",
    "We can also see `NaN` values in row 3. That's the fourth row - be careful of this, indexing starts\n",
    "at zero. This stands for *Not a Number* and means that we can't use those values as they stand.\n",
    "We don't know why they are there - paerhaps the data got corrupted. It's a fact of life though\n",
    "that data sets are often a bit messy with wrong, missing or corrupted values. We'll see a\n",
    "couple of ways to deal with these instances below.\n",
    "\n",
    "We haven't listed every row - just the `head` of the data table. Another way to visualize these \n",
    "data is to use a scatter plot.\n",
    "\n",
    "See e.g. <https://seaborn.pydata.org/generated/seaborn.scatterplot.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp, x=\"body_mass_g\", y=\"bill_depth_mm\", hue=\"species\", style=\"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If that looks a little cramped you can control the size like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(data=dfp, x=\"body_mass_g\", y=\"bill_depth_mm\", hue=\"island\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we issued the command `dfp.head()` above we got to see the top of the table. We can also \n",
    "see the bottom like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This has given us two species, *Adelie* and *Gentoo*, but from the plots\n",
    "above we know there is also a third: *Chinstrap*.\n",
    "\n",
    "We can also see from the *head* and *tail* functions that there are two islands,\n",
    "*Torgersen* and *Biscoe*, and that - from the plots - there is a third,\n",
    "*Dream*. \n",
    "\n",
    "How could we find these without having to plot the data? Well, we could look\n",
    "at the whole table with this command:\n",
    "\n",
    "`print(dfp.to_string())`\n",
    "\n",
    "Try it in the cell below - uncomment it and execute the cell. It's a bit messy\n",
    "(and what if we had millions of rows?).\n",
    "\n",
    "Now re-comment it and execute the cell again to clear that very long output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print(dfp.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simpler way is to ask for all the unique entries in the *species* column, and  \n",
    "in the *island* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp.island.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "We have seen that three species are documented on three Antarctic islands.\n",
    "\n",
    "We have also seen that some values are undefined: `NaN` stands for \n",
    "*Not a Number*. This may indicate that the data was not captured reliably.\n",
    "\n",
    "We can see how many rows contain undefined values with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are at least eleven - and there could be 11+2+2+2+2. Let's find them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following `axis=1` tells python that we want to find **rows** with `NaN`\n",
    "in, as opposed to **columns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp[dfp.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can get a list of the row index numbers like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NaN_rows = dfp[dfp.isna().any(axis=1)]\n",
    "print(NaN_rows.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we can use these as an alternative to the `axis=1` command above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp.loc[NaN_rows.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Engineering - our first method\n",
    "\n",
    "One way to deal with missing values like this is to simply fill them\n",
    "with 'reasonable' values. For example, we can replace the numerical\n",
    "values with the mean, or average, of that feature, and replace\n",
    "categorical values with just one of the possible categories.\n",
    "\n",
    "For example, let's use the mean for numerical values and treat all \n",
    "missing genders as *Female*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# from https://datagy.io/pandas-fillna/\n",
    "dfp1 = dfp.fillna({'bill_length_mm'   : dfp['bill_length_mm'].mean(),\n",
    "                   'bill_depth_mm'    : dfp['bill_depth_mm'].mean(),\n",
    "                   'flipper_length_mm': dfp['flipper_length_mm'].mean(),\n",
    "                   'body_mass_g'      : dfp['body_mass_g'].mean(),\n",
    "                   'sex': 'Female'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Mean Imputation\n",
    "\n",
    "Replacing a missing numerical feature value with the mean of the known\n",
    "feature values in this way is called **imputing the mean**. It is easy\n",
    "to implement - just one line above - but you should be aware that it \n",
    "corrupts the original data set.\n",
    "\n",
    "- On the upside this process maintains the sample size\n",
    "- On the downside it (probably) alters some statistical properties\n",
    "of the data (the unknown variance, for example).\n",
    "\n",
    "As an analyst you would be responsible for taking a decision as to \n",
    "how to deal with missing values. You may not be the only one involved\n",
    "in that decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can compare the old and new data frames just to check this worked as\n",
    "expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here is the new one with the NaN's replaced - or engineered out\n",
    "dfp1.loc[NaN_rows.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here is the old one with the NaN's\n",
    "dfp.loc[NaN_rows.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is always good practice to check your work. This can be challenging when\n",
    "dealing with large data sets because you can't keep printing them out and\n",
    "checking every item to make sure that no errors have been introduced.\n",
    "\n",
    "One way to make sure that these commands didn't do something unexpected\n",
    "behind the scenes is just to plot each data set and make sure they look\n",
    "the same.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp, x=\"body_mass_g\", y=\"bill_depth_mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp1, x=\"body_mass_g\", y=\"bill_depth_mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternatively, the `describe()` function prints summary statistics. These should\n",
    "be the same for each.\n",
    "\n",
    "Below we see how this works. What do you think? Is everything broadly OK with our data set?\n",
    "\n",
    "Can you explain the differences? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Engineering - our second method\n",
    "\n",
    "In the method above we just replaced missing values with (hopefully)\n",
    "*nearby* ones.\n",
    "\n",
    "On the other hand, if we have a lot of data and are able to live with\n",
    "a little less of it then we can just drop the data items (rows) that\n",
    "contain one or more undefined values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **THINK ABOUT**: what could go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example: let's recall the rows with `NaN` entries and then total up\n",
    "how many there are in each column, and in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp.loc[NaN_rows.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We could have written `dfp.isna().sum(axis=0)` to insist that we are counting\n",
    "down columns here, but that's the default so the `axis=0` isn't needed.\n",
    "\n",
    "We can see that there are no more that two `NaN` values in the third to sixth\n",
    "columns, but eleven in the last, the seventh, column.\n",
    "\n",
    "**NOTE**: the digit in the left most column is just the index of the column - it\n",
    "is not considered part of the data set.\n",
    "\n",
    "So, given that we have 344 data points (penguins), it looks like we can afford to drop these\n",
    "bad data rows from the set. We can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2 = dfp.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It looks fine - the `NaN` values have disappeared from the newly \n",
    "engineered dataset. We can check, as above, by counting how many \n",
    "`NaN`'s are found in the new data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On the other hand, the index values in the left most column are off. There is\n",
    "no **3** for example. We can reset them with the `reset_index()` function but\n",
    "we have to make sure we drop the original indices otherwise they will persist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# don't do this - you'll just a column of old and useless index labels.\n",
    "# dfp2 = dfp2.reset_index()\n",
    "# instead reset the index and drop the original index column\n",
    "dfp2 = dfp2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we have a clean data set with no false values introduced, with no\n",
    "undefined entries, and with consecutive labelling down the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualization\n",
    "\n",
    "Data sets are often much too large to be able to effectively work with them\n",
    "in tabular form. Visualization is then more useful.\n",
    "\n",
    "Let's pause to explore a few visuals of our cleaned-up data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"body_mass_g\", y=\"flipper_length_mm\", hue=\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(dfp2, hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# lots of options for the above. See\n",
    "# https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "sns.pairplot(dfp2, corner=True, hue='species', height=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.pairplot(dfp2, diag_kind=\"kde\", hue='species')\n",
    "g.map_lower(sns.kdeplot, levels=4, color=\".2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Further Exploration of the Data Set\n",
    "\n",
    "So far we have loaded the data, and operated on it row by row as well as\n",
    "plotted various views of the data. \n",
    "\n",
    "Let's look now at how to manipulate the data set at a lower level, and\n",
    "see how we might separate out clusters of data - data items that each\n",
    "share a common feature.\n",
    "\n",
    "Recall, this is what our set contains..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see how the species form almost distinct clusters with the\n",
    "following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can access the column of `species` data using square brackets like this\n",
    "\n",
    "`dfp2['species']`\n",
    "\n",
    "This refers to every row - with lots of repeated values. In fact they wont all\n",
    "get printed out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can squeeze out the repeats into just one uniquely occuring \n",
    "feature value like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2['species'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This tells us that there are three unique species. We knew this from the\n",
    "plots - but that was a human taking a look. This method allows the code to \n",
    "determine the same information without human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Creating Data Subsets\n",
    "\n",
    "It is sometimes useful to be able to separate out the data subsets, by a\n",
    "given feature value. If we choose to separate by 'species' then this command\n",
    "\n",
    "`dfp2.loc[ dfp2['species'] == 'Adelie' ]`\n",
    "\n",
    "will give us back a new data frame that just contains the Adelie penguin\n",
    "data. It does this by using square brackets and double equals so that\n",
    "this statement,\n",
    "\n",
    "`dfp2['species'] == 'Adelie'`\n",
    "\n",
    "evaluates to **true** if, for a given row, the species feature is\n",
    "*Adelie*. Then\n",
    "\n",
    "`dfp2.loc[ ? ]`\n",
    "\n",
    "keeps only those rows for which the question mark is *true*. We can\n",
    "assign these rows to a new data frame.\n",
    "\n",
    "This means that we can create three data subsets - one for each \n",
    "species - as follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfA = dfp2.loc[dfp2['species'] == 'Adelie']\n",
    "dfC = dfp2.loc[dfp2['species'] == 'Chinstrap']\n",
    "dfG = dfp2.loc[dfp2['species'] == 'Gentoo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using `matplotlib` to plot the clusters separately\n",
    "\n",
    "We can use `plt.scatter` to plot scatter plots directly in \n",
    "`matplotlib` as below. First we create arrays (vectors if you like)\n",
    "of values, and then we plot them in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "blA=np.array(dfA['bill_length_mm'].tolist())\n",
    "bdA=np.array(dfA['bill_depth_mm'].tolist())\n",
    "plt.scatter(blA,bdA,color='blue')\n",
    "\n",
    "blC=np.array(dfC['bill_length_mm'].tolist())\n",
    "bdC=np.array(dfC['bill_depth_mm'].tolist())\n",
    "plt.scatter(blC,bdC,color='orange')\n",
    "\n",
    "blG=np.array(dfG['bill_length_mm'].tolist())\n",
    "bdG=np.array(dfG['bill_depth_mm'].tolist())\n",
    "plt.scatter(blG,bdG,color='green')\n",
    "plt.xlabel('bill_length_mm')\n",
    "plt.ylabel('bill_depth_mm')\n",
    "plt.legend(['Adelie', 'Chinstrap', 'Gentoo'],loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interpreting the plot. We can get some statistics by using `describe` - as\n",
    "we have seen before. By comparing the means, below, with the plot above \n",
    "we can check that all is as it should be.\n",
    "\n",
    "Finding short cut ways to sanity check your working like this is useful.\n",
    "\n",
    "Here `dfA` is plotted in blue, and we can check that the means look reasonable \n",
    "given the axis labelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfA.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you are interested in the arrays that we created in order to do these plots\n",
    "you can take a look at them like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are `numpy` arrays. There are a number of ways that you can select\n",
    "out just a subset of an array by using square brackets with `slicing`.\n",
    "\n",
    "For example, we can look at the third to fifth entries like this:\n",
    "\n",
    "`blA[2:5]`\n",
    "\n",
    "Indexing starts at zero, hence the `2`. The `5` denotes the first \n",
    "index that is *not used*. This is confusing so watch out for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blA[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we can look at all entries except the last five like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blA[:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look now at how we can interrogate our three smaller data subsets.\n",
    "\n",
    "Here are two ways to determine the number of rows in each.\n",
    "\n",
    "First, using `shape[0]`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('number of rows in dfA = ', dfA.shape[0], '; in dfC = ', dfC.shape[0], ' and in dfG = ', dfG.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And second using the fact that shape provides a list of two values,\n",
    "and we can ignore the second with `_`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rA, _ = dfA.shape; rC, _ = dfC.shape; rG, _ = dfG.shape\n",
    "print('number of rows in dfA = ', rA, '; in dfC = ', rC, ' and in dfG = ', rG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each of these can be used to determine how many of each species there are,\n",
    "because there is one row for each penguin in each data subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $k$-NN's - developing intuition\n",
    "\n",
    "We can now look at the $k$ Nearest Neighbours, or $k$-NN, method for classification\n",
    "of data.\n",
    "\n",
    "The setting we assume at the outset is that we have a 'training set' of data such\n",
    "that each row of the data set corresponds to one observation.\n",
    "\n",
    "Moreover, in each row there are numerical features which can be organized into\n",
    "a vector, $\\boldsymbol{x}=(x_1,x_2,\\ldots,x_n)^T$, and a label, $y$,\n",
    "which is categorical. \n",
    "\n",
    "There may be other numerical and categorical data that we choose not to use.\n",
    "\n",
    "We imagine plotting these data points in $n$-dimensional space (hard to imagine\n",
    "when $n>3$, which is why the abstraction of mathematics is so useful), and we \n",
    "imagine them being coloured according to the value of the label $y$.\n",
    "\n",
    "In the example above we had \n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{x} & = (\\mathtt{bill\\underline{\\ }length\\underline{\\ }mm},\n",
    "                    \\mathtt{bill\\underline{\\ }depth\\underline{\\ }mm})^T\n",
    "\\\\\n",
    "y & = (\\mathtt{Adelie}, \\mathtt{Chinstrap}, \\mathtt{Gentoo})^T\n",
    "\\end{align}\n",
    "\n",
    "and we coloured the labels as blue, orange or green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now imagine that a field researcher reports in some new measurements for\n",
    "a penguin, and that we want to classify its species based only on those\n",
    "measurements.\n",
    "\n",
    "The idea is to plot the new measurements and see which cluster of like\n",
    "colour they are closest to. This closest cluster (colour) is then used to\n",
    "assign the species to that new measurement.\n",
    "\n",
    "Let's see a dummy run of this in a picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the diagram below we pretend that we only have the first twenty\n",
    "rows of each of the data subsets. We plot them as coloured dots, just as above.\n",
    "\n",
    "Then we pretend that we get three new observations. For illustration \n",
    "purposes we take the entries from the fourth from last position in each\n",
    "data set.\n",
    "\n",
    "But in the **_REAL WORLD_** we would be expecting new data to be arriving \n",
    "**_UNSEEN_** from the field.\n",
    "\n",
    "We plot these 'new observations' with a cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot first twenty rows of each as coloured dots.\n",
    "plt.scatter(blA[0:20],bdA[0:20],color='blue')\n",
    "plt.scatter(blC[0:20],bdC[0:20],color='orange')\n",
    "plt.scatter(blG[0:20],bdG[0:20],color='green')\n",
    "plt.legend(['Adelie', 'Chinstrap', 'Gentoo'],loc='lower right')\n",
    "plt.xlabel('bill_length_mm')\n",
    "plt.ylabel('bill_depth_mm')\n",
    "\n",
    "# pick out the data item fourth from the end in each\n",
    "indx = -4\n",
    "# and plot each as a cross\n",
    "plt.scatter(blA[indx],bdA[indx],color='blue', marker='x', s=500)\n",
    "plt.scatter(blC[indx],bdC[indx],color='orange', marker='x', s=500)\n",
    "plt.scatter(blG[indx],bdG[indx],color='green', marker='x', s=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We carry out the classification as follows:\n",
    "\n",
    "1. The green cross is quite central in the green, Gentoo, cluster and\n",
    "so we can classify this new observation as a Gentoo penguin.\n",
    "\n",
    "2. The blue cross isn't that central in the blue cluster, but on the\n",
    "other hand it is far away from the yellow and green clusters and\n",
    "so we can safely classify this observation as an Adelie penguin.\n",
    "\n",
    "3. The yellow cross presents us with more of a dilemma though. A careful\n",
    "look suggests that it is slightly closer to the yellow cluster than the\n",
    "blue and so, on that basis, we would probably choose to classify that\n",
    "penguin as a Chinstrap. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Any comments, thoughts, questions?\n",
    "\n",
    "The first two steps seem safe, and justifiable. They are *explainable*. The third \n",
    "less so. We can see that the yellow cross corresponds to a fairly typical bill\n",
    "depth for an Adelie.\n",
    "\n",
    "- So is it a Chinstrap?\n",
    "\n",
    "- We can also see that Adelie penguins have bill lengths that straddle the value\n",
    "indicated by the yellow cross.\n",
    "\n",
    "- So should the yellow cross observation be classified as a Chinstrap?\n",
    "\n",
    "- We see here that the issue of **explainability** can be vexed.\n",
    "\n",
    "- If we had more data the yellow cross might become obviously a Chinstrap,\n",
    "\n",
    "- Or it might be obvious that it is an Adelie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**_Explainability_** may or may not matter. But it is increasingly becoming a hot \n",
    "topic in data science. \n",
    "\n",
    "Suppose your pension fund invested everything in a new tech venture that \n",
    "was going to design batteries with infinite life. It will fail of course.\n",
    "\n",
    "If this venture was suggested by an Artificially Intelligent agent powered by \n",
    "machine learning algorithms then the pension company directors wont be\n",
    "able to explain their reasoning if the underlying data science was not explainable.\n",
    "\n",
    "This is hardly realistic, but explainability is a big and important deal in \n",
    "areas like finance and investing, and in medical diagnosis, to name but two. The\n",
    "reasons for its importance are obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $k$-NN's - the mathematical details\n",
    "\n",
    "We index each data point in the training set with a subscript. So we have\n",
    "the feature vectors $\\boldsymbol{x}_1$, $\\boldsymbol{x}_2$,\n",
    "$\\boldsymbol{x}_3$, $\\ldots$.\n",
    "Each of these has a label, $y_1$, $y_2$, $y_3$, $\\ldots$.\n",
    "\n",
    "These are the coloured dots above. The positions are the features.\n",
    "The colours are the labels.\n",
    "\n",
    "We now get a new observation, $\\boldsymbol{x}^*$ and we want to classify it - we\n",
    "want to apply a label to it using the data from the training set.\n",
    "\n",
    "The mathematical version of the process we followed above was to determine\n",
    "the distance between $\\boldsymbol{x}^*$ and each $\\boldsymbol{x}_i$ using\n",
    "\n",
    "$$\n",
    "\\Vert\\boldsymbol{x}^* - \\boldsymbol{x}_i\\Vert_2\n",
    "\\qquad\\text{(recall: the Euclidean, Pythagorean or $\\ell_2$ norm).}\n",
    "$$\n",
    "\n",
    "We then to choose the value $i$ such that this distance is a minimum. The\n",
    "label, $y_i$, corresponding to that particular $i$ is then assigned to\n",
    "the new observation $\\boldsymbol{x}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Reference to the Assigned Reading\n",
    "\n",
    "You were recommended to read pages 19 - 25 of\n",
    "\n",
    "- MLFCES: Machine Learning: A First Course for Engineers and Scientists, by Andreas Lindholm,\n",
    "  Niklas Wahlström, Fredrik Lindsten, Thomas B. Schön. Cambridge University Press. \n",
    "  <http://smlbook.org>.\n",
    "\n",
    "More details on this are given there, in paticular:\n",
    "\n",
    "- the use of $k$-NN for regression as well as classification.\n",
    "- the use of more than one 'nearest' neighbour - see which cluster 'wins' a vote.\n",
    "- notes on how to choose the number of neighbours, and 'overfitting'.\n",
    "- the importance of normalizing the inputs\n",
    "\n",
    "Also of importance, but not mentioned in the book, is the choice of norm. \n",
    "We referred to the Euclidean or Pythagorean norm above, but we could just as easily\n",
    "have chosen any of the other $p$ norms that we discussed when we reviewed the material\n",
    "on vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "In the discussion above we just touched upon the important issue\n",
    "of picking *hyperparameters*. These are values and choices that need\n",
    "to be specified to the algorithm, the code, prior to the machine learning\n",
    "phase.\n",
    "\n",
    "In the above we mentioned that we need to choose:\n",
    "\n",
    "- $k$ - the number of nearest neighbours to search for.\n",
    "- $p$ - the choice of norm to use to measure distance, *nearness*.\n",
    "\n",
    "These are *human* choices: the *hyperparameters* are not learned from the\n",
    "data, but need to be chosen upfront."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Set Bifurcation and Trifurcation\n",
    "\n",
    "However, we don't necessarily need to worry about making a wrong choice \n",
    "of hyperparameters that cannot subsequently be changed. In practice we\n",
    "would be prepared to *calibrate* the model by *tuning* its performance\n",
    "by turning the dials on the hyperparameter values.\n",
    "\n",
    "Usually the dataset that we are working with will be either *bifurcated*\n",
    "into a *training* and a *test* set. Or will be *trifurcated* into a *training*,\n",
    "*validation* and a *test* set.\n",
    "\n",
    "We'll return to this as we go through, but briefly...\n",
    "\n",
    "- The *training set*: used to initialise the machine learning model.\n",
    "- The *validation set*: used to tune the hyperparameters.\n",
    "- The *test set*: used as **unseen data** to derive final performance quality\n",
    "measurements after training and validation has been completed.\n",
    "\n",
    "It is important to realise that the test set output should never be used to\n",
    "further tune and calibrate the model. It is a **_hold out_** set that \n",
    "simulates how the model will perform in the **_real world_** on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data set is treated in all of these cases as *ground truth* - it is \n",
    "believed to be true, although in practice some data points might contain\n",
    "errors, or be missing. And there is almost certainly going to be some\n",
    "noise on any numerical values recorded in the data.\n",
    "\n",
    "There no hard and fast rules on the proportions to use to bifurcate\n",
    "or trifurcate the data set. We might bifurcate using 75%/25% for \n",
    "example, or trifurcate with 50%/25%/25%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing `scikit-learn`, our first visit\n",
    "\n",
    "Let's now see now how to use `scikit-learn` to do $k$-NN classification with the\n",
    "penguins data that we cleaned and prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following code was adapted in its early stages from\n",
    "*Machine Learning with Python, tutorialspoint* as found here\n",
    "<https://www.tutorialspoint.com/machine_learning_with_python/index.htm>\n",
    "or here\n",
    "<https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_tutorial.pdf>\n",
    "\n",
    "You'll have seen a number of instances by now in these notebooks where external\n",
    "sources are liberally referenced. Feel free to do this - but make sure that you\n",
    "**always acknowledge your sources**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are going to work with the entire cleaned-up penguins data set that we\n",
    "originally stored in `dfp2`.\n",
    "\n",
    "Let's remember what it loked like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to use the numerical features (values) in each row to predict species. \n",
    "\n",
    "Before we start using the `sklearn` python library we need to see how we can \n",
    "pick these data items out using **array slicing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, we can pick out the value of the species with this command\n",
    "(the colon part is important - it refers to column zero)\n",
    "\n",
    "`dfp2.iloc[2, 0:1].values`\n",
    "\n",
    "This refers to the entry in the third, the '2', row and first, the '0:1', column.\n",
    "\n",
    "To refer to all rows we replace the `2` with a colon `:` - as we'll see below.\n",
    "\n",
    "Let's see it in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2.iloc[2, 0:1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Second, we can refer to the four numerical features with this command\n",
    "\n",
    "`dfp2.iloc[1, 2:6].values`\n",
    "\n",
    "which refers to second row, and columns three to six inclusive. Once \n",
    "again we will use a colon to refer to all rows.\n",
    "\n",
    "Again, let's see this in action... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dfp2.iloc[1, 2:6].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using `sklearn`\n",
    "\n",
    "We will now fit the $k$-NN model using the Manhattan, or taxicab, norm, which we also\n",
    "call the $p=1$ norm:\n",
    "\n",
    "$$\n",
    "\\Vert\\boldsymbol{x}^* - \\boldsymbol{x}_i\\Vert_1.\n",
    "$$\n",
    "\n",
    "In addition, we will use two ($k=2$) nearest neighbours, and we will also obtain\n",
    "something called the confusion matrix, and will print some performance data.\n",
    "\n",
    "The last two of these will be re-visited because they exhibit two very important\n",
    "means in which we can assess the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically we assign the data set features to a variable called `X`, and the \n",
    "data set labels to a variable called `y`. Using the array slicing that we\n",
    "saw above this is straightforward... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We assign the numerical features to X\n",
    "X = dfp2.iloc[:, 2:6].values\n",
    "# And we assign the species label to y\n",
    "y = dfp2.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We could bifurcate the data into a training and test set ourselves, but `sklearn` \n",
    "provides a helper function for this. It is called `train_test_split`.\n",
    "\n",
    "First we have to import it. Then we give it `X` and `y` and specify the\n",
    "proportion of the data that we use for the *hold out*, or *test* set.\n",
    "we'll specify that 40% of the data should be reserved for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from the scikit-learn library we use 40% of the data to test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The function returns four subsets of data:\n",
    "\n",
    "```\n",
    "X_train - 60% of the data set features to be used to configure the model\n",
    "X_test  - 40% of the data set features to used to test the configured model\n",
    "y_train - 60% of the data set features matching the X_train features\n",
    "y_test  - 40% of the data set features matching the X_test features\n",
    "```\n",
    "\n",
    "We can look at the sizes of each of these by using `shape` as follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('shape of X_train = ', X_train.shape,' and of X_test = ', X_test.shape)\n",
    "print('shape of y_train = ', y_train.shape,' and of y_test = ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Normalization of Data\n",
    "\n",
    "The next step is to normalize the feature data - the importance and role of this step\n",
    "is discussed in the recommended reading of pages 19 - 25 [MLFCES]. Again,\n",
    "`sklearn` provides a helper function for this called `StandardScaler`. \n",
    "This will remove the mean from the data and scale to unit variance. You\n",
    "can read more about this here:\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import the helper and give it a name\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# initialise the scaler by feeding it the training data\n",
    "scaler.fit(X_train)\n",
    "# now carry out the transformation of all of the feauture data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**REMARK:** note that `X_train` is used to provide the scaling data, and\n",
    "not `X_test`. This is because `X_test` is *hold out data*. We must treat it as\n",
    "**unseen**. We can freely transform it though, because that can be done \n",
    "without actually looking at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fitting: Learning from Data\n",
    "\n",
    "We can now bring in the $k$-NN classifier method from `sklearn`\n",
    "and obtain a `classifier` object that uses $k=2$ nearest neighbours and \n",
    "the $p=1$ norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the k-NN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# assign it with k=2 and p=1\n",
    "classifier = KNeighborsClassifier(n_neighbors=2, p=1)\n",
    "# give the training data to the classifier\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The last step above is just like the coloured cluster plots above **before**\n",
    "we plotted the larger crosses. The model now has *knowledge* of these clusters,\n",
    "this is an example of *machine learning*.\n",
    "\n",
    "By giving the model the unseen test data we are in effect telling it where the\n",
    "large crosses are. The model then finds the two nearest neighbours, using the\n",
    "manhattan norm, to classify the species of those crosses. This produces predictions\n",
    "of the species in `y_test`, and we call these predicted species values `y_pred`.\n",
    "\n",
    "So, with the `crosses` as the features in the test set, we feed this in to the\n",
    "classifier and obtain the predicted values as follows... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluation of Performance\n",
    "\n",
    "Now we come to the real crux of the matter. We know what `X_test` should\n",
    "produce as species values - they are in `y_test`. What we actually\n",
    "get though are `y_pred`. If `y_pred = y_test` then we should be very happy\n",
    "because it indicates that the model works very well on unseen data.\n",
    "\n",
    "In practice though, it is unlikely that each of the 134 elements in `y_pred`\n",
    "will match every one of the corresponding values in `y_test`.\n",
    "\n",
    "We have several tools available to assess the quality of the model. We'll\n",
    "take a quick look at a couple of these now, with a brief explanation,\n",
    "and we'll return many times to them later and understand them in more detail.\n",
    "\n",
    "First we import the helper functions. Then we obtain and print the \n",
    "**confusion matrix**, next some statistics in a **classification report**,\n",
    "and then an **accuracy score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "clsrep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print(clsrep)\n",
    "\n",
    "accsc = accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy:\", accsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll come back to the classification report later, and for now just note that\n",
    "the accuracy score tells us the proportion of the test set for which the species\n",
    "was correctly predicted.\n",
    "\n",
    "What we want to spend some time on here is the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The confusion matrix is square with the same number of rows/columns\n",
    "as there are values for the label. In our case there are three \n",
    "possible label values: *Adelie*, *Chinstrap*, and *Gentoo*. We can refer\n",
    "to these as group 1, 2 and 3.\n",
    "\n",
    "The entry in row $i$ and column $j$ of the confusion matrix tells\n",
    "us how many data points in `X_test` that were in group $i$ were\n",
    "predicted by the model to be in group $j$.\n",
    "\n",
    "Now, the representation of the confusion matrix above is a numpy\n",
    "array and although it is useful for coding, it isn't very \n",
    "user friendly. The following code gives us something much nicer,\n",
    "and it is much easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cmplot = ConfusionMatrixDisplay(cm, display_labels=classifier.classes_)\n",
    "cmplot.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can now immediately get a feeling for *how good* the model is. The diagonal\n",
    "elements tell us how many species predictions match the true value. The\n",
    "off-diagonals tell us how many misses there are, and how they missed.\n",
    "\n",
    "For example, the number in the middle of the top row tells us how many Adelie\n",
    "penguins were mistakenly predicted to be Chinstraps.\n",
    "\n",
    "Also, the overall accuracy percentage can be determined by adding all the\n",
    "numbers in the matrix, calling the total $B$, and adding all the diagonal elements\n",
    "together, as $A$. The value of $A/B$ then tells us the proportion of correct \n",
    "predictions - and that is the *Accuracy* score above.\n",
    "\n",
    "We haven't yet properly reviewed the mathematical concept and notion of\n",
    "a matrix yet, although we will do soon. We will be coming back to \n",
    "confusion matrices over and over again though. \n",
    "\n",
    "Before moving on to some exercises we close with a comment about using\n",
    "the $k$-NN model for *regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-NN for regression\n",
    "\n",
    "Above we saw how we can use $k$-N for classification: here, given feature data\n",
    "from an observation, we predict the label as the category the observation\n",
    "should be assigned to.\n",
    "\n",
    "Regression is where the features and the labels vary in continuous sets of\n",
    "values. For example, we might want to predict the amount of rainfall given\n",
    "the number of hours of cloud, sun, daylight, along with air tempertaure,\n",
    "humidity and pressure.\n",
    "\n",
    "These are all continuous variables, not discrete categorical ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $k$-NN technique can also be used for regression by, in effect, \n",
    "turning the continuous variables into discrete ones. To get an idea\n",
    "of this imagine learning a function $y=f(x)$ as follows.\n",
    "\n",
    "- take a set of values (features) $x_1, x_2, x_3, \\ldots$\n",
    "- take the corresponding labels $f(x_1), f(x_2), f(x_3), \\ldots$\n",
    "- plot these points - treat each as a cluster, as above.\n",
    "\n",
    "The predict the function value given a new point, $x^*$, we would:\n",
    "\n",
    "1. determine $i$ such that $\\vert x^* - x_i\\vert$ is minimal over all of $x_1, x_2, x_3, \\ldots$.\n",
    "2. say that $x_i$ is the nearest neighbour to $x^*$\n",
    "3. estimate $f(x^*)$ by $f(x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This will work well if $f$ is quite well-behaved: continuous and not \n",
    "very rapidly varying, for example.\n",
    "\n",
    "We wont touch more on this - it isn't on our journey. If you are interested\n",
    "in seeing more about this though you can, for example, look here\n",
    "<https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/>\n",
    "for a demonstration of this using the California house data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Experiment with the $k$-NN classifier we just developed. For example,\n",
    "\n",
    "- Change the 60%/40% bifurcation\n",
    "- Change the value of $k$: decrease it to $1$, or increase it to $3,4,5,\\ldots$\n",
    "- Change the norm from $p=1$ to $p>1$. \n",
    "- Does $p<1$ make any sense here?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Look at the following scatter plots. Suppose we wished to predict gender\n",
    "from two features.\n",
    "\n",
    "- What two features would work best do you think?\n",
    "- Which pairs of features are unlikely to work well?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"bill_length_mm\", y=\"bill_depth_mm\", style=\"species\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"flipper_length_mm\", y=\"body_mass_g\", style=\"species\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"body_mass_g\", y=\"bill_depth_mm\", style=\"species\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"body_mass_g\", y=\"bill_length_mm\", style=\"species\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"bill_length_mm\", y=\"flipper_length_mm\", style=\"species\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfp2, x=\"bill_depth_mm\", y=\"flipper_length_mm\", style=\"species\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "The confusion matrix we generated above is a **numpy array**. We will be looking\n",
    "in much more detail at these objects - both mathematically and in code - soon,\n",
    "but first here is a warm up. Let's recall the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can use `cm[0,0]` to access the value in the first row and first column.\n",
    "\n",
    "- What do you think `cm[1,1]` and `cm[2,2]` refer to?\n",
    "- what do you think `cm[0,0]+cm[1,1]+cm[2,2]` produces?\n",
    "\n",
    "Check your answers by using \n",
    "\n",
    "- `print(cm[1,1],cm[2,2])`\n",
    "- `print(cm[0,0]+cm[1,1]+cm[2,2])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(cm[1,1],cm[2,2])\n",
    "print(cm[0,0]+cm[1,1]+cm[2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do you think `cm.sum()` produces? Check, or discover, with\n",
    "\n",
    "- `print(cm.sum())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(cm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do you think `cm[0,0]+cm[1,1]+cm[2,2]` and `cm.sum()` relate to the\n",
    "`Accuracy` score given above? Print out your answer and check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print((cm[0,0]+cm[1,1]+cm[2,2])/cm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Compare `np.trace(cm)` to `cm[0,0]+cm[1,1]+cm[2,2]` - use your findings \n",
    "to shorten the command above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.trace(cm)/cm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Technical Notes, Production and Archiving\n",
    "\n",
    "Ignore the material below. What follows is not relevant to the material being taught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Production Workflow\n",
    "\n",
    "- Finalise the notebook material above\n",
    "- Clear and fresh run of entire notebook\n",
    "- Create html slide show:\n",
    "  - `jupyter nbconvert --to slides 3_knn.ipynb `\n",
    "- Set `OUTPUTTING=1` below\n",
    "- Comment out the display of web-sourced diagrams\n",
    "- Clear and fresh run of entire notebook\n",
    "- Comment back in the display of web-sourced diagrams\n",
    "- Clear all cell output\n",
    "- Set `OUTPUTTING=0` below\n",
    "- Save\n",
    "- git add, commit and push to FML\n",
    "- copy PDF, HTML etc to web site\n",
    "  - git add, commit and push\n",
    "- rebuild binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some of this originated from\n",
    "\n",
    "<https://stackoverflow.com/questions/38540326/save-html-of-a-jupyter-notebook-from-within-the-notebook>\n",
    "\n",
    "These lines create a back up of the notebook. They can be ignored.\n",
    "\n",
    "At some point this is better as a bash script outside of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NBROOTNAME='3_knn'\n",
    "OUTPUTTING=0\n",
    "\n",
    "if [ $OUTPUTTING -eq 1 ]; then\n",
    "  jupyter nbconvert --to html $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.html ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.html\n",
    "  mv -f $NBROOTNAME.html ./formats/html/\n",
    "\n",
    "  jupyter nbconvert --to pdf $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.pdf ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.pdf\n",
    "  mv -f $NBROOTNAME.pdf ./formats/pdf/\n",
    "\n",
    "  jupyter nbconvert --to script $NBROOTNAME.ipynb\n",
    "  cp $NBROOTNAME.py ../backups/$(date +\"%m_%d_%Y-%H%M%S\")_$NBROOTNAME.py\n",
    "  mv -f $NBROOTNAME.py ./formats/py/\n",
    "else\n",
    "  echo 'Not Generating html, pdf and py output versions'\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
